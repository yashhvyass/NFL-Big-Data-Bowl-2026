{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 114239,
          "databundleVersionId": 14210809,
          "sourceType": "competition"
        },
        {
          "sourceId": 13441428,
          "sourceType": "datasetVersion",
          "datasetId": 8531647
        },
        {
          "sourceId": 13734475,
          "sourceType": "datasetVersion",
          "datasetId": 8738717
        },
        {
          "sourceId": 13754705,
          "sourceType": "datasetVersion",
          "datasetId": 8752519
        },
        {
          "sourceId": 13754830,
          "sourceType": "datasetVersion",
          "datasetId": 8752624
        },
        {
          "sourceId": 13761334,
          "sourceType": "datasetVersion",
          "datasetId": 8577863,
          "isSourceIdPinned": false
        },
        {
          "sourceId": 13978819,
          "sourceType": "datasetVersion",
          "datasetId": 8911353
        },
        {
          "sourceId": 627985,
          "sourceType": "modelInstanceVersion",
          "modelInstanceId": 472923,
          "modelId": 488792
        }
      ],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "All ensemble 6 models",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yashhvyass/NFL-Big-Data-Bowl-2026/blob/main/All_ensemble_6_models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "O1LSsNW1ed9Q"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "nfl_big_data_bowl_2026_prediction_path = kagglehub.competition_download('nfl-big-data-bowl-2026-prediction')\n",
        "chengzhijiang_hsiaosuan_sttn_path = kagglehub.dataset_download('chengzhijiang/hsiaosuan-sttn')\n",
        "goose666_nfl_gnn_a43_path = kagglehub.dataset_download('goose666/nfl-gnn-a43')\n",
        "goose666_1113gru_0576_path = kagglehub.dataset_download('goose666/1113gru-0576')\n",
        "goose666_1114kaiyuan_0562_path = kagglehub.dataset_download('goose666/1114kaiyuan-0562')\n",
        "pankajiitr_nfl_bdb_2026_path = kagglehub.dataset_download('pankajiitr/nfl-bdb-2026')\n",
        "yashhvyass_cumsum_model_dataset_path = kagglehub.dataset_download('yashhvyass/cumsum-model-dataset')\n",
        "goose666_1103new_all_all_all_pytorch_default_1_path = kagglehub.model_download('goose666/1103new-all-all-all/PyTorch/default/1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "t0jzbitmed9R"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nfl_mymodel.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import polars as pl\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "import joblib\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class Config:\n",
        "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n",
        "\n",
        "    SEED = 42\n",
        "    N_FOLDS = 5\n",
        "    BATCH_SIZE = 256\n",
        "    WINDOW_SIZE = 12\n",
        "    HIDDEN_DIM = 128\n",
        "    MAX_FUTURE_HORIZON = 94\n",
        "\n",
        "    K_NEIGH = 6\n",
        "    RADIUS = 30.0\n",
        "    TAU = 8.0\n",
        "    N_ROUTE_CLUSTERS = 7\n",
        "\n",
        "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
        "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(Config.SEED)\n",
        "\n",
        "class ResidualMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        layers.append(nn.LayerNorm(hidden_dim))\n",
        "        layers.append(nn.GELU())\n",
        "        layers.append(nn.Dropout(dropout))\n",
        "\n",
        "        for _ in range(num_layers - 2):\n",
        "            layers.append(ResidualBlock(hidden_dim, hidden_dim, dropout))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.net = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "        self.activation = nn.GELU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.activation(self.net(x) + x)\n",
        "\n",
        "class SpatioTemporalTransformer(nn.Module):\n",
        "    def __init__(self, input_dim, horizon, hidden_dim=256, num_heads=8, num_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.horizon = horizon\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "        self.temporal_pos_encoding = nn.Parameter(torch.randn(1, Config.WINDOW_SIZE, hidden_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            norm_first=True\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.prediction_head = ResidualMLP(\n",
        "            input_dim=hidden_dim,\n",
        "            hidden_dim=hidden_dim * 2,\n",
        "            output_dim=horizon,\n",
        "            num_layers=3,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        self.output_norm = nn.LayerNorm(horizon)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.constant_(module.bias, 0.0)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.constant_(module.bias, 0.0)\n",
        "            torch.nn.init.constant_(module.weight, 1.0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, window_size, _ = x.shape\n",
        "\n",
        "        x = self.input_projection(x)\n",
        "        x = x + self.temporal_pos_encoding[:, :window_size, :]\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        attention_weights = torch.softmax(torch.mean(x, dim=-1), dim=-1)\n",
        "        x_pooled = torch.sum(x * attention_weights.unsqueeze(-1), dim=1)\n",
        "\n",
        "        pred = self.prediction_head(x_pooled)\n",
        "        pred = self.output_norm(pred)\n",
        "        pred = torch.cumsum(pred, dim=1)\n",
        "\n",
        "        return pred\n",
        "\n",
        "class ImprovedSeqModel(nn.Module):\n",
        "    def __init__(self, input_dim, horizon):\n",
        "        super().__init__()\n",
        "        self.horizon = horizon\n",
        "        self.model = SpatioTemporalTransformer(\n",
        "            input_dim=input_dim,\n",
        "            horizon=horizon,\n",
        "            hidden_dim=256,\n",
        "            num_heads=8,\n",
        "            num_layers=4,\n",
        "            dropout=0.1\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "def height_to_feet(height_str):\n",
        "    try:\n",
        "        ft, inches = map(int, str(height_str).split('-'))\n",
        "        return ft + inches/12\n",
        "    except:\n",
        "        return 6.0\n",
        "\n",
        "def get_velocity(speed, direction_deg):\n",
        "    theta = np.deg2rad(direction_deg)\n",
        "    return speed * np.sin(theta), speed * np.cos(theta)\n",
        "\n",
        "def create_base_features(input_df):\n",
        "    df = input_df.copy()\n",
        "\n",
        "    df['player_height_feet'] = df['player_height'].apply(height_to_feet)\n",
        "\n",
        "    height_parts = df['player_height'].str.split('-', expand=True)\n",
        "    df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n",
        "    df['bmi'] = (df['player_weight'] / (df['height_inches']**2)) * 703\n",
        "\n",
        "    dir_rad = np.deg2rad(df['dir'].fillna(0))\n",
        "    df['velocity_x'] = df['s'] * np.sin(dir_rad)\n",
        "    df['velocity_y'] = df['s'] * np.cos(dir_rad)\n",
        "    df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n",
        "    df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n",
        "\n",
        "    df['is_offense'] = (df['player_side'] == 'Offense').astype(int)\n",
        "    df['is_defense'] = (df['player_side'] == 'Defense').astype(int)\n",
        "    df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(int)\n",
        "    df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(int)\n",
        "    df['is_passer'] = (df['player_role'] == 'Passer').astype(int)\n",
        "\n",
        "    df['role_targeted_receiver'] = df['is_receiver']\n",
        "    df['role_defensive_coverage'] = df['is_coverage']\n",
        "    df['role_passer'] = df['is_passer']\n",
        "    df['side_offense'] = df['is_offense']\n",
        "\n",
        "    mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n",
        "    df['momentum_x'] = df['velocity_x'] * df['player_weight']\n",
        "    df['momentum_y'] = df['velocity_y'] * df['player_weight']\n",
        "    df['kinetic_energy'] = 0.5 * df['player_weight'] * (df['s'] ** 2)\n",
        "\n",
        "    df['speed_squared'] = df['s'] ** 2\n",
        "    df['accel_magnitude'] = np.sqrt(df['acceleration_x']**2 + df['acceleration_y']**2)\n",
        "    df['orientation_diff'] = np.abs(df['o'] - df['dir'])\n",
        "    df['orientation_diff'] = np.minimum(df['orientation_diff'], 360 - df['orientation_diff'])\n",
        "\n",
        "    if 'ball_land_x' in df.columns:\n",
        "        ball_dx = df['ball_land_x'] - df['x']\n",
        "        ball_dy = df['ball_land_y'] - df['y']\n",
        "        df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
        "        df['dist_to_ball'] = df['distance_to_ball']\n",
        "        df['dist_squared'] = df['distance_to_ball'] ** 2\n",
        "        df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
        "        df['ball_direction_x'] = ball_dx / (df['distance_to_ball'] + 1e-6)\n",
        "        df['ball_direction_y'] = ball_dy / (df['distance_to_ball'] + 1e-6)\n",
        "        df['closing_speed_ball'] = (\n",
        "            df['velocity_x'] * df['ball_direction_x'] +\n",
        "            df['velocity_y'] * df['ball_direction_y']\n",
        "        )\n",
        "        df['velocity_toward_ball'] = (\n",
        "            df['velocity_x'] * np.cos(df['angle_to_ball']) +\n",
        "            df['velocity_y'] * np.sin(df['angle_to_ball'])\n",
        "        )\n",
        "        df['velocity_alignment'] = np.cos(df['angle_to_ball'] - dir_rad)\n",
        "        df['angle_diff'] = np.abs(df['o'] - np.degrees(df['angle_to_ball']))\n",
        "        df['angle_diff'] = np.minimum(df['angle_diff'], 360 - df['angle_diff'])\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_lag_features(df, window_size=8):\n",
        "    df = df.copy()\n",
        "\n",
        "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
        "\n",
        "    for lag in [1, 2, 3, 4, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_lag{lag}'] = df.groupby(gcols)[col].shift(lag)\n",
        "\n",
        "    for window in [3, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
        "            if col in df.columns:\n",
        "                df[f'{col}_rolling_mean_{window}'] = (\n",
        "                    df.groupby(gcols)[col]\n",
        "                    .rolling(window, min_periods=1).mean()\n",
        "                    .reset_index(level=[0,1,2], drop=True)\n",
        "                )\n",
        "                df[f'{col}_rolling_std_{window}'] = (\n",
        "                    df.groupby(gcols)[col]\n",
        "                    .rolling(window, min_periods=1).std()\n",
        "                    .reset_index(level=[0,1,2], drop=True)\n",
        "                )\n",
        "\n",
        "    for col in ['velocity_x', 'velocity_y']:\n",
        "        if col in df.columns:\n",
        "            df[f'{col}_delta'] = df.groupby(gcols)[col].diff()\n",
        "\n",
        "    df['velocity_x_ema'] = df.groupby(gcols)['velocity_x'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "    df['velocity_y_ema'] = df.groupby(gcols)['velocity_y'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "    df['speed_ema'] = df.groupby(gcols)['s'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_opponent_features(input_df):\n",
        "    features = []\n",
        "\n",
        "    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']),\n",
        "                                desc=\"üèà Opponents\", leave=False):\n",
        "        last = group.sort_values('frame_id').groupby('nfl_id').last()\n",
        "\n",
        "        if len(last) < 2:\n",
        "            continue\n",
        "\n",
        "        positions = last[['x', 'y']].values\n",
        "        sides = last['player_side'].values\n",
        "        speeds = last['s'].values\n",
        "        directions = last['dir'].values\n",
        "        roles = last['player_role'].values\n",
        "\n",
        "        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n",
        "\n",
        "        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n",
        "            opp_mask = sides != side\n",
        "\n",
        "            feat = {\n",
        "                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
        "                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n",
        "                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n",
        "                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n",
        "                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n",
        "                'mirror_wr_dist': 50.0,\n",
        "            }\n",
        "\n",
        "            if not opp_mask.any():\n",
        "                features.append(feat)\n",
        "                continue\n",
        "\n",
        "            opp_positions = positions[opp_mask]\n",
        "            distances = np.sqrt(((positions[i] - opp_positions) ** 2).sum(axis=1))\n",
        "\n",
        "            if len(distances) == 0:\n",
        "                features.append(feat)\n",
        "                continue\n",
        "\n",
        "            nearest_idx = distances.argmin()\n",
        "            feat['nearest_opp_dist'] = distances[nearest_idx]\n",
        "            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n",
        "            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n",
        "\n",
        "            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n",
        "            opp_speeds = speeds[opp_mask]\n",
        "            opp_dirs = directions[opp_mask]\n",
        "            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n",
        "\n",
        "            rel_vx = my_vx - opp_vx\n",
        "            rel_vy = my_vy - opp_vy\n",
        "            to_me = positions[i] - opp_positions[nearest_idx]\n",
        "            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n",
        "            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n",
        "\n",
        "            if role == 'Defensive Coverage' and receiver_mask.any():\n",
        "                rec_positions = positions[receiver_mask]\n",
        "                rec_distances = np.sqrt(((positions[i] - rec_positions) ** 2).sum(axis=1))\n",
        "\n",
        "                if len(rec_distances) > 0:\n",
        "                    closest_rec_idx = rec_distances.argmin()\n",
        "                    rec_indices = np.where(receiver_mask)[0]\n",
        "                    actual_rec_idx = rec_indices[closest_rec_idx]\n",
        "\n",
        "                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n",
        "\n",
        "                    feat['mirror_wr_vx'] = rec_vx\n",
        "                    feat['mirror_wr_vy'] = rec_vy\n",
        "                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n",
        "                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n",
        "                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n",
        "\n",
        "            features.append(feat)\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "def extract_route_patterns(input_df, kmeans=None, scaler=None, fit=False):\n",
        "    route_features = []\n",
        "\n",
        "    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']),\n",
        "                                      desc=\"üõ£Ô∏è Routes\", leave=False):\n",
        "        traj = group.sort_values('frame_id').tail(5)\n",
        "\n",
        "        if len(traj) < 3:\n",
        "            continue\n",
        "\n",
        "        positions = traj[['x', 'y']].values\n",
        "        speeds = traj['s'].values\n",
        "\n",
        "        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n",
        "        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 +\n",
        "                               (positions[-1, 1] - positions[0, 1])**2)\n",
        "        straightness = displacement / (total_dist + 0.1)\n",
        "\n",
        "        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n",
        "        if len(angles) > 1:\n",
        "            angle_changes = np.abs(np.diff(angles))\n",
        "            max_turn = np.max(angle_changes)\n",
        "            mean_turn = np.mean(angle_changes)\n",
        "        else:\n",
        "            max_turn = mean_turn = 0\n",
        "\n",
        "        speed_mean = speeds.mean()\n",
        "        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n",
        "        dx = positions[-1, 0] - positions[0, 0]\n",
        "        dy = positions[-1, 1] - positions[0, 1]\n",
        "\n",
        "        route_features.append({\n",
        "            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
        "            'traj_straightness': straightness,\n",
        "            'traj_max_turn': max_turn,\n",
        "            'traj_mean_turn': mean_turn,\n",
        "            'traj_depth': abs(dx),\n",
        "            'traj_width': abs(dy),\n",
        "            'speed_mean': speed_mean,\n",
        "            'speed_change': speed_change,\n",
        "        })\n",
        "\n",
        "    route_df = pd.DataFrame(route_features)\n",
        "    if route_df.empty or 'traj_straightness' not in route_df.columns:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
        "                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n",
        "    X = route_df[feat_cols].fillna(0)\n",
        "\n",
        "    if kmeans is None or scaler is None:\n",
        "        return route_df\n",
        "\n",
        "    X_scaled = scaler.transform(X)\n",
        "    route_df['route_pattern'] = kmeans.predict(X_scaled)\n",
        "    return route_df\n",
        "\n",
        "def compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH,\n",
        "                                radius=Config.RADIUS, tau=Config.TAU):\n",
        "\n",
        "    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\",\n",
        "                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n",
        "    src = input_df[cols_needed].copy()\n",
        "\n",
        "    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
        "               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n",
        "               .tail(1)\n",
        "               .rename(columns={\"frame_id\": \"last_frame_id\"})\n",
        "               .reset_index(drop=True))\n",
        "\n",
        "    tmp = last.merge(\n",
        "        src.rename(columns={\n",
        "            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n",
        "            \"x\": \"x_nb\", \"y\": \"y_nb\",\n",
        "            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\",\n",
        "            \"player_side\": \"player_side_nb\"\n",
        "        }),\n",
        "        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n",
        "        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n",
        "    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n",
        "    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n",
        "    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n",
        "    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n",
        "    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n",
        "\n",
        "    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n",
        "    if radius is not None:\n",
        "        tmp = tmp[tmp[\"dist\"] <= radius]\n",
        "\n",
        "    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n",
        "\n",
        "    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
        "    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n",
        "    if k_neigh is not None:\n",
        "        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n",
        "\n",
        "    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n",
        "    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n",
        "    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n",
        "\n",
        "    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n",
        "    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n",
        "\n",
        "    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n",
        "        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n",
        "        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n",
        "\n",
        "    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n",
        "    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n",
        "\n",
        "    ag = tmp.groupby(keys).agg(\n",
        "        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n",
        "        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n",
        "        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n",
        "        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n",
        "        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n",
        "        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n",
        "        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n",
        "        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n",
        "        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n",
        "        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n",
        "        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n",
        "        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n",
        "        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n",
        "        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n",
        "    if len(near) > 0:\n",
        "        near[\"rnk\"] = near[\"rnk\"].astype(int)\n",
        "        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n",
        "        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n",
        "        ag = ag.merge(dwide, on=keys, how=\"left\")\n",
        "\n",
        "    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n",
        "              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n",
        "        ag[c] = ag[c].fillna(0.0)\n",
        "    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n",
        "        ag[c] = ag[c].fillna(0.0)\n",
        "    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\",\n",
        "              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n",
        "        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n",
        "\n",
        "    return ag\n",
        "\n",
        "def compute_geometric_endpoint(df):\n",
        "    \"\"\"Âü∫‰∫éÂá†‰ΩïËßÑÂàôËÆ°ÁÆóÊØè‰∏™ÁêÉÂëòÁöÑÁªàÁÇπ‰ΩçÁΩÆ\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    if 'num_frames_output' in df.columns:\n",
        "        t_total = df['num_frames_output'] / 10.0\n",
        "    else:\n",
        "        t_total = 3.0\n",
        "\n",
        "    df['time_to_endpoint'] = t_total\n",
        "\n",
        "    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n",
        "    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n",
        "\n",
        "    if 'ball_land_x' in df.columns:\n",
        "        receiver_mask = df['player_role'] == 'Targeted Receiver'\n",
        "        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n",
        "        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n",
        "\n",
        "        defender_mask = df['player_role'] == 'Defensive Coverage'\n",
        "        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n",
        "        coverage_mask = defender_mask & has_mirror\n",
        "\n",
        "        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n",
        "            df.loc[coverage_mask, 'ball_land_x'] +\n",
        "            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n",
        "        )\n",
        "        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n",
        "            df.loc[coverage_mask, 'ball_land_y'] +\n",
        "            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n",
        "        )\n",
        "\n",
        "    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
        "    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_geometric_features(df):\n",
        "    df = compute_geometric_endpoint(df)\n",
        "\n",
        "    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n",
        "    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n",
        "    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n",
        "\n",
        "    t = df['time_to_endpoint'] + 0.1\n",
        "    df['geo_required_vx'] = df['geo_vector_x'] / t\n",
        "    df['geo_required_vy'] = df['geo_vector_y'] / t\n",
        "\n",
        "    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n",
        "    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n",
        "    df['geo_velocity_error'] = np.sqrt(\n",
        "        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n",
        "    )\n",
        "\n",
        "    t_sq = t * t\n",
        "    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n",
        "    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n",
        "    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n",
        "    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n",
        "\n",
        "    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n",
        "    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n",
        "    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n",
        "    df['geo_alignment'] = (\n",
        "        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n",
        "    ) / (velocity_mag + 0.1)\n",
        "\n",
        "    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n",
        "    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_advanced_features(df):\n",
        "    df = df.copy()\n",
        "    df = df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
        "\n",
        "    if 'distance_to_ball' in df.columns:\n",
        "        df['distance_to_ball_change'] = df.groupby(gcols)['distance_to_ball'].diff().fillna(0)\n",
        "        df['distance_to_ball_accel'] = df.groupby(gcols)['distance_to_ball_change'].diff().fillna(0)\n",
        "        df['time_to_intercept'] = (df['distance_to_ball'] /\n",
        "                                  (np.abs(df['distance_to_ball_change']) + 0.1)).clip(0, 10)\n",
        "\n",
        "    if 'ball_direction_x' in df.columns:\n",
        "        df['velocity_alignment'] = (\n",
        "            df['velocity_x'] * df['ball_direction_x'] +\n",
        "            df['velocity_y'] * df['ball_direction_y']\n",
        "        )\n",
        "        df['velocity_perpendicular'] = (\n",
        "            df['velocity_x'] * (-df['ball_direction_y']) +\n",
        "            df['velocity_y'] * df['ball_direction_x']\n",
        "        )\n",
        "        if 'acceleration_x' in df.columns:\n",
        "            df['accel_alignment'] = (\n",
        "                df['acceleration_x'] * df['ball_direction_x'] +\n",
        "                df['acceleration_y'] * df['ball_direction_y']\n",
        "            )\n",
        "\n",
        "    if 'velocity_x' in df.columns:\n",
        "        df['velocity_x_change'] = df.groupby(gcols)['velocity_x'].diff().fillna(0)\n",
        "        df['velocity_y_change'] = df.groupby(gcols)['velocity_y'].diff().fillna(0)\n",
        "        df['speed_change'] = df.groupby(gcols)['s'].diff().fillna(0)\n",
        "        df['direction_change'] = df.groupby(gcols)['dir'].diff().fillna(0)\n",
        "        df['direction_change'] = df['direction_change'].apply(\n",
        "            lambda x: x if abs(x) < 180 else x - 360 * np.sign(x)\n",
        "        )\n",
        "\n",
        "    df['dist_from_left'] = df['y']\n",
        "    df['dist_from_right'] = 53.3 - df['y']\n",
        "    df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n",
        "    df['dist_from_endzone'] = np.minimum(df['x'], 120 - df['x'])\n",
        "\n",
        "    if 'is_receiver' in df.columns and 'velocity_alignment' in df.columns:\n",
        "        df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n",
        "        df['receiver_deviation'] = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0))\n",
        "    if 'is_coverage' in df.columns and 'closing_speed' in df.columns:\n",
        "        df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n",
        "\n",
        "    df['frames_elapsed'] = df.groupby(gcols).cumcount()\n",
        "    df['normalized_time'] = df.groupby(gcols)['frames_elapsed'].transform(\n",
        "        lambda x: x / (x.max() + 1)\n",
        "    )\n",
        "\n",
        "    if 'nearest_opp_dist' in df.columns:\n",
        "        df['pressure'] = 1 / np.maximum(df['nearest_opp_dist'], 0.5)\n",
        "        df['under_pressure'] = (df['nearest_opp_dist'] < 3).astype(int)\n",
        "        df['pressure_x_speed'] = df['pressure'] * df['s']\n",
        "\n",
        "    if 'mirror_wr_vx' in df.columns:\n",
        "        s_safe = np.maximum(df['s'], 0.1)\n",
        "        df['mirror_similarity'] = (\n",
        "                df['velocity_x'] * df['mirror_wr_vx'] +\n",
        "                df['velocity_y'] * df['mirror_wr_vy']\n",
        "        ) / s_safe\n",
        "        df['mirror_offset_dist'] = np.sqrt(\n",
        "            df['mirror_offset_x'] ** 2 + df['mirror_offset_y'] ** 2\n",
        "        )\n",
        "        df['mirror_alignment'] = df['mirror_similarity'] * df['is_coverage']\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_time_features(df):\n",
        "    if 'num_frames_output' not in df.columns:\n",
        "        return df\n",
        "\n",
        "    max_frames = df['num_frames_output']\n",
        "\n",
        "    df['max_play_duration'] = max_frames / 10.0\n",
        "    df['frame_time'] = df['frame_id'] / 10.0\n",
        "    df['progress_ratio'] = df['frame_id'] / np.maximum(max_frames, 1)\n",
        "    df['time_remaining'] = (max_frames - df['frame_id']) / 10.0\n",
        "    df['frames_remaining'] = max_frames - df['frame_id']\n",
        "\n",
        "    df['expected_x_at_ball'] = df['x'] + df['velocity_x'] * df['frame_time']\n",
        "    df['expected_y_at_ball'] = df['y'] + df['velocity_y'] * df['frame_time']\n",
        "\n",
        "    if 'ball_land_x' in df.columns:\n",
        "        df['error_from_ball_x'] = df['expected_x_at_ball'] - df['ball_land_x']\n",
        "        df['error_from_ball_y'] = df['expected_y_at_ball'] - df['ball_land_y']\n",
        "        df['error_from_ball'] = np.sqrt(\n",
        "            df['error_from_ball_x']**2 + df['error_from_ball_y']**2\n",
        "        )\n",
        "\n",
        "        df['weighted_dist_by_time'] = df['dist_to_ball'] / (df['frame_time'] + 0.1)\n",
        "        df['dist_scaled_by_progress'] = df['dist_to_ball'] * (1 - df['progress_ratio'])\n",
        "\n",
        "    df['time_squared'] = df['frame_time'] ** 2\n",
        "    df['velocity_x_progress'] = df['velocity_x'] * df['progress_ratio']\n",
        "    df['velocity_y_progress'] = df['velocity_y'] * df['progress_ratio']\n",
        "    df['speed_scaled_by_time_left'] = df['s'] * df['time_remaining']\n",
        "\n",
        "    df['actual_play_length'] = max_frames\n",
        "    df['length_ratio'] = max_frames / 30.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def get_feature_columns(df):\n",
        "    base_feature_cols = [\n",
        "        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id',\n",
        "        'ball_land_x', 'ball_land_y',\n",
        "        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n",
        "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
        "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
        "        'speed_squared', 'accel_magnitude', 'orientation_diff',\n",
        "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
        "        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer', 'side_offense',\n",
        "        'distance_to_ball', 'dist_to_ball', 'dist_squared', 'angle_to_ball',\n",
        "        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n",
        "        'velocity_toward_ball', 'velocity_alignment', 'angle_diff',\n",
        "    ]\n",
        "\n",
        "    opponent_cols = [\n",
        "        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n",
        "        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y', 'mirror_wr_dist',\n",
        "    ]\n",
        "\n",
        "    route_cols = [\n",
        "        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
        "        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n",
        "    ]\n",
        "\n",
        "    gnn_cols = [\n",
        "        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n",
        "        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n",
        "        'gnn_ally_cnt', 'gnn_opp_cnt', 'gnn_ally_dmin', 'gnn_ally_dmean',\n",
        "        'gnn_opp_dmin', 'gnn_opp_dmean', 'gnn_d1', 'gnn_d2', 'gnn_d3',\n",
        "    ]\n",
        "\n",
        "    temporal_cols = []\n",
        "    for lag in [1, 2, 3, 4, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
        "            temporal_cols.append(f'{col}_lag{lag}')\n",
        "\n",
        "    for window in [3, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
        "            temporal_cols.append(f'{col}_rolling_mean_{window}')\n",
        "            temporal_cols.append(f'{col}_rolling_std_{window}')\n",
        "\n",
        "    temporal_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n",
        "    temporal_cols.extend(['velocity_x_ema', 'velocity_y_ema', 'speed_ema'])\n",
        "\n",
        "    time_cols = [\n",
        "        'max_play_duration', 'frame_time', 'progress_ratio', 'time_remaining', 'frames_remaining',\n",
        "        'expected_x_at_ball', 'expected_y_at_ball',\n",
        "        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n",
        "        'time_squared', 'weighted_dist_by_time',\n",
        "        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n",
        "        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n",
        "    ]\n",
        "\n",
        "    advanced_cols = [\n",
        "        'distance_to_ball_change', 'distance_to_ball_accel', 'time_to_intercept',\n",
        "        'velocity_alignment', 'velocity_perpendicular', 'accel_alignment',\n",
        "        'velocity_x_change', 'velocity_y_change', 'speed_change', 'direction_change',\n",
        "        'dist_from_sideline', 'dist_from_endzone',\n",
        "        'receiver_optimality', 'receiver_deviation', 'defender_closing_speed',\n",
        "        'frames_elapsed', 'normalized_time',\n",
        "        'pressure', 'under_pressure', 'pressure_x_speed',\n",
        "        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment'\n",
        "    ]\n",
        "\n",
        "    geometric_cols = [\n",
        "        'geo_endpoint_x', 'geo_endpoint_y',\n",
        "        'geo_vector_x', 'geo_vector_y', 'geo_distance',\n",
        "        'geo_required_vx', 'geo_required_vy',\n",
        "        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n",
        "        'geo_required_ax', 'geo_required_ay',\n",
        "        'geo_alignment', 'geo_receiver_urgency', 'geo_defender_coupling'\n",
        "    ]\n",
        "\n",
        "    all_feature_cols = (base_feature_cols + opponent_cols + route_cols + gnn_cols +\n",
        "                       temporal_cols + time_cols + advanced_cols + geometric_cols)\n",
        "\n",
        "    return [c for c in all_feature_cols if c in df.columns]\n",
        "\n",
        "def wrap_angle_deg(s):\n",
        "    return ((s + 180.0) % 360.0) - 180.0\n",
        "\n",
        "def unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    if 'play_direction' not in df.columns:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    right = df['play_direction'].eq('right')\n",
        "    if 'x' in df.columns: df.loc[right, 'x'] = Config.FIELD_X_MAX - df.loc[right, 'x']\n",
        "    if 'y' in df.columns: df.loc[right, 'y'] = Config.FIELD_Y_MAX - df.loc[right, 'y']\n",
        "    for col in ('dir','o'):\n",
        "        if col in df.columns:\n",
        "            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n",
        "    if 'ball_land_x' in df.columns:\n",
        "        df.loc[right, 'ball_land_x'] = Config.FIELD_X_MAX - df.loc[right, 'ball_land_x']\n",
        "    if 'ball_land_y' in df.columns:\n",
        "        df.loc[right, 'ball_land_y'] = Config.FIELD_Y_MAX - df.loc[right, 'ball_land_y']\n",
        "    return df\n",
        "\n",
        "def build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n",
        "    s = (\n",
        "        df_in[['game_id','play_id','play_direction']]\n",
        "        .drop_duplicates()\n",
        "        .set_index(['game_id','play_id'])['play_direction']\n",
        "    )\n",
        "    return s\n",
        "\n",
        "def apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n",
        "    if 'play_direction' not in df.columns:\n",
        "        dir_df = dir_map.reset_index()\n",
        "        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n",
        "    return unify_left_direction(df)\n",
        "\n",
        "def invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n",
        "    if not play_dir_right:\n",
        "        return float(x_u), float(y_u)\n",
        "    return float(Config.FIELD_X_MAX - x_u), float(Config.FIELD_Y_MAX - y_u)\n",
        "\n",
        "def prepare_sequences_fixed(input_df, output_df=None, test_template=None,\n",
        "                           is_training=False, window_size=Config.WINDOW_SIZE,\n",
        "                           route_kmeans=None, route_scaler=None):\n",
        "    dir_map = build_play_direction_map(input_df)\n",
        "    input_df_u = apply_direction_to_df(input_df, dir_map)\n",
        "\n",
        "    if is_training:\n",
        "        out_u = apply_direction_to_df(output_df, dir_map)\n",
        "        target_rows = out_u\n",
        "        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n",
        "    else:\n",
        "        if 'play_direction' not in test_template.columns:\n",
        "            dir_df = dir_map.reset_index()\n",
        "            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n",
        "        target_rows = test_template\n",
        "        target_groups = target_rows[['game_id','play_id','nfl_id']].drop_duplicates()\n",
        "\n",
        "    input_df_u = create_base_features(input_df_u)\n",
        "    input_df_u = create_lag_features(input_df_u, window_size)\n",
        "    opponent_features = get_opponent_features(input_df_u)\n",
        "    input_df_u = input_df_u.merge(opponent_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "    route_features = extract_route_patterns(input_df_u, route_kmeans, route_scaler, fit=False)\n",
        "    if not route_features.empty:\n",
        "        input_df_u = input_df_u.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "    gnn_features = compute_neighbor_embeddings(input_df_u)\n",
        "    input_df_u = input_df_u.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "    input_df_u = add_advanced_features(input_df_u)\n",
        "    input_df_u = add_time_features(input_df_u)\n",
        "    input_df_u = add_geometric_features(input_df_u)\n",
        "    feature_cols = get_feature_columns(input_df_u)\n",
        "\n",
        "    input_df_u.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
        "    grouped = input_df_u.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
        "\n",
        "    sequences, sequence_ids = [], []\n",
        "    geo_endpoints_x, geo_endpoints_y = [], []\n",
        "\n",
        "    for _, row in tqdm(target_groups.iterrows(), desc=\"sequences ing\"):\n",
        "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
        "        try:\n",
        "            group_df = grouped.get_group(key)\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        input_window = group_df.tail(window_size)\n",
        "        if len(input_window) < window_size:\n",
        "            pad_len = window_size - len(input_window)\n",
        "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
        "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
        "\n",
        "        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
        "        seq = input_window[feature_cols].values\n",
        "\n",
        "        if np.isnan(seq).any():\n",
        "            seq = np.nan_to_num(seq, nan=0.0)\n",
        "\n",
        "        sequences.append(seq)\n",
        "\n",
        "        geo_x = input_window.iloc[-1]['geo_endpoint_x']\n",
        "        geo_y = input_window.iloc[-1]['geo_endpoint_y']\n",
        "        geo_endpoints_x.append(geo_x)\n",
        "        geo_endpoints_y.append(geo_y)\n",
        "\n",
        "        sequence_ids.append({\n",
        "            'game_id': key[0],\n",
        "            'play_id': key[1],\n",
        "            'nfl_id': key[2],\n",
        "            'frame_id': input_window.iloc[-1]['frame_id'],\n",
        "            'play_direction': input_window.iloc[-1]['play_direction'],\n",
        "            'last_x': input_window.iloc[-1]['x'],\n",
        "            'last_y': input_window.iloc[-1]['y']\n",
        "        })\n",
        "\n",
        "    return sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y, feature_cols\n",
        "\n",
        "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame:\n",
        "\n",
        "    test_pd = test.to_pandas()\n",
        "    test_input_pd = test_input.to_pandas()\n",
        "\n",
        "    MODEL_DIR = Path(\"/kaggle/input/1103new-all-all-all/pytorch/default/1/1103new_all_all_all\")\n",
        "    try:\n",
        "        route_kmeans = joblib.load(MODEL_DIR / \"route_kmeans.pkl\")\n",
        "        route_scaler = joblib.load(MODEL_DIR / \"route_scaler.pkl\")\n",
        "    except:\n",
        "        route_kmeans = None\n",
        "        route_scaler = None\n",
        "\n",
        "    sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y, feature_cols = prepare_sequences_fixed(\n",
        "        test_input_pd, test_template=test_pd, is_training=False,\n",
        "        window_size=Config.WINDOW_SIZE, route_kmeans=route_kmeans, route_scaler=route_scaler\n",
        "    )\n",
        "\n",
        "    if not sequences:\n",
        "        return pl.DataFrame({\"x\": [], \"y\": []})\n",
        "\n",
        "    X_test = np.array(sequences, dtype=object)\n",
        "\n",
        "    x_last_u = np.array([s[-1, 0] for s in X_test])\n",
        "    y_last_u = np.array([s[-1, 1] for s in X_test])\n",
        "\n",
        "    model_x_paths = [MODEL_DIR / f\"model_x_fold{i+1}.pth\" for i in range(Config.N_FOLDS)]\n",
        "    model_y_paths = [MODEL_DIR / f\"model_y_fold{i+1}.pth\" for i in range(Config.N_FOLDS)]\n",
        "    scaler_paths = [MODEL_DIR / f\"scaler_fold{i+1}.pkl\" for i in range(Config.N_FOLDS)]\n",
        "\n",
        "    for p in model_x_paths + model_y_paths + scaler_paths:\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f\"loss: {p.name}\")\n",
        "\n",
        "    models_x = []\n",
        "    models_y = []\n",
        "    scalers = []\n",
        "\n",
        "    for i in range(Config.N_FOLDS):\n",
        "\n",
        "        model_x = ImprovedSeqModel(input_dim=X_test[0].shape[1], horizon=Config.MAX_FUTURE_HORIZON).to(Config.DEVICE)\n",
        "        model_x.load_state_dict(torch.load(model_x_paths[i], map_location=Config.DEVICE))\n",
        "        model_x.eval()\n",
        "        models_x.append(model_x)\n",
        "\n",
        "        model_y = ImprovedSeqModel(input_dim=X_test[0].shape[1], horizon=Config.MAX_FUTURE_HORIZON).to(Config.DEVICE)\n",
        "        model_y.load_state_dict(torch.load(model_y_paths[i], map_location=Config.DEVICE))\n",
        "        model_y.eval()\n",
        "        models_y.append(model_y)\n",
        "\n",
        "        scaler = joblib.load(scaler_paths[i])\n",
        "        scalers.append(scaler)\n",
        "\n",
        "    all_dx, all_dy = [], []\n",
        "    for mx, my, sc in zip(models_x, models_y, scalers):\n",
        "        X_scaled = np.stack([sc.transform(s) for s in X_test])\n",
        "        X_tensor = torch.tensor(X_scaled.astype(np.float32)).to(Config.DEVICE)\n",
        "\n",
        "        mx.eval()\n",
        "        my.eval()\n",
        "        with torch.no_grad():\n",
        "            dx = mx(X_tensor).cpu().numpy()\n",
        "            dy = my(X_tensor).cpu().numpy()\n",
        "        all_dx.append(dx)\n",
        "        all_dy.append(dy)\n",
        "\n",
        "    ens_dx = np.mean(all_dx, axis=0)\n",
        "    ens_dy = np.mean(all_dy, axis=0)\n",
        "\n",
        "    rows = []\n",
        "    H = ens_dx.shape[1]\n",
        "\n",
        "    for i, sid in enumerate(sequence_ids):\n",
        "        fids = test_pd[\n",
        "            (test_pd['game_id'] == sid['game_id']) &\n",
        "            (test_pd['play_id'] == sid['play_id']) &\n",
        "            (test_pd['nfl_id'] == sid['nfl_id'])\n",
        "        ]['frame_id'].sort_values().tolist()\n",
        "\n",
        "        play_dir_right = (sid['play_direction'] == 'right')\n",
        "\n",
        "        for t, fid in enumerate(fids):\n",
        "            tt = min(t, H - 1)\n",
        "\n",
        "            x_u = np.clip(x_last_u[i] + ens_dx[i, tt], 0, 120)\n",
        "            y_u = np.clip(y_last_u[i] + ens_dy[i, tt], 0, 53.3)\n",
        "\n",
        "            x_orig, y_orig = invert_to_original_direction(x_u, y_u, play_dir_right)\n",
        "\n",
        "            rows.append({\n",
        "                'x': float(x_orig),\n",
        "                'y': float(y_orig)\n",
        "            })\n",
        "\n",
        "    return pl.DataFrame(rows)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T14:30:15.936111Z",
          "iopub.execute_input": "2025-12-02T14:30:15.936587Z",
          "iopub.status.idle": "2025-12-02T14:30:15.962626Z",
          "shell.execute_reply.started": "2025-12-02T14:30:15.936563Z",
          "shell.execute_reply": "2025-12-02T14:30:15.961829Z"
        },
        "id": "iq__ayuged9S"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nfl_gnn.py\n",
        "\n",
        "\"\"\"\n",
        "NFL BIG DATA BOWL 2026 - GEOMETRIC NEURAL BREAKTHROUGH\n",
        "üèÜ The Winning Architecture\n",
        "\n",
        "INSIGHT: Football players follow geometric rules with learned corrections\n",
        "- Receivers ‚Üí Ball landing (geometric)\n",
        "- Defenders ‚Üí Mirror receivers (geometric coupling)\n",
        "- Others ‚Üí Momentum (physics)\n",
        "- Model learns only CORRECTIONS for coverage, collisions, boundaries\n",
        "\n",
        "Architecture:\n",
        "‚úì Proven GRU + Attention (your 0.59 base)\n",
        "‚úì 154 proven features (unchanged)\n",
        "‚úì +15 geometric features (our discovery)\n",
        "‚úì Train on corrections to geometric baseline (the breakthrough)\n",
        "\n",
        "Target: 0.54-0.56 LB\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "import warnings\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.cluster import KMeans\n",
        "from multiprocessing import Pool as MultiprocessingPool, cpu_count\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIG\n",
        "# ============================================================================\n",
        "\n",
        "class Config:\n",
        "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n",
        "    OUTPUT_DIR = Path(\"./outputs\")\n",
        "    OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "    # Where to persist trained artifacts (models, scalers, route objects)\n",
        "    MODEL_DIR = OUTPUT_DIR / \"models\"\n",
        "    MODEL_DIR.mkdir(exist_ok=True)\n",
        "    # Toggle saving/loading of artifacts\n",
        "    SAVE_ARTIFACTS = True\n",
        "    LOAD_ARTIFACTS = True\n",
        "    LOAD_DIR = '/kaggle/input/nfl-gnn-a43/outputs/models'\n",
        "\n",
        "    SEED = 42\n",
        "    N_FOLDS = 10\n",
        "    BATCH_SIZE = 256\n",
        "    EPOCHS = 200\n",
        "    PATIENCE = 30\n",
        "    LEARNING_RATE = 1e-3\n",
        "\n",
        "    WINDOW_SIZE = 10\n",
        "    HIDDEN_DIM = 128\n",
        "    MAX_FUTURE_HORIZON = 94\n",
        "\n",
        "    # === Transformer Ë∂ÖÂèÇÊï∞ ===\n",
        "    N_HEADS = 4  # Transformer ÁöÑÊ≥®ÊÑèÂäõÂ§¥Êï∞\n",
        "    N_LAYERS = 2 # Transformer ÁºñÁ†ÅÂô®ÁöÑÂ±ÇÊï∞\n",
        "\n",
        "    # === Êñ∞Â¢ûÔºöResidualMLP Head Ë∂ÖÂèÇÊï∞ ===\n",
        "    MLP_HIDDEN_DIM = 256 # MLP Â§¥ÁöÑÂÜÖÈÉ®ÈöêËóèÁª¥Â∫¶\n",
        "    N_RES_BLOCKS = 2     # ÊÆãÂ∑ÆÂùóÁöÑÊï∞Èáè\n",
        "    # ==================================\n",
        "\n",
        "    FIELD_X_MIN, FIELD_X_MAX = 0.0, 120.0\n",
        "    FIELD_Y_MIN, FIELD_Y_MAX = 0.0, 53.3\n",
        "\n",
        "    K_NEIGH = 6\n",
        "    RADIUS = 30.0\n",
        "    TAU = 8.0\n",
        "    N_ROUTE_CLUSTERS = 7\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    DEBUG = False\n",
        "    if DEBUG:\n",
        "        N_FOLDS = 2\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "set_seed(Config.SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# GEOMETRIC BASELINE - THE BREAKTHROUGH\n",
        "# ============================================================================\n",
        "\n",
        "def compute_geometric_endpoint(df):\n",
        "    \"\"\"\n",
        "    Compute where each player SHOULD end up based on geometry.\n",
        "    This is the deterministic part - no learning needed.\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Time to play end\n",
        "    if 'num_frames_output' in df.columns:\n",
        "        t_total = df['num_frames_output'] / 10.0\n",
        "    else:\n",
        "        t_total = 3.0\n",
        "\n",
        "    df['time_to_endpoint'] = t_total\n",
        "\n",
        "    # Initialize with momentum (default rule)\n",
        "    df['geo_endpoint_x'] = df['x'] + df['velocity_x'] * t_total\n",
        "    df['geo_endpoint_y'] = df['y'] + df['velocity_y'] * t_total\n",
        "\n",
        "    # Rule 1: Targeted Receivers converge to ball\n",
        "    if 'ball_land_x' in df.columns:\n",
        "        receiver_mask = df['player_role'] == 'Targeted Receiver'\n",
        "        df.loc[receiver_mask, 'geo_endpoint_x'] = df.loc[receiver_mask, 'ball_land_x']\n",
        "        df.loc[receiver_mask, 'geo_endpoint_y'] = df.loc[receiver_mask, 'ball_land_y']\n",
        "\n",
        "        # Rule 2: Defenders mirror receivers (maintain offset)\n",
        "        defender_mask = df['player_role'] == 'Defensive Coverage'\n",
        "        has_mirror = df.get('mirror_offset_x', 0).notna() & (df.get('mirror_wr_dist', 50) < 15)\n",
        "        coverage_mask = defender_mask & has_mirror\n",
        "\n",
        "        df.loc[coverage_mask, 'geo_endpoint_x'] = (\n",
        "            df.loc[coverage_mask, 'ball_land_x'] +\n",
        "            df.loc[coverage_mask, 'mirror_offset_x'].fillna(0)\n",
        "        )\n",
        "        df.loc[coverage_mask, 'geo_endpoint_y'] = (\n",
        "            df.loc[coverage_mask, 'ball_land_y'] +\n",
        "            df.loc[coverage_mask, 'mirror_offset_y'].fillna(0)\n",
        "        )\n",
        "\n",
        "    # Clip to field\n",
        "    df['geo_endpoint_x'] = df['geo_endpoint_x'].clip(Config.FIELD_X_MIN, Config.FIELD_X_MAX)\n",
        "    df['geo_endpoint_y'] = df['geo_endpoint_y'].clip(Config.FIELD_Y_MIN, Config.FIELD_Y_MAX)\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_geometric_features(df):\n",
        "    \"\"\"Add features that describe the geometric solution\"\"\"\n",
        "    df = compute_geometric_endpoint(df)\n",
        "\n",
        "    # Vector to geometric endpoint\n",
        "    df['geo_vector_x'] = df['geo_endpoint_x'] - df['x']\n",
        "    df['geo_vector_y'] = df['geo_endpoint_y'] - df['y']\n",
        "    df['geo_distance'] = np.sqrt(df['geo_vector_x']**2 + df['geo_vector_y']**2)\n",
        "\n",
        "    # Required velocity to reach geometric endpoint\n",
        "    t = df['time_to_endpoint'] + 0.1\n",
        "    df['geo_required_vx'] = df['geo_vector_x'] / t\n",
        "    df['geo_required_vy'] = df['geo_vector_y'] / t\n",
        "\n",
        "    # Current velocity vs required\n",
        "    df['geo_velocity_error_x'] = df['geo_required_vx'] - df['velocity_x']\n",
        "    df['geo_velocity_error_y'] = df['geo_required_vy'] - df['velocity_y']\n",
        "    df['geo_velocity_error'] = np.sqrt(\n",
        "        df['geo_velocity_error_x']**2 + df['geo_velocity_error_y']**2\n",
        "    )\n",
        "\n",
        "    # Required constant acceleration (a = 2*Œîx/t¬≤)\n",
        "    t_sq = t * t\n",
        "    df['geo_required_ax'] = 2 * df['geo_vector_x'] / t_sq\n",
        "    df['geo_required_ay'] = 2 * df['geo_vector_y'] / t_sq\n",
        "    df['geo_required_ax'] = df['geo_required_ax'].clip(-10, 10)\n",
        "    df['geo_required_ay'] = df['geo_required_ay'].clip(-10, 10)\n",
        "\n",
        "    # Alignment with geometric path\n",
        "    velocity_mag = np.sqrt(df['velocity_x']**2 + df['velocity_y']**2)\n",
        "    geo_unit_x = df['geo_vector_x'] / (df['geo_distance'] + 0.1)\n",
        "    geo_unit_y = df['geo_vector_y'] / (df['geo_distance'] + 0.1)\n",
        "    df['geo_alignment'] = (\n",
        "        df['velocity_x'] * geo_unit_x + df['velocity_y'] * geo_unit_y\n",
        "    ) / (velocity_mag + 0.1)\n",
        "\n",
        "    # Role-specific geometric quality\n",
        "    df['geo_receiver_urgency'] = df['is_receiver'] * df['geo_distance'] / (t + 0.1)\n",
        "    df['geo_defender_coupling'] = df['is_coverage'] * (1.0 / (df.get('mirror_wr_dist', 50) + 1.0))\n",
        "\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# PROVEN FEATURE ENGINEERING (YOUR 0.59 BASE)\n",
        "# ============================================================================\n",
        "\n",
        "def get_velocity(speed, direction_deg):\n",
        "    theta = np.deg2rad(direction_deg)\n",
        "    return speed * np.sin(theta), speed * np.cos(theta)\n",
        "\n",
        "def height_to_feet(height_str):\n",
        "    try:\n",
        "        ft, inches = map(int, str(height_str).split('-'))\n",
        "        return ft + inches/12\n",
        "    except:\n",
        "        return 6.0\n",
        "\n",
        "def get_opponent_features(input_df):\n",
        "    \"\"\"Enhanced opponent interaction with MIRROR WR tracking\"\"\"\n",
        "    features = []\n",
        "\n",
        "    for (gid, pid), group in tqdm(input_df.groupby(['game_id', 'play_id']),\n",
        "                                   desc=\"üèà Opponents\", leave=False):\n",
        "        last = group.sort_values('frame_id').groupby('nfl_id').last()\n",
        "\n",
        "        if len(last) < 2:\n",
        "            continue\n",
        "\n",
        "        positions = last[['x', 'y']].values\n",
        "        sides = last['player_side'].values\n",
        "        speeds = last['s'].values\n",
        "        directions = last['dir'].values\n",
        "        roles = last['player_role'].values\n",
        "\n",
        "        receiver_mask = np.isin(roles, ['Targeted Receiver', 'Other Route Runner'])\n",
        "\n",
        "        for i, (nid, side, role) in enumerate(zip(last.index, sides, roles)):\n",
        "            opp_mask = sides != side\n",
        "\n",
        "            feat = {\n",
        "                'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
        "                'nearest_opp_dist': 50.0, 'closing_speed': 0.0,\n",
        "                'num_nearby_opp_3': 0, 'num_nearby_opp_5': 0,\n",
        "                'mirror_wr_vx': 0.0, 'mirror_wr_vy': 0.0,\n",
        "                'mirror_offset_x': 0.0, 'mirror_offset_y': 0.0,\n",
        "                'mirror_wr_dist': 50.0,\n",
        "            }\n",
        "\n",
        "            if not opp_mask.any():\n",
        "                features.append(feat)\n",
        "                continue\n",
        "\n",
        "            opp_positions = positions[opp_mask]\n",
        "            distances = np.sqrt(((positions[i] - opp_positions)**2).sum(axis=1))\n",
        "\n",
        "            if len(distances) == 0:\n",
        "                features.append(feat)\n",
        "                continue\n",
        "\n",
        "            nearest_idx = distances.argmin()\n",
        "            feat['nearest_opp_dist'] = distances[nearest_idx]\n",
        "            feat['num_nearby_opp_3'] = (distances < 3.0).sum()\n",
        "            feat['num_nearby_opp_5'] = (distances < 5.0).sum()\n",
        "\n",
        "            my_vx, my_vy = get_velocity(speeds[i], directions[i])\n",
        "            opp_speeds = speeds[opp_mask]\n",
        "            opp_dirs = directions[opp_mask]\n",
        "            opp_vx, opp_vy = get_velocity(opp_speeds[nearest_idx], opp_dirs[nearest_idx])\n",
        "\n",
        "            rel_vx = my_vx - opp_vx\n",
        "            rel_vy = my_vy - opp_vy\n",
        "            to_me = positions[i] - opp_positions[nearest_idx]\n",
        "            to_me_norm = to_me / (np.linalg.norm(to_me) + 0.1)\n",
        "            feat['closing_speed'] = -(rel_vx * to_me_norm[0] + rel_vy * to_me_norm[1])\n",
        "\n",
        "            if role == 'Defensive Coverage' and receiver_mask.any():\n",
        "                rec_positions = positions[receiver_mask]\n",
        "                rec_distances = np.sqrt(((positions[i] - rec_positions)**2).sum(axis=1))\n",
        "\n",
        "                if len(rec_distances) > 0:\n",
        "                    closest_rec_idx = rec_distances.argmin()\n",
        "                    rec_indices = np.where(receiver_mask)[0]\n",
        "                    actual_rec_idx = rec_indices[closest_rec_idx]\n",
        "\n",
        "                    rec_vx, rec_vy = get_velocity(speeds[actual_rec_idx], directions[actual_rec_idx])\n",
        "\n",
        "                    feat['mirror_wr_vx'] = rec_vx\n",
        "                    feat['mirror_wr_vy'] = rec_vy\n",
        "                    feat['mirror_wr_dist'] = rec_distances[closest_rec_idx]\n",
        "                    feat['mirror_offset_x'] = positions[i][0] - rec_positions[closest_rec_idx][0]\n",
        "                    feat['mirror_offset_y'] = positions[i][1] - rec_positions[closest_rec_idx][1]\n",
        "\n",
        "            features.append(feat)\n",
        "\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "def extract_route_patterns(input_df, kmeans=None, scaler=None, fit=True):\n",
        "    \"\"\"Route clustering\"\"\"\n",
        "    route_features = []\n",
        "\n",
        "    for (gid, pid, nid), group in tqdm(input_df.groupby(['game_id', 'play_id', 'nfl_id']),\n",
        "                                        desc=\"üõ£Ô∏è  Routes\", leave=False):\n",
        "        traj = group.sort_values('frame_id').tail(5)\n",
        "\n",
        "        if len(traj) < 3:\n",
        "            continue\n",
        "\n",
        "        positions = traj[['x', 'y']].values\n",
        "        speeds = traj['s'].values\n",
        "\n",
        "        total_dist = np.sum(np.sqrt(np.diff(positions[:, 0])**2 + np.diff(positions[:, 1])**2))\n",
        "        displacement = np.sqrt((positions[-1, 0] - positions[0, 0])**2 +\n",
        "                              (positions[-1, 1] - positions[0, 1])**2)\n",
        "        straightness = displacement / (total_dist + 0.1)\n",
        "\n",
        "        angles = np.arctan2(np.diff(positions[:, 1]), np.diff(positions[:, 0]))\n",
        "        if len(angles) > 1:\n",
        "            angle_changes = np.abs(np.diff(angles))\n",
        "            max_turn = np.max(angle_changes)\n",
        "            mean_turn = np.mean(angle_changes)\n",
        "        else:\n",
        "            max_turn = mean_turn = 0\n",
        "\n",
        "        speed_mean = speeds.mean()\n",
        "        speed_change = speeds[-1] - speeds[0] if len(speeds) > 1 else 0\n",
        "        dx = positions[-1, 0] - positions[0, 0]\n",
        "        dy = positions[-1, 1] - positions[0, 1]\n",
        "\n",
        "        route_features.append({\n",
        "            'game_id': gid, 'play_id': pid, 'nfl_id': nid,\n",
        "            'traj_straightness': straightness,\n",
        "            'traj_max_turn': max_turn,\n",
        "            'traj_mean_turn': mean_turn,\n",
        "            'traj_depth': abs(dx),\n",
        "            'traj_width': abs(dy),\n",
        "            'speed_mean': speed_mean,\n",
        "            'speed_change': speed_change,\n",
        "        })\n",
        "\n",
        "    route_df = pd.DataFrame(route_features)\n",
        "    feat_cols = ['traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
        "                 'traj_depth', 'traj_width', 'speed_mean', 'speed_change']\n",
        "    X = route_df[feat_cols].fillna(0)\n",
        "\n",
        "    if fit:\n",
        "        scaler = StandardScaler()\n",
        "        X_scaled = scaler.fit_transform(X)\n",
        "        kmeans = KMeans(n_clusters=Config.N_ROUTE_CLUSTERS, random_state=Config.SEED, n_init=10)\n",
        "        route_df['route_pattern'] = kmeans.fit_predict(X_scaled)\n",
        "        return route_df, kmeans, scaler\n",
        "    else:\n",
        "        X_scaled = scaler.transform(X)\n",
        "        route_df['route_pattern'] = kmeans.predict(X_scaled)\n",
        "        return route_df\n",
        "\n",
        "def compute_neighbor_embeddings(input_df, k_neigh=Config.K_NEIGH,\n",
        "                                radius=Config.RADIUS, tau=Config.TAU):\n",
        "    \"\"\"GNN-lite embeddings\"\"\"\n",
        "    print(\"üï∏Ô∏è  GNN embeddings...\")\n",
        "\n",
        "    cols_needed = [\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\", \"x\", \"y\",\n",
        "                   \"velocity_x\", \"velocity_y\", \"player_side\"]\n",
        "    src = input_df[cols_needed].copy()\n",
        "\n",
        "    last = (src.sort_values([\"game_id\", \"play_id\", \"nfl_id\", \"frame_id\"])\n",
        "               .groupby([\"game_id\", \"play_id\", \"nfl_id\"], as_index=False)\n",
        "               .tail(1)\n",
        "               .rename(columns={\"frame_id\": \"last_frame_id\"})\n",
        "               .reset_index(drop=True))\n",
        "\n",
        "    tmp = last.merge(\n",
        "        src.rename(columns={\n",
        "            \"frame_id\": \"nb_frame_id\", \"nfl_id\": \"nfl_id_nb\",\n",
        "            \"x\": \"x_nb\", \"y\": \"y_nb\",\n",
        "            \"velocity_x\": \"vx_nb\", \"velocity_y\": \"vy_nb\",\n",
        "            \"player_side\": \"player_side_nb\"\n",
        "        }),\n",
        "        left_on=[\"game_id\", \"play_id\", \"last_frame_id\"],\n",
        "        right_on=[\"game_id\", \"play_id\", \"nb_frame_id\"],\n",
        "        how=\"left\"\n",
        "    )\n",
        "\n",
        "    tmp = tmp[tmp[\"nfl_id_nb\"] != tmp[\"nfl_id\"]]\n",
        "    tmp[\"dx\"] = tmp[\"x_nb\"] - tmp[\"x\"]\n",
        "    tmp[\"dy\"] = tmp[\"y_nb\"] - tmp[\"y\"]\n",
        "    tmp[\"dvx\"] = tmp[\"vx_nb\"] - tmp[\"velocity_x\"]\n",
        "    tmp[\"dvy\"] = tmp[\"vy_nb\"] - tmp[\"velocity_y\"]\n",
        "    tmp[\"dist\"] = np.sqrt(tmp[\"dx\"]**2 + tmp[\"dy\"]**2)\n",
        "\n",
        "    tmp = tmp[np.isfinite(tmp[\"dist\"]) & (tmp[\"dist\"] > 1e-6)]\n",
        "    if radius is not None:\n",
        "        tmp = tmp[tmp[\"dist\"] <= radius]\n",
        "\n",
        "    tmp[\"is_ally\"] = (tmp[\"player_side_nb\"] == tmp[\"player_side\"]).astype(np.float32)\n",
        "\n",
        "    keys = [\"game_id\", \"play_id\", \"nfl_id\"]\n",
        "    tmp[\"rnk\"] = tmp.groupby(keys)[\"dist\"].rank(method=\"first\")\n",
        "    if k_neigh is not None:\n",
        "        tmp = tmp[tmp[\"rnk\"] <= float(k_neigh)]\n",
        "\n",
        "    tmp[\"w\"] = np.exp(-tmp[\"dist\"] / float(tau))\n",
        "    sum_w = tmp.groupby(keys)[\"w\"].transform(\"sum\")\n",
        "    tmp[\"wn\"] = np.where(sum_w > 0, tmp[\"w\"] / sum_w, 0.0)\n",
        "\n",
        "    tmp[\"wn_ally\"] = tmp[\"wn\"] * tmp[\"is_ally\"]\n",
        "    tmp[\"wn_opp\"] = tmp[\"wn\"] * (1.0 - tmp[\"is_ally\"])\n",
        "\n",
        "    for col in [\"dx\", \"dy\", \"dvx\", \"dvy\"]:\n",
        "        tmp[f\"{col}_ally_w\"] = tmp[col] * tmp[\"wn_ally\"]\n",
        "        tmp[f\"{col}_opp_w\"] = tmp[col] * tmp[\"wn_opp\"]\n",
        "\n",
        "    tmp[\"dist_ally\"] = np.where(tmp[\"is_ally\"] > 0.5, tmp[\"dist\"], np.nan)\n",
        "    tmp[\"dist_opp\"] = np.where(tmp[\"is_ally\"] < 0.5, tmp[\"dist\"], np.nan)\n",
        "\n",
        "    ag = tmp.groupby(keys).agg(\n",
        "        gnn_ally_dx_mean=(\"dx_ally_w\", \"sum\"),\n",
        "        gnn_ally_dy_mean=(\"dy_ally_w\", \"sum\"),\n",
        "        gnn_ally_dvx_mean=(\"dvx_ally_w\", \"sum\"),\n",
        "        gnn_ally_dvy_mean=(\"dvy_ally_w\", \"sum\"),\n",
        "        gnn_opp_dx_mean=(\"dx_opp_w\", \"sum\"),\n",
        "        gnn_opp_dy_mean=(\"dy_opp_w\", \"sum\"),\n",
        "        gnn_opp_dvx_mean=(\"dvx_opp_w\", \"sum\"),\n",
        "        gnn_opp_dvy_mean=(\"dvy_opp_w\", \"sum\"),\n",
        "        gnn_ally_cnt=(\"is_ally\", \"sum\"),\n",
        "        gnn_opp_cnt=(\"is_ally\", lambda s: float(len(s) - s.sum())),\n",
        "        gnn_ally_dmin=(\"dist_ally\", \"min\"),\n",
        "        gnn_ally_dmean=(\"dist_ally\", \"mean\"),\n",
        "        gnn_opp_dmin=(\"dist_opp\", \"min\"),\n",
        "        gnn_opp_dmean=(\"dist_opp\", \"mean\"),\n",
        "    ).reset_index()\n",
        "\n",
        "    near = tmp.loc[tmp[\"rnk\"] <= 3, keys + [\"rnk\", \"dist\"]].copy()\n",
        "    if len(near) > 0:\n",
        "        near[\"rnk\"] = near[\"rnk\"].astype(int)\n",
        "        dwide = near.pivot_table(index=keys, columns=\"rnk\", values=\"dist\", aggfunc=\"first\")\n",
        "        dwide = dwide.rename(columns={1: \"gnn_d1\", 2: \"gnn_d2\", 3: \"gnn_d3\"}).reset_index()\n",
        "        ag = ag.merge(dwide, on=keys, how=\"left\")\n",
        "\n",
        "    for c in [\"gnn_ally_dx_mean\", \"gnn_ally_dy_mean\", \"gnn_ally_dvx_mean\", \"gnn_ally_dvy_mean\",\n",
        "              \"gnn_opp_dx_mean\", \"gnn_opp_dy_mean\", \"gnn_opp_dvx_mean\", \"gnn_opp_dvy_mean\"]:\n",
        "        ag[c] = ag[c].fillna(0.0)\n",
        "    for c in [\"gnn_ally_cnt\", \"gnn_opp_cnt\"]:\n",
        "        ag[c] = ag[c].fillna(0.0)\n",
        "    for c in [\"gnn_ally_dmin\", \"gnn_opp_dmin\", \"gnn_ally_dmean\", \"gnn_opp_dmean\",\n",
        "              \"gnn_d1\", \"gnn_d2\", \"gnn_d3\"]:\n",
        "        ag[c] = ag[c].fillna(radius if radius is not None else 30.0)\n",
        "\n",
        "    return ag\n",
        "\n",
        "# ============================================================================\n",
        "# SEQUENCE PREPARATION WITH GEOMETRIC FEATURES\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_sequences_geometric(input_df, output_df=None, test_template=None,\n",
        "                                is_training=True, window_size=10,\n",
        "                                route_kmeans=None, route_scaler=None):\n",
        "    \"\"\"YOUR 154 features + 13 geometric features = 167 total\"\"\"\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PREPARING GEOMETRIC SEQUENCES\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    input_df = input_df.copy()\n",
        "    input_df = input_df.sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "\n",
        "    print(\"Step 1: Base features...\")\n",
        "\n",
        "    input_df['player_height_feet'] = input_df['player_height'].apply(height_to_feet)\n",
        "    height_parts = input_df['player_height'].str.split('-', expand=True)\n",
        "    input_df['height_inches'] = height_parts[0].astype(float) * 12 + height_parts[1].astype(float)\n",
        "    input_df['bmi'] = (input_df['player_weight'] / (input_df['height_inches']**2)) * 703\n",
        "\n",
        "    dir_rad = np.deg2rad(input_df['dir'].fillna(0))\n",
        "    input_df['velocity_x'] = input_df['s'] * np.sin(dir_rad)\n",
        "    input_df['velocity_y'] = input_df['s'] * np.cos(dir_rad)\n",
        "    input_df['acceleration_x'] = input_df['a'] * np.cos(dir_rad)\n",
        "    input_df['acceleration_y'] = input_df['a'] * np.sin(dir_rad)\n",
        "\n",
        "    input_df['speed_squared'] = input_df['s'] ** 2\n",
        "    input_df['accel_magnitude'] = np.sqrt(input_df['acceleration_x']**2 + input_df['acceleration_y']**2)\n",
        "    input_df['momentum_x'] = input_df['velocity_x'] * input_df['player_weight']\n",
        "    input_df['momentum_y'] = input_df['velocity_y'] * input_df['player_weight']\n",
        "    input_df['kinetic_energy'] = 0.5 * input_df['player_weight'] * input_df['speed_squared']\n",
        "\n",
        "    input_df['orientation_diff'] = np.abs(input_df['o'] - input_df['dir'])\n",
        "    input_df['orientation_diff'] = np.minimum(input_df['orientation_diff'], 360 - input_df['orientation_diff'])\n",
        "\n",
        "    input_df['is_offense'] = (input_df['player_side'] == 'Offense').astype(int)\n",
        "    input_df['is_defense'] = (input_df['player_side'] == 'Defense').astype(int)\n",
        "    input_df['is_receiver'] = (input_df['player_role'] == 'Targeted Receiver').astype(int)\n",
        "    input_df['is_coverage'] = (input_df['player_role'] == 'Defensive Coverage').astype(int)\n",
        "    input_df['is_passer'] = (input_df['player_role'] == 'Passer').astype(int)\n",
        "    input_df['role_targeted_receiver'] = input_df['is_receiver']\n",
        "    input_df['role_defensive_coverage'] = input_df['is_coverage']\n",
        "    input_df['role_passer'] = input_df['is_passer']\n",
        "    input_df['side_offense'] = input_df['is_offense']\n",
        "\n",
        "    if 'ball_land_x' in input_df.columns:\n",
        "        ball_dx = input_df['ball_land_x'] - input_df['x']\n",
        "        ball_dy = input_df['ball_land_y'] - input_df['y']\n",
        "        input_df['distance_to_ball'] = np.sqrt(ball_dx**2 + ball_dy**2)\n",
        "        input_df['dist_to_ball'] = input_df['distance_to_ball']\n",
        "        input_df['dist_squared'] = input_df['distance_to_ball'] ** 2\n",
        "        input_df['angle_to_ball'] = np.arctan2(ball_dy, ball_dx)\n",
        "        input_df['ball_direction_x'] = ball_dx / (input_df['distance_to_ball'] + 1e-6)\n",
        "        input_df['ball_direction_y'] = ball_dy / (input_df['distance_to_ball'] + 1e-6)\n",
        "        input_df['closing_speed_ball'] = (\n",
        "            input_df['velocity_x'] * input_df['ball_direction_x'] +\n",
        "            input_df['velocity_y'] * input_df['ball_direction_y']\n",
        "        )\n",
        "        input_df['velocity_toward_ball'] = (\n",
        "            input_df['velocity_x'] * np.cos(input_df['angle_to_ball']) +\n",
        "            input_df['velocity_y'] * np.sin(input_df['angle_to_ball'])\n",
        "        )\n",
        "        input_df['velocity_alignment'] = np.cos(input_df['angle_to_ball'] - dir_rad)\n",
        "        input_df['angle_diff'] = np.abs(input_df['o'] - np.degrees(input_df['angle_to_ball']))\n",
        "        input_df['angle_diff'] = np.minimum(input_df['angle_diff'], 360 - input_df['angle_diff'])\n",
        "\n",
        "    print(\"Step 2: Advanced features...\")\n",
        "\n",
        "    opp_features = get_opponent_features(input_df)\n",
        "    input_df = input_df.merge(opp_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "\n",
        "    if is_training:\n",
        "        route_features, route_kmeans, route_scaler = extract_route_patterns(input_df)\n",
        "    else:\n",
        "        route_features = extract_route_patterns(input_df, route_kmeans, route_scaler, fit=False)\n",
        "    input_df = input_df.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "\n",
        "    if not route_features.empty:\n",
        "        input_df = input_df.merge(route_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "\n",
        "    gnn_features = compute_neighbor_embeddings(input_df)\n",
        "    input_df = input_df.merge(gnn_features, on=['game_id', 'play_id', 'nfl_id'], how='left')\n",
        "\n",
        "    if 'nearest_opp_dist' in input_df.columns:\n",
        "        input_df['pressure'] = 1 / np.maximum(input_df['nearest_opp_dist'], 0.5)\n",
        "        input_df['under_pressure'] = (input_df['nearest_opp_dist'] < 3).astype(int)\n",
        "        input_df['pressure_x_speed'] = input_df['pressure'] * input_df['s']\n",
        "\n",
        "    if 'mirror_wr_vx' in input_df.columns:\n",
        "        s_safe = np.maximum(input_df['s'], 0.1)\n",
        "        input_df['mirror_similarity'] = (\n",
        "            input_df['velocity_x'] * input_df['mirror_wr_vx'] +\n",
        "            input_df['velocity_y'] * input_df['mirror_wr_vy']\n",
        "        ) / s_safe\n",
        "        input_df['mirror_offset_dist'] = np.sqrt(\n",
        "            input_df['mirror_offset_x']**2 + input_df['mirror_offset_y']**2\n",
        "        )\n",
        "        input_df['mirror_alignment'] = input_df['mirror_similarity'] * input_df['role_defensive_coverage']\n",
        "\n",
        "    print(\"Step 3: Temporal features...\")\n",
        "\n",
        "    gcols = ['game_id', 'play_id', 'nfl_id']\n",
        "\n",
        "    for lag in [1, 2, 3, 4, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
        "            if col in input_df.columns:\n",
        "                input_df[f'{col}_lag{lag}'] = input_df.groupby(gcols)[col].shift(lag)\n",
        "\n",
        "    for window in [3, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
        "            if col in input_df.columns:\n",
        "                input_df[f'{col}_rolling_mean_{window}'] = (\n",
        "                    input_df.groupby(gcols)[col]\n",
        "                      .rolling(window, min_periods=1).mean()\n",
        "                      .reset_index(level=[0,1,2], drop=True)\n",
        "                )\n",
        "                input_df[f'{col}_rolling_std_{window}'] = (\n",
        "                    input_df.groupby(gcols)[col]\n",
        "                      .rolling(window, min_periods=1).std()\n",
        "                      .reset_index(level=[0,1,2], drop=True)\n",
        "                )\n",
        "\n",
        "    for col in ['velocity_x', 'velocity_y']:\n",
        "        if col in input_df.columns:\n",
        "            input_df[f'{col}_delta'] = input_df.groupby(gcols)[col].diff()\n",
        "\n",
        "    input_df['velocity_x_ema'] = input_df.groupby(gcols)['velocity_x'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "    input_df['velocity_y_ema'] = input_df.groupby(gcols)['velocity_y'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "    input_df['speed_ema'] = input_df.groupby(gcols)['s'].transform(\n",
        "        lambda x: x.ewm(alpha=0.3, adjust=False).mean()\n",
        "    )\n",
        "\n",
        "    print(\"Step 4: Time features...\")\n",
        "\n",
        "    if 'num_frames_output' in input_df.columns:\n",
        "        max_frames = input_df['num_frames_output']\n",
        "\n",
        "        input_df['max_play_duration'] = max_frames / 10.0\n",
        "        input_df['frame_time'] = input_df['frame_id'] / 10.0\n",
        "        input_df['progress_ratio'] = input_df['frame_id'] / np.maximum(max_frames, 1)\n",
        "        input_df['time_remaining'] = (max_frames - input_df['frame_id']) / 10.0\n",
        "        input_df['frames_remaining'] = max_frames - input_df['frame_id']\n",
        "\n",
        "        input_df['expected_x_at_ball'] = input_df['x'] + input_df['velocity_x'] * input_df['frame_time']\n",
        "        input_df['expected_y_at_ball'] = input_df['y'] + input_df['velocity_y'] * input_df['frame_time']\n",
        "\n",
        "        if 'ball_land_x' in input_df.columns:\n",
        "            input_df['error_from_ball_x'] = input_df['expected_x_at_ball'] - input_df['ball_land_x']\n",
        "            input_df['error_from_ball_y'] = input_df['expected_y_at_ball'] - input_df['ball_land_y']\n",
        "            input_df['error_from_ball'] = np.sqrt(\n",
        "                input_df['error_from_ball_x']**2 + input_df['error_from_ball_y']**2\n",
        "            )\n",
        "\n",
        "            input_df['weighted_dist_by_time'] = input_df['dist_to_ball'] / (input_df['frame_time'] + 0.1)\n",
        "            input_df['dist_scaled_by_progress'] = input_df['dist_to_ball'] * (1 - input_df['progress_ratio'])\n",
        "\n",
        "        input_df['time_squared'] = input_df['frame_time'] ** 2\n",
        "        input_df['velocity_x_progress'] = input_df['velocity_x'] * input_df['progress_ratio']\n",
        "        input_df['velocity_y_progress'] = input_df['velocity_y'] * input_df['progress_ratio']\n",
        "        input_df['speed_scaled_by_time_left'] = input_df['s'] * input_df['time_remaining']\n",
        "\n",
        "        input_df['actual_play_length'] = max_frames\n",
        "        input_df['length_ratio'] = max_frames / 30.0\n",
        "\n",
        "    # üéØ THE BREAKTHROUGH: Add geometric features\n",
        "    print(\"Step 5: üéØ Geometric endpoint features...\")\n",
        "    input_df = add_geometric_features(input_df)\n",
        "\n",
        "    print(\"Step 6: Building feature list...\")\n",
        "\n",
        "    # Your 154 proven features\n",
        "    feature_cols = [\n",
        "        'x', 'y', 's', 'a', 'o', 'dir', 'frame_id', 'ball_land_x', 'ball_land_y',\n",
        "        'player_height_feet', 'player_weight', 'height_inches', 'bmi',\n",
        "        'velocity_x', 'velocity_y', 'acceleration_x', 'acceleration_y',\n",
        "        'momentum_x', 'momentum_y', 'kinetic_energy',\n",
        "        'speed_squared', 'accel_magnitude', 'orientation_diff',\n",
        "        'is_offense', 'is_defense', 'is_receiver', 'is_coverage', 'is_passer',\n",
        "        'role_targeted_receiver', 'role_defensive_coverage', 'role_passer', 'side_offense',\n",
        "        'distance_to_ball', 'dist_to_ball', 'dist_squared', 'angle_to_ball',\n",
        "        'ball_direction_x', 'ball_direction_y', 'closing_speed_ball',\n",
        "        'velocity_toward_ball', 'velocity_alignment', 'angle_diff',\n",
        "        'nearest_opp_dist', 'closing_speed', 'num_nearby_opp_3', 'num_nearby_opp_5',\n",
        "        'mirror_wr_vx', 'mirror_wr_vy', 'mirror_offset_x', 'mirror_offset_y',\n",
        "        'pressure', 'under_pressure', 'pressure_x_speed',\n",
        "        'mirror_similarity', 'mirror_offset_dist', 'mirror_alignment',\n",
        "        'route_pattern', 'traj_straightness', 'traj_max_turn', 'traj_mean_turn',\n",
        "        'traj_depth', 'traj_width', 'speed_mean', 'speed_change',\n",
        "        'gnn_ally_dx_mean', 'gnn_ally_dy_mean', 'gnn_ally_dvx_mean', 'gnn_ally_dvy_mean',\n",
        "        'gnn_opp_dx_mean', 'gnn_opp_dy_mean', 'gnn_opp_dvx_mean', 'gnn_opp_dvy_mean',\n",
        "        'gnn_ally_cnt', 'gnn_opp_cnt',\n",
        "        'gnn_ally_dmin', 'gnn_ally_dmean', 'gnn_opp_dmin', 'gnn_opp_dmean',\n",
        "        'gnn_d1', 'gnn_d2', 'gnn_d3',\n",
        "    ]\n",
        "\n",
        "    for lag in [1, 2, 3, 4, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's', 'a']:\n",
        "            feature_cols.append(f'{col}_lag{lag}')\n",
        "\n",
        "    for window in [3, 5]:\n",
        "        for col in ['x', 'y', 'velocity_x', 'velocity_y', 's']:\n",
        "            feature_cols.append(f'{col}_rolling_mean_{window}')\n",
        "            feature_cols.append(f'{col}_rolling_std_{window}')\n",
        "\n",
        "    feature_cols.extend(['velocity_x_delta', 'velocity_y_delta'])\n",
        "    feature_cols.extend(['velocity_x_ema', 'velocity_y_ema', 'speed_ema'])\n",
        "\n",
        "    feature_cols.extend([\n",
        "        'max_play_duration', 'frame_time', 'progress_ratio', 'time_remaining', 'frames_remaining',\n",
        "        'expected_x_at_ball', 'expected_y_at_ball',\n",
        "        'error_from_ball_x', 'error_from_ball_y', 'error_from_ball',\n",
        "        'time_squared', 'weighted_dist_by_time',\n",
        "        'velocity_x_progress', 'velocity_y_progress', 'dist_scaled_by_progress',\n",
        "        'speed_scaled_by_time_left', 'actual_play_length', 'length_ratio',\n",
        "    ])\n",
        "\n",
        "    # üéØ Add 13 geometric features\n",
        "    feature_cols.extend([\n",
        "        'geo_endpoint_x', 'geo_endpoint_y',\n",
        "        'geo_vector_x', 'geo_vector_y', 'geo_distance',\n",
        "        'geo_required_vx', 'geo_required_vy',\n",
        "        'geo_velocity_error_x', 'geo_velocity_error_y', 'geo_velocity_error',\n",
        "        'geo_required_ax', 'geo_required_ay',\n",
        "        'geo_alignment',\n",
        "    ])\n",
        "\n",
        "    feature_cols = [c for c in feature_cols if c in input_df.columns]\n",
        "    print(f\"‚úì Using {len(feature_cols)} features (154 proven + 13 geometric)\")\n",
        "\n",
        "    print(\"Step 7: Creating sequences...\")\n",
        "\n",
        "    input_df.set_index(['game_id', 'play_id', 'nfl_id'], inplace=True)\n",
        "    grouped = input_df.groupby(level=['game_id', 'play_id', 'nfl_id'])\n",
        "\n",
        "    target_rows = output_df if is_training else test_template\n",
        "    target_groups = target_rows[['game_id', 'play_id', 'nfl_id']].drop_duplicates()\n",
        "\n",
        "    sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids = [], [], [], [], []\n",
        "    geo_endpoints_x, geo_endpoints_y = [], []\n",
        "\n",
        "    for _, row in tqdm(target_groups.iterrows(), total=len(target_groups), desc=\"Creating sequences\"):\n",
        "        key = (row['game_id'], row['play_id'], row['nfl_id'])\n",
        "\n",
        "        try:\n",
        "            group_df = grouped.get_group(key)\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        input_window = group_df.tail(window_size)\n",
        "\n",
        "        if len(input_window) < window_size:\n",
        "            if is_training:\n",
        "                continue\n",
        "            pad_len = window_size - len(input_window)\n",
        "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
        "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
        "\n",
        "        input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
        "        seq = input_window[feature_cols].values\n",
        "\n",
        "        if np.isnan(seq).any():\n",
        "            if is_training:\n",
        "                continue\n",
        "            seq = np.nan_to_num(seq, nan=0.0)\n",
        "\n",
        "        sequences.append(seq)\n",
        "\n",
        "        # Store geometric endpoint for this player\n",
        "        geo_x = input_window.iloc[-1]['geo_endpoint_x']\n",
        "        geo_y = input_window.iloc[-1]['geo_endpoint_y']\n",
        "        geo_endpoints_x.append(geo_x)\n",
        "        geo_endpoints_y.append(geo_y)\n",
        "\n",
        "        if is_training:\n",
        "            out_grp = output_df[\n",
        "                (output_df['game_id']==row['game_id']) &\n",
        "                (output_df['play_id']==row['play_id']) &\n",
        "                (output_df['nfl_id']==row['nfl_id'])\n",
        "            ].sort_values('frame_id')\n",
        "\n",
        "            last_x = input_window.iloc[-1]['x']\n",
        "            last_y = input_window.iloc[-1]['y']\n",
        "\n",
        "            dx = out_grp['x'].values - last_x\n",
        "            dy = out_grp['y'].values - last_y\n",
        "\n",
        "            targets_dx.append(dx)\n",
        "            targets_dy.append(dy)\n",
        "            targets_frame_ids.append(out_grp['frame_id'].values)\n",
        "\n",
        "        sequence_ids.append({\n",
        "            'game_id': key[0],\n",
        "            'play_id': key[1],\n",
        "            'nfl_id': key[2],\n",
        "            'frame_id': input_window.iloc[-1]['frame_id']\n",
        "        })\n",
        "\n",
        "    print(f\"‚úì Created {len(sequences)} sequences\")\n",
        "\n",
        "    if is_training:\n",
        "        return (sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids,\n",
        "                geo_endpoints_x, geo_endpoints_y, route_kmeans, route_scaler)\n",
        "    return sequences, sequence_ids, geo_endpoints_x, geo_endpoints_y\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL ARCHITECTURE (ST-TRANSFORMER with ResidualMLP Head)\n",
        "# ============================================================================\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ‰∏Ä‰∏™Ê†áÂáÜÁöÑÊÆãÂ∑ÆÂùóÔºöFFN + Âø´Êç∑ËøûÊé•\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, hidden_dim, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(dim, hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pre-normalization\n",
        "        return x + self.ffn(self.norm(x))\n",
        "\n",
        "class ResidualMLPHead(nn.Module):\n",
        "    \"\"\"\n",
        "    ÊõøÊç¢ÂéüÊúâÁöÑ nn.Sequential Head\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_res_blocks=2, dropout=0.2):\n",
        "        super().__init__()\n",
        "        # 1. ‰ªé context_dim (128) ÊäïÂΩ±Âà∞ mlp_hidden_dim (256)\n",
        "        self.input_layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "\n",
        "        # 2. ‰∏ÄÁ≥ªÂàóÁöÑÊÆãÂ∑ÆÂùó (Âú® 256 Áª¥Â∫¶‰∏äÊìç‰Ωú)\n",
        "        self.residual_blocks = nn.Sequential(\n",
        "            *[ResidualBlock(hidden_dim, hidden_dim * 2, dropout) for _ in range(n_res_blocks)]\n",
        "        )\n",
        "\n",
        "        # 3. ÊúÄÂêéÁöÑ LayerNorm ÂíåËæìÂá∫ÊäïÂΩ±\n",
        "        self.output_norm = nn.LayerNorm(hidden_dim)\n",
        "        self.output_layer = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.input_layer(x)\n",
        "        x = self.residual_blocks(x)\n",
        "        x = self.output_norm(x)\n",
        "        x = self.output_layer(x)\n",
        "        return x\n",
        "\n",
        "class STTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatio-Temporal Transformer\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, hidden_dim, horizon, window_size, n_heads, n_layers, dropout=0.1):\n",
        "        super().__init__()\n",
        "        config = Config() # Ëé∑Âèñ MLP ÁöÑË∂ÖÂèÇÊï∞\n",
        "        self.horizon = horizon\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # 1. Spatio: ÁâπÂæÅÂµåÂÖ•\n",
        "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
        "\n",
        "        # 2. Temporal: ÂèØÂ≠¶‰π†ÁöÑ‰ΩçÁΩÆÁºñÁ†Å\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, window_size, hidden_dim))\n",
        "        self.embed_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # 3. Transformer Encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_dim,\n",
        "            nhead=n_heads,\n",
        "            dim_feedforward=hidden_dim * 4,\n",
        "            dropout=dropout,\n",
        "            batch_first=True,\n",
        "            activation='gelu'\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=n_layers\n",
        "        )\n",
        "\n",
        "        # 4. Ê±†Âåñ (Â§çÁî®‰Ω†ÊàêÁÜüÁöÑ Attention Pooling Êú∫Âà∂)\n",
        "        self.pool_ln = nn.LayerNorm(hidden_dim)\n",
        "        self.pool_attn = nn.MultiheadAttention(hidden_dim, num_heads=n_heads, batch_first=True)\n",
        "        self.pool_query = nn.Parameter(torch.randn(1, 1, hidden_dim))\n",
        "\n",
        "        # 5. ËæìÂá∫ Head (!!! Â∑≤ÊõøÊç¢‰∏∫ ResidualMLPHead !!!)\n",
        "        self.head = ResidualMLPHead(\n",
        "            input_dim=hidden_dim,                   # 128\n",
        "            hidden_dim=config.MLP_HIDDEN_DIM,       # 256\n",
        "            output_dim=horizon * 2,                 # 188\n",
        "            n_res_blocks=config.N_RES_BLOCKS,       # 2\n",
        "            dropout=0.2\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, S, _ = x.shape\n",
        "        x_embed = self.input_projection(x)\n",
        "        x = x_embed + self.pos_embed[:, :S, :]\n",
        "        x = self.embed_dropout(x)\n",
        "\n",
        "        h = self.transformer_encoder(x)\n",
        "\n",
        "        q = self.pool_query.expand(B, -1, -1)\n",
        "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
        "        ctx = ctx.squeeze(1)\n",
        "\n",
        "        out = self.head(ctx) # <--- ‰ΩøÁî®Êñ∞ÁöÑ Head\n",
        "        out = out.view(B, self.horizon, 2)\n",
        "\n",
        "        out = torch.cumsum(out, dim=1)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# ============================================================================\n",
        "# LOSS (YOUR PROVEN TEMPORAL HUBER)\n",
        "# ============================================================================\n",
        "\n",
        "class TemporalHuber(nn.Module):\n",
        "    def __init__(self, delta=0.5, time_decay=0.03):\n",
        "        super().__init__()\n",
        "        self.delta = delta\n",
        "        self.time_decay = time_decay\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        err = pred - target\n",
        "        abs_err = torch.abs(err)\n",
        "        huber = torch.where(abs_err <= self.delta, 0.5 * err * err,\n",
        "                           self.delta * (abs_err - 0.5 * self.delta))\n",
        "\n",
        "        if self.time_decay > 0:\n",
        "            L = pred.size(1)\n",
        "            t = torch.arange(L, device=pred.device).float()\n",
        "            weight = torch.exp(-self.time_decay * t).view(1, L, 1)\n",
        "            huber = huber * weight\n",
        "            mask = mask.unsqueeze(-1) * weight\n",
        "\n",
        "        return (huber * mask).sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "# ============================================================================\n",
        "# TRAINING\n",
        "# ============================================================================\n",
        "\n",
        "def prepare_targets(batch_dx, batch_dy, max_h):\n",
        "    tensors_x, tensors_y, masks = [], [], []\n",
        "\n",
        "    for dx, dy in zip(batch_dx, batch_dy):\n",
        "        L = len(dx)\n",
        "        padded_x = np.pad(dx, (0, max_h - L), constant_values=0).astype(np.float32)\n",
        "        padded_y = np.pad(dy, (0, max_h - L), constant_values=0).astype(np.float32)\n",
        "        mask = np.zeros(max_h, dtype=np.float32)\n",
        "        mask[:L] = 1.0\n",
        "\n",
        "        tensors_x.append(torch.tensor(padded_x))\n",
        "        tensors_y.append(torch.tensor(padded_y))\n",
        "        masks.append(torch.tensor(mask))\n",
        "\n",
        "    targets = torch.stack([torch.stack(tensors_x), torch.stack(tensors_y)], dim=-1)\n",
        "    return targets, torch.stack(masks)\n",
        "\n",
        "def compute_val_rmse(model, X_val, y_val_dx, y_val_dy, horizon, device, batch_size=256):\n",
        "    \"\"\"\n",
        "    Compute validation RMSE (Root Mean Squared Error) for 2D trajectory prediction.\n",
        "    Returns the average Euclidean distance error across all predictions.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    all_errors = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(X_val), batch_size):\n",
        "            end = min(i + batch_size, len(X_val))\n",
        "            bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32)).to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            pred = model(bx).cpu().numpy()  # Shape: (batch, horizon, 2)\n",
        "\n",
        "            # Process each sample in the batch\n",
        "            for j, idx in enumerate(range(i, end)):\n",
        "                dx_true = y_val_dx[idx]\n",
        "                dy_true = y_val_dy[idx]\n",
        "                n_steps = len(dx_true)\n",
        "\n",
        "                # Extract predictions for this sample\n",
        "                dx_pred = pred[j, :n_steps, 0]\n",
        "                dy_pred = pred[j, :n_steps, 1]\n",
        "\n",
        "                # Compute Euclidean distance for each time step\n",
        "                sq_errors = (dx_pred - dx_true) ** 2 + (dy_pred - dy_true) ** 2\n",
        "                all_errors.extend(sq_errors)\n",
        "\n",
        "    # Return RMSE\n",
        "    return np.sqrt(np.mean(all_errors))\n",
        "\n",
        "def train_model(X_train, y_train_dx, y_train_dy, X_val, y_val_dx, y_val_dy,\n",
        "                input_dim, horizon, config):\n",
        "    device = config.DEVICE\n",
        "\n",
        "    model = STTransformer(\n",
        "        input_dim=input_dim,\n",
        "        hidden_dim=config.HIDDEN_DIM,\n",
        "        horizon=horizon,\n",
        "        window_size=config.WINDOW_SIZE,\n",
        "        n_heads=config.N_HEADS,\n",
        "        n_layers=config.N_LAYERS\n",
        "    ).to(device)\n",
        "    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n",
        "\n",
        "    train_batches = []\n",
        "    for i in range(0, len(X_train), config.BATCH_SIZE):\n",
        "        end = min(i + config.BATCH_SIZE, len(X_train))\n",
        "        bx = torch.tensor(np.stack(X_train[i:end]).astype(np.float32))\n",
        "        by, bm = prepare_targets(\n",
        "            [y_train_dx[j] for j in range(i, end)],\n",
        "            [y_train_dy[j] for j in range(i, end)],\n",
        "            horizon\n",
        "        )\n",
        "        train_batches.append((bx, by, bm))\n",
        "\n",
        "    val_batches = []\n",
        "    for i in range(0, len(X_val), config.BATCH_SIZE):\n",
        "        end = min(i + config.BATCH_SIZE, len(X_val))\n",
        "        bx = torch.tensor(np.stack(X_val[i:end]).astype(np.float32))\n",
        "        by, bm = prepare_targets(\n",
        "            [y_val_dx[j] for j in range(i, end)],\n",
        "            [y_val_dy[j] for j in range(i, end)],\n",
        "            horizon\n",
        "        )\n",
        "        val_batches.append((bx, by, bm))\n",
        "\n",
        "    best_loss, best_state, bad = float('inf'), None, 0\n",
        "\n",
        "    for epoch in range(1, config.EPOCHS + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for bx, by, bm in train_batches:\n",
        "            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
        "            pred = model(bx)\n",
        "            loss = criterion(pred, by, bm)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for bx, by, bm in val_batches:\n",
        "                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
        "                pred = model(bx)\n",
        "                val_losses.append(criterion(pred, by, bm).item())\n",
        "\n",
        "        train_loss, val_loss = np.mean(train_losses), np.mean(val_losses)\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"  Epoch {epoch}: train={train_loss:.4f}, val={val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
        "            bad = 0\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= config.PATIENCE:\n",
        "                print(f\"  Early stop at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    return model, best_loss\n",
        "\n",
        "import kaggle_evaluation.nfl_inference_server\n",
        "import warnings\n",
        "import polars as pl # Import Polars for the predict function signature\n",
        "\n",
        "# Move all utility functions (get_velocity, height_to_feet, compute_geometric_endpoint, etc.)\n",
        "# and the Config/set_seed outside the class but above it, or make them static methods.\n",
        "\n",
        "class NFLPredictor:\n",
        "    def __init__(self):\n",
        "        warnings.filterwarnings('ignore')\n",
        "        config = Config() # Use the existing Config class\n",
        "        set_seed(config.SEED)\n",
        "        self.config = config\n",
        "\n",
        "        # If configured, try to load pre-saved artifacts to skip training and data prep\n",
        "        load_dir = Path(config.LOAD_DIR) if config.LOAD_DIR is not None else config.MODEL_DIR\n",
        "        artifacts_present = all((load_dir / f\"model_fold{f}.pt\").exists() for f in range(1, config.N_FOLDS + 1)) and (\n",
        "            (load_dir / \"route_kmeans.pkl\").exists() and (load_dir / \"route_scaler.pkl\").exists()\n",
        "        )\n",
        "\n",
        "        if config.LOAD_ARTIFACTS and artifacts_present:\n",
        "            print(f\"\\n[1/2] Loading trained artifacts from disk (from {load_dir})...\")\n",
        "            self.models = []\n",
        "            self.scalers = []\n",
        "            # load route objects\n",
        "            try:\n",
        "                with open(load_dir / \"route_kmeans.pkl\", \"rb\") as f:\n",
        "                    self.route_kmeans = pickle.load(f)\n",
        "                with open(load_dir / \"route_scaler.pkl\", \"rb\") as f:\n",
        "                    self.route_scaler = pickle.load(f)\n",
        "            except Exception as e:\n",
        "                print(\"Failed to load route artifacts:\", e)\n",
        "\n",
        "            # For input_dim, use a dummy array if needed\n",
        "            dummy_input_dim = 167  # fallback if not available\n",
        "            input_dim = dummy_input_dim\n",
        "            # Try to infer input_dim from scaler if possible\n",
        "            try:\n",
        "                with open(load_dir / \"scaler_fold1.pkl\", \"rb\") as f:\n",
        "                    scaler1 = pickle.load(f)\n",
        "                input_dim = scaler1.mean_.shape[0]\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            for fold in range(1, config.N_FOLDS + 1):\n",
        "                model_path = load_dir / f\"model_fold{fold}.pt\"\n",
        "                scaler_path = load_dir / f\"scaler_fold{fold}.pkl\"\n",
        "\n",
        "                # load scaler\n",
        "                try:\n",
        "                    with open(scaler_path, \"rb\") as f:\n",
        "                        scaler = pickle.load(f)\n",
        "                except Exception:\n",
        "                    scaler = None\n",
        "\n",
        "                # instantiate model and load state\n",
        "                model = STTransformer(\n",
        "                        input_dim=input_dim,\n",
        "                        hidden_dim=config.HIDDEN_DIM,\n",
        "                        horizon=config.MAX_FUTURE_HORIZON,\n",
        "                        window_size=config.WINDOW_SIZE,\n",
        "                        n_heads=config.N_HEADS,\n",
        "                        n_layers=config.N_LAYERS).to(config.DEVICE)\n",
        "                try:\n",
        "                    state = torch.load(model_path, map_location='cpu')\n",
        "                    model.load_state_dict(state)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to load model fold {fold}:\", e)\n",
        "\n",
        "                model.to(config.DEVICE)\n",
        "                model.eval()\n",
        "\n",
        "                self.models.append(model)\n",
        "                self.scalers.append(scaler)\n",
        "\n",
        "            print(f\"‚úì Loaded {len(self.models)} models and scalers from {load_dir}\")\n",
        "            print(\"[2/2] Ready for inference. Skipped all training and data preparation.\")\n",
        "            return\n",
        "\n",
        "        # If not loading, proceed with training and data preparation\n",
        "        # 1. Load Data\n",
        "        print(\"[1/4] Loading data for training...\")\n",
        "        train_input_files = [config.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
        "        train_output_files = [config.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
        "        if Config.DEBUG:\n",
        "            # Load a small amount of data to quickly get play IDs\n",
        "            sample_train_input = pd.read_csv(train_input_files[0])\n",
        "            sample_train_output = pd.read_csv(train_output_files[0])\n",
        "            # Get the first 100 unique plays (game_id, play_id combinations)\n",
        "            sample_plays = sample_train_output[['game_id', 'play_id']].drop_duplicates().head(100)\n",
        "            # Filter the full data for only these plays\n",
        "            train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n",
        "            train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n",
        "            # Perform the actual filtering to get a small, but complete, set of plays\n",
        "            train_input = train_input.merge(sample_plays, on=['game_id', 'play_id'], how='inner')\n",
        "            train_output = train_output.merge(sample_plays, on=['game_id', 'play_id'], how='inner')\n",
        "            print(f\"‚úì Reduced to {len(sample_plays)} unique plays.\")\n",
        "        else:\n",
        "            train_input = pd.concat([pd.read_csv(f) for f in train_input_files if f.exists()])\n",
        "            train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()])\n",
        "\n",
        "        # 2. Prepare Sequences and Train Feature Objects\n",
        "        print(\"\\n[2/4] Preparing geometric sequences and feature scalers...\")\n",
        "        result = prepare_sequences_geometric(\n",
        "            train_input, train_output, is_training=True, window_size=config.WINDOW_SIZE\n",
        "        )\n",
        "        sequences, targets_dx, targets_dy, targets_frame_ids, sequence_ids, geo_x, geo_y, route_kmeans, route_scaler = result\n",
        "\n",
        "        # Store feature objects for later inference\n",
        "        self.route_kmeans = route_kmeans\n",
        "        self.route_scaler = route_scaler\n",
        "\n",
        "        sequences = list(sequences)\n",
        "        targets_dx = list(targets_dx)\n",
        "        targets_dy = list(targets_dy)\n",
        "\n",
        "        # Ensure model dir exists\n",
        "        config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 3. Train Models\n",
        "        print(\"\\n[3/4] Training geometric models...\")\n",
        "        groups = np.array([d['game_id'] for d in sequence_ids])\n",
        "        gkf = GroupKFold(n_splits=config.N_FOLDS)\n",
        "\n",
        "        self.models, self.scalers = [], []\n",
        "        fold_losses = []\n",
        "        fold_rmses = []\n",
        "\n",
        "        for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
        "            # ... (rest of the fold training logic, exactly as in the original main()) ...\n",
        "            X_tr = [sequences[i] for i in tr]\n",
        "            X_va = [sequences[i] for i in va]\n",
        "            y_tr_dx = [targets_dx[i] for i in tr]\n",
        "            y_va_dx = [targets_dx[i] for i in va]\n",
        "            y_tr_dy = [targets_dy[i] for i in tr]\n",
        "            y_va_dy = [targets_dy[i] for i in va]\n",
        "            scaler = StandardScaler()\n",
        "            scaler.fit(np.vstack([s for s in X_tr]))\n",
        "            X_tr_sc = [scaler.transform(s) for s in X_tr]\n",
        "            X_va_sc = [scaler.transform(s) for s in X_va]\n",
        "            model, loss = train_model(\n",
        "                X_tr_sc, y_tr_dx, y_tr_dy,\n",
        "                X_va_sc, y_va_dx, y_va_dy,\n",
        "                X_tr[0].shape[-1], config.MAX_FUTURE_HORIZON, config\n",
        "            )\n",
        "\n",
        "            # Compute validation RMSE\n",
        "            val_rmse = compute_val_rmse(\n",
        "                model, X_va_sc, y_va_dx, y_va_dy,\n",
        "                config.MAX_FUTURE_HORIZON, config.DEVICE, config.BATCH_SIZE\n",
        "            )\n",
        "\n",
        "            self.models.append(model)\n",
        "            self.scalers.append(scaler)\n",
        "            fold_losses.append(loss)\n",
        "            fold_rmses.append(val_rmse)\n",
        "\n",
        "            print(f\"\\n‚úì Fold {fold} - Loss: {loss:.5f}, Validation RMSE: {val_rmse:.5f}\")\n",
        "            # Persist fold artifacts if requested\n",
        "            if config.SAVE_ARTIFACTS:\n",
        "                try:\n",
        "                    model_path = config.MODEL_DIR / f\"model_fold{fold}.pt\"\n",
        "                    scaler_path = config.MODEL_DIR / f\"scaler_fold{fold}.pkl\"\n",
        "                    # save model state dict on CPU to avoid device issues\n",
        "                    state = {k: v.cpu() for k, v in model.state_dict().items()}\n",
        "                    torch.save(state, str(model_path))\n",
        "                    with open(scaler_path, \"wb\") as f:\n",
        "                        pickle.dump(scaler, f)\n",
        "                    print(f\"  Saved artifacts for fold {fold} -> {model_path.name}, {scaler_path.name}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  Warning: failed to save artifacts for fold {fold}:\", e)\n",
        "\n",
        "        # Print summary statistics across all folds\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"CROSS-VALIDATION SUMMARY\")\n",
        "        print(\"=\"*60)\n",
        "        avg_loss = np.mean(fold_losses)\n",
        "        std_loss = np.std(fold_losses)\n",
        "        avg_rmse = np.mean(fold_rmses)\n",
        "        std_rmse = np.std(fold_rmses)\n",
        "\n",
        "        print(f\"Average Loss:           {avg_loss:.5f} ¬± {std_loss:.5f}\")\n",
        "        print(f\"Average Validation RMSE: {avg_rmse:.5f} ¬± {std_rmse:.5f}\")\n",
        "        print(f\"\\nPer-Fold Results:\")\n",
        "        for i, (loss, rmse) in enumerate(zip(fold_losses, fold_rmses), 1):\n",
        "            print(f\"  Fold {i}: Loss={loss:.5f}, RMSE={rmse:.5f}\")\n",
        "        print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "        # Ensure all models are in eval mode (best practice)\n",
        "        # Optionally persist route clustering objects\n",
        "        if config.SAVE_ARTIFACTS:\n",
        "            try:\n",
        "                with open(config.MODEL_DIR / \"route_kmeans.pkl\", \"wb\") as f:\n",
        "                    pickle.dump(self.route_kmeans, f)\n",
        "                with open(config.MODEL_DIR / \"route_scaler.pkl\", \"wb\") as f:\n",
        "                    pickle.dump(self.route_scaler, f)\n",
        "                print(f\"‚úì Saved route artifacts -> route_kmeans.pkl, route_scaler.pkl\")\n",
        "            except Exception as e:\n",
        "                print(\"Warning: failed to save route artifacts:\", e)\n",
        "\n",
        "        for model in self.models:\n",
        "            model.eval()\n",
        "        print(\"\\nüèÜ Geometric Neural Breakthrough Model is ready for inference! üèÜ\")\n",
        "\n",
        "\n",
        "    def predict(self, test: pl.DataFrame, test_input: pl.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Inference function called by the API for each time step.\"\"\"\n",
        "\n",
        "        # Convert Polars DataFrames to Pandas, as the original notebook uses Pandas extensively\n",
        "        test_input_pd = test_input.to_pandas()\n",
        "        test_template_pd = test.to_pandas()\n",
        "\n",
        "        # 1. Prepare Test Sequences\n",
        "        # Use the stored feature objects (route_kmeans, route_scaler) for inference\n",
        "        test_seq, test_ids, test_geo_x, test_geo_y = prepare_sequences_geometric(\n",
        "            test_input_pd, test_template=test_template_pd, is_training=False,\n",
        "            window_size=self.config.WINDOW_SIZE,\n",
        "            route_kmeans=self.route_kmeans, route_scaler=self.route_scaler\n",
        "        )\n",
        "\n",
        "        X_test = list(test_seq)\n",
        "        x_last = np.array([s[-1, 0] for s in X_test])\n",
        "        y_last = np.array([s[-1, 1] for s in X_test])\n",
        "\n",
        "        # 2. Ensemble Prediction\n",
        "        all_preds = []\n",
        "        H = self.config.MAX_FUTURE_HORIZON\n",
        "\n",
        "        for model, sc in zip(self.models, self.scalers):\n",
        "            X_sc = [sc.transform(s) for s in X_test]\n",
        "            X_t = torch.tensor(np.stack(X_sc).astype(np.float32)).to(self.config.DEVICE)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                preds = model(X_t).cpu().numpy()\n",
        "\n",
        "            all_preds.append(preds)\n",
        "\n",
        "        ens_preds = np.mean(all_preds, axis=0)\n",
        "\n",
        "        # 3. Format Submission (Corrected for API)\n",
        "        rows = []\n",
        "        for i, sid in enumerate(test_ids):\n",
        "            # The template DataFrame 'test' contains the info needed to map predictions\n",
        "            fids = test_template_pd[\n",
        "                (test_template_pd['game_id'] == sid['game_id']) &\n",
        "                (test_template_pd['play_id'] == sid['play_id']) &\n",
        "                (test_template_pd['nfl_id'] == sid['nfl_id'])\n",
        "            ]['frame_id'].sort_values().tolist()\n",
        "\n",
        "            for t, fid in enumerate(fids):\n",
        "                tt = min(t, H - 1)\n",
        "                px = np.clip(x_last[i] + ens_preds[i, tt, 0], 0, self.config.FIELD_X_MAX)\n",
        "                py = np.clip(y_last[i] + ens_preds[i, tt, 1], 0, self.config.FIELD_Y_MAX)\n",
        "\n",
        "                # DO NOT include 'id' in the rows dict or final DataFrame\n",
        "                # The API will handle the row IDs based on the provided 'test' DataFrame\n",
        "                rows.append({\n",
        "                    'x': px,\n",
        "                    'y': py\n",
        "                })\n",
        "\n",
        "        # The final returned DataFrame MUST only contain 'x' and 'y' columns,\n",
        "        # matching the order of rows in the input 'test' DataFrame.\n",
        "        submission = pd.DataFrame(rows)\n",
        "        return submission\n",
        "\n",
        "# This must be outside the class and is what the API calls\n",
        "predictor = NFLPredictor()\n",
        "\n",
        "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"The function the API calls.\"\"\"\n",
        "    return predictor.predict(test, test_input)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T14:30:16.04043Z",
          "iopub.execute_input": "2025-12-02T14:30:16.041229Z",
          "iopub.status.idle": "2025-12-02T14:30:16.072133Z",
          "shell.execute_reply.started": "2025-12-02T14:30:16.041205Z",
          "shell.execute_reply": "2025-12-02T14:30:16.07133Z"
        },
        "id": "F0e1wRqqed9T"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nfl_gru3.py\n",
        "\n",
        "# -------------------------------\n",
        "# Global imports + cuDF accelerator\n",
        "# -------------------------------\n",
        "import os\n",
        "USE_CUDF = False\n",
        "try:\n",
        "    # zero/low-code GPU acceleration for DataFrame ops\n",
        "    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    import cupy as cp  # optional (not strictly required below)\n",
        "    USE_CUDF = True\n",
        "    print(\"using cuda_backend pandas for faster parallel data processing\")\n",
        "except Exception:\n",
        "    print(\"cuda df not used\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pathlib import Path\n",
        "from shutil import make_archive\n",
        "import json\n",
        "import random\n",
        "import joblib\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# New imports for evaluation API\n",
        "import polars as pl\n",
        "import kaggle_evaluation.nfl_inference_server\n",
        "\n",
        "# -------------------------------\n",
        "# Constants & helpers\n",
        "# -------------------------------\n",
        "YARDS_TO_METERS = 0.9144\n",
        "FPS = 10.0\n",
        "FIELD_LENGTH, FIELD_WIDTH = 120.0, 53.3\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "print(\"environment set up!\")\n",
        "def wrap_angle_deg(s):\n",
        "    # map to (-180, 180]\n",
        "    return ((s + 180.0) % 360.0) - 180.0\n",
        "\n",
        "def unify_left_direction(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Mirror rightward plays so all samples are 'left' oriented (x,y, dir, o, ball_land).\"\"\"\n",
        "    if 'play_direction' not in df.columns:\n",
        "        return df\n",
        "    df = df.copy()\n",
        "    right = df['play_direction'].eq('right')\n",
        "    # positions\n",
        "    if 'x' in df.columns: df.loc[right, 'x'] = FIELD_LENGTH - df.loc[right, 'x']\n",
        "    if 'y' in df.columns: df.loc[right, 'y'] = FIELD_WIDTH  - df.loc[right, 'y']\n",
        "    # angles in degrees\n",
        "    for col in ('dir','o'):\n",
        "        if col in df.columns:\n",
        "            df.loc[right, col] = (df.loc[right, col] + 180.0) % 360.0\n",
        "    # ball landing\n",
        "    if 'ball_land_x' in df.columns:\n",
        "        df.loc[right, 'ball_land_x'] = FIELD_LENGTH - df.loc[right, 'ball_land_x']\n",
        "    if 'ball_land_y' in df.columns:\n",
        "        df.loc[right, 'ball_land_y'] = FIELD_WIDTH  - df.loc[right, 'ball_land_y']\n",
        "    return df\n",
        "\n",
        "def invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n",
        "    \"\"\"Invert unified (left) coordinates back to original play direction.\"\"\"\n",
        "    if not play_dir_right:\n",
        "        return float(x_u), float(y_u)\n",
        "    return float(FIELD_LENGTH - x_u), float(FIELD_WIDTH - y_u)\n",
        "\n",
        "# -------------------------------\n",
        "# Config\n",
        "# -------------------------------\n",
        "class Config:\n",
        "    DATA_DIR = Path(\"/kaggle/input/nfl-big-data-bowl-2026-prediction/\")\n",
        "    OUTPUT_DIR = Path(\"./outputs\"); OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "    SAVE_DIR = Path(\"./saved_models\")\n",
        "    SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    MODELS_DIR = Path(\"/kaggle/input/hsiaosuan-sttn/saved_models\")\n",
        "    MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "    # TRAIN=True: Train models and save to ./saved_models\n",
        "    # SUB=True: Use evaluation API (new format) - Cell 9 will start the server\n",
        "    TRAIN = False\n",
        "    SUB   = True\n",
        "\n",
        "    #ÊòØÂê¶‰ΩøÁî®Bi-GRU\n",
        "    BIDIRECTIONAL = True\n",
        "\n",
        "\n",
        "\n",
        "    #ÊòæÂºèÊåáÂÆöÁâπÂæÅÁªÑÔºàÁ°Æ‰øù train/sub ‰∏ÄËá¥Ôºâ\n",
        "    FEATURE_GROUPS = [\n",
        "        'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n",
        "        'velocity_changes','field_position','role_specific','time_features','jerk_features','interaction_features_mid','qb_relative',\n",
        "        # 'interaction_features', #‰øùÂÆàÁâàÊú¨ÂØπÊäóÁâπÂæÅ\n",
        "        # 'curvature_land_features',  #Ëã•Á∫ø‰∏äÊó†ËêΩÁÇπÔºåÂãøÂºÄÂêØ\n",
        "    ]\n",
        "\n",
        "    #Training Setting\n",
        "    SEED = 42\n",
        "    SEEDS = [42, 19, 89, 64]   #Â§öÁßçÂ≠êÈõÜÊàê\n",
        "    N_FOLDS = 5\n",
        "    BATCH_SIZE = 256\n",
        "    EPOCHS = 200\n",
        "    PATIENCE = 30\n",
        "    LEARNING_RATE = 1e-3\n",
        "\n",
        "    WINDOW_SIZE = 10\n",
        "    HIDDEN_DIM = 128\n",
        "    MAX_FUTURE_HORIZON = 94  #‰∏çË¶ÅÂä®Ëøô‰∏™‰∏úË•ø\n",
        "\n",
        "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "set_seed(Config.SEED)\n",
        "\n",
        "def compute_val_rmse(mx, my, X_val_sc, ydx_list, ydy_list, horizon, device, mode=\"per-dim\"):\n",
        "    \"\"\"\n",
        "    ËÆ°ÁÆóÈ™åËØÅÈõÜËΩ®ËøπËØØÂ∑Æ„ÄÇ\n",
        "    mode:\n",
        "      - \"per-dim\"  : sqrt( ( (dx^2+dy^2) ÊÄªÂíå / (2N) ) )  ‚Üê ‰Ω†Êà™ÂõæÈáåÁöÑÂÖ¨ÂºèÔºàÊé®ËçêÁî®‰∫éÂØπÈΩêÔºâ\n",
        "      - \"2d\"       : sqrt( ( (dx^2+dy^2) ÊÄªÂíå / N ) )     ‚Üê ‰∫åÁª¥Ê¨ßÊ∞è RMSEÔºà‰ºöÊØî per-dim Â§ß sqrt(2)Ôºâ\n",
        "      - \"mean-dist\": Âπ≥ÂùáÊ¨ßÊ∞èË∑ùÁ¶ª E[ sqrt(dx^2+dy^2) ]     ‚Üê Êüê‰∫õÊØîËµõÁî®Ëøô‰∏™Âè£ÂæÑ\n",
        "    N = ÊúâÊïàÁöÑÊ†∑Êú¨√óÊó∂Èó¥Ê≠•ÔºàÊåâ mask ÁªüËÆ°Ôºâ\n",
        "    \"\"\"\n",
        "    X_t = torch.tensor(X_val_sc.astype(np.float32)).to(device)\n",
        "    with torch.no_grad():\n",
        "        pdx = mx(X_t).cpu().numpy()   # (N, H)\n",
        "        pdy = my(X_t).cpu().numpy()   # (N, H)\n",
        "\n",
        "    ydx, m = prepare_targets(ydx_list, horizon)  # (N,H), (N,H)\n",
        "    ydy, _ = prepare_targets(ydy_list, horizon)\n",
        "    ydx, ydy, m = ydx.numpy(), ydy.numpy(), m.numpy()\n",
        "\n",
        "    se_sum2d = ((pdx - ydx)**2 + (pdy - ydy)**2) * m\n",
        "    denom = m.sum() + 1e-8\n",
        "\n",
        "    if mode == \"per-dim\":\n",
        "        return float(np.sqrt(se_sum2d.sum() / (2.0 * denom)))\n",
        "    elif mode == \"2d\":\n",
        "        return float(np.sqrt(se_sum2d.sum() / denom))\n",
        "    elif mode == \"mean-dist\":\n",
        "        dist = np.sqrt(se_sum2d)  # ÂÖÉÁ¥†Á∫ßÂºÄÊ†πÂè∑\n",
        "        return float(dist.sum() / denom)\n",
        "    else:\n",
        "        raise ValueError(\"mode must be one of {'per-dim','2d','mean-dist'}\")\n",
        "\n",
        "# -------------------------------\n",
        "# Feature Engineering\n",
        "# -------------------------------\n",
        "class FeatureEngineer:\n",
        "    \"\"\"\n",
        "    Modular, ablation-friendly feature builder (pandas or cuDF pandas-API).\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_groups_to_create):\n",
        "        self.gcols = ['game_id', 'play_id', 'nfl_id']\n",
        "        self.active_groups = feature_groups_to_create\n",
        "        self.feature_creators = {\n",
        "            'distance_rate': self._create_distance_rate_features,\n",
        "            'target_alignment': self._create_target_alignment_features,\n",
        "            'multi_window_rolling': self._create_multi_window_rolling_features,\n",
        "            'extended_lags': self._create_extended_lag_features,\n",
        "            'velocity_changes': self._create_velocity_change_features,\n",
        "            'field_position': self._create_field_position_features,\n",
        "            'role_specific': self._create_role_specific_features,\n",
        "            'time_features': self._create_time_features,\n",
        "            'jerk_features': self._create_jerk_features,\n",
        "            'curvature_land_features': self._create_curvature_land_features,\n",
        "            'interaction_features': self._create_interaction_features, #Ê∑ªÂä†,‰∫§‰∫íÂØπÊäóÁâπÂæÅ\n",
        "            'interaction_features_mid': self._create_interaction_features_mid,  #ÊøÄËøõÁöÑ‰∫§‰∫íÂØπÊäóÁâπÂæÅ\n",
        "            'qb_relative': self._create_qb_relative_features, #Áõ∏ÂØπ‰∫éÂõõÂàÜÂç´‰ΩçÁΩÆÁöÑÂª∫Ê®°\n",
        "        }\n",
        "        self.created_feature_cols = []\n",
        "\n",
        "    def _height_to_feet(self, height_str):\n",
        "        try:\n",
        "            ft, inches = map(int, str(height_str).split('-'))\n",
        "            return ft + inches / 12\n",
        "        except Exception:\n",
        "            return 6.0\n",
        "\n",
        "    def _create_basic_features(self, df):\n",
        "        print(\"Step 1/3: Adding basic features...\")\n",
        "        df = df.copy()\n",
        "        df['player_height_feet'] = df['player_height'].apply(self._height_to_feet)\n",
        "\n",
        "        # Correct kinematics: dir is from +x CCW\n",
        "        dir_rad = np.deg2rad(df['dir'].fillna(0.0).astype('float32'))\n",
        "        df['velocity_x']     = df['s'] * np.cos(dir_rad)\n",
        "        df['velocity_y']     = df['s'] * np.sin(dir_rad)\n",
        "        df['acceleration_x'] = df['a'] * np.cos(dir_rad)\n",
        "        df['acceleration_y'] = df['a'] * np.sin(dir_rad)\n",
        "\n",
        "        # Roles\n",
        "        df['is_offense']  = (df['player_side'] == 'Offense').astype(np.int8)\n",
        "        df['is_defense']  = (df['player_side'] == 'Defense').astype(np.int8)\n",
        "        df['is_receiver'] = (df['player_role'] == 'Targeted Receiver').astype(np.int8)\n",
        "        df['is_coverage'] = (df['player_role'] == 'Defensive Coverage').astype(np.int8)\n",
        "        df['is_passer']   = (df['player_role'] == 'Passer').astype(np.int8)\n",
        "\n",
        "        # Energetics (consistent units)\n",
        "        mass_kg = df['player_weight'].fillna(200.0) / 2.20462\n",
        "        v_ms = df['s'] * YARDS_TO_METERS\n",
        "        df['momentum_x'] = mass_kg * df['velocity_x'] * YARDS_TO_METERS\n",
        "        df['momentum_y'] = mass_kg * df['velocity_y'] * YARDS_TO_METERS\n",
        "        df['kinetic_energy'] = 0.5 * mass_kg * (v_ms ** 2)\n",
        "\n",
        "        # Ball landing geometry (static)\n",
        "        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n",
        "            ball_dx = df['ball_land_x'] - df['x']\n",
        "            ball_dy = df['ball_land_y'] - df['y']\n",
        "            dist = np.hypot(ball_dx, ball_dy)\n",
        "            df['distance_to_ball'] = dist\n",
        "            inv = 1.0 / (dist + 1e-6)\n",
        "            df['ball_direction_x'] = ball_dx * inv\n",
        "            df['ball_direction_y'] = ball_dy * inv\n",
        "            df['closing_speed'] = (\n",
        "                df['velocity_x'] * df['ball_direction_x'] +\n",
        "                df['velocity_y'] * df['ball_direction_y']\n",
        "            )\n",
        "\n",
        "        base = [\n",
        "            'x','y','s','a','o','dir','frame_id','ball_land_x','ball_land_y',\n",
        "            'player_height_feet','player_weight',\n",
        "            'velocity_x','velocity_y','acceleration_x','acceleration_y',\n",
        "            'momentum_x','momentum_y','kinetic_energy',\n",
        "            'is_offense','is_defense','is_receiver','is_coverage','is_passer',\n",
        "            'distance_to_ball','ball_direction_x','ball_direction_y','closing_speed'\n",
        "        ]\n",
        "        self.created_feature_cols.extend([c for c in base if c in df.columns])\n",
        "        return df\n",
        "\n",
        "    # ---- feature groups ----\n",
        "    def _create_distance_rate_features(self, df):\n",
        "        new_cols = []\n",
        "        if 'distance_to_ball' in df.columns:\n",
        "            d = df.groupby(self.gcols)['distance_to_ball'].diff()\n",
        "            df['d2ball_dt']  = d.fillna(0.0) * FPS\n",
        "            df['d2ball_ddt'] = df.groupby(self.gcols)['d2ball_dt'].diff().fillna(0.0) * FPS\n",
        "            df['time_to_intercept'] = (df['distance_to_ball'] /\n",
        "                                       (df['d2ball_dt'].abs() + 1e-3)).clip(0, 10)\n",
        "            new_cols = ['d2ball_dt','d2ball_ddt','time_to_intercept']\n",
        "        return df, new_cols\n",
        "\n",
        "    def _create_target_alignment_features(self, df):\n",
        "        new_cols = []\n",
        "        if {'ball_direction_x','ball_direction_y','velocity_x','velocity_y'}.issubset(df.columns):\n",
        "            df['velocity_alignment'] = df['velocity_x']*df['ball_direction_x'] + df['velocity_y']*df['ball_direction_y']\n",
        "            df['velocity_perpendicular'] = df['velocity_x']*(-df['ball_direction_y']) + df['velocity_y']*df['ball_direction_x']\n",
        "            new_cols.extend(['velocity_alignment','velocity_perpendicular'])\n",
        "            if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n",
        "                df['accel_alignment'] = df['acceleration_x']*df['ball_direction_x'] + df['acceleration_y']*df['ball_direction_y']\n",
        "                new_cols.append('accel_alignment')\n",
        "        return df, new_cols\n",
        "\n",
        "    def _create_multi_window_rolling_features(self, df):\n",
        "        # keep it simple & compatible (works with cuDF pandas-API); vectorized rolling per group\n",
        "        new_cols = []\n",
        "        for window in (3, 5, 10):\n",
        "            for col in ('velocity_x','velocity_y','s','a'):\n",
        "                if col in df.columns:\n",
        "                    r_mean = df.groupby(self.gcols)[col].rolling(window, min_periods=1).mean()\n",
        "                    r_std  = df.groupby(self.gcols)[col].rolling(window, min_periods=1).std()\n",
        "                    # align indices\n",
        "                    r_mean = r_mean.reset_index(level=list(range(len(self.gcols))), drop=True)\n",
        "                    r_std  = r_std.reset_index(level=list(range(len(self.gcols))), drop=True)\n",
        "                    df[f'{col}_roll{window}'] = r_mean\n",
        "                    df[f'{col}_std{window}']  = r_std.fillna(0.0)\n",
        "                    new_cols.extend([f'{col}_roll{window}', f'{col}_std{window}'])\n",
        "        return df, new_cols\n",
        "\n",
        "    def _create_extended_lag_features(self, df):\n",
        "        new_cols = []\n",
        "        for lag in (1,2,3,4,5):\n",
        "            for col in ('x','y','velocity_x','velocity_y'):\n",
        "                if col in df.columns:\n",
        "                    g = df.groupby(self.gcols)[col]\n",
        "                    lagv = g.shift(lag)\n",
        "                    # safe fill for first frames (no \"future\" leakage)\n",
        "                    df[f'{col}_lag{lag}'] = lagv.fillna(g.transform('first'))\n",
        "                    new_cols.append(f'{col}_lag{lag}')\n",
        "        return df, new_cols\n",
        "\n",
        "    def _create_velocity_change_features(self, df):\n",
        "        new_cols = []\n",
        "        if 'velocity_x' in df.columns:\n",
        "            df['velocity_x_change'] = df.groupby(self.gcols)['velocity_x'].diff().fillna(0.0)\n",
        "            df['velocity_y_change'] = df.groupby(self.gcols)['velocity_y'].diff().fillna(0.0)\n",
        "            df['speed_change']      = df.groupby(self.gcols)['s'].diff().fillna(0.0)\n",
        "            d = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n",
        "            df['direction_change']  = wrap_angle_deg(d)\n",
        "            new_cols = ['velocity_x_change','velocity_y_change','speed_change','direction_change']\n",
        "        return df, new_cols\n",
        "\n",
        "    def _create_field_position_features(self, df):\n",
        "        df['dist_from_left'] = df['y']\n",
        "        df['dist_from_right'] = FIELD_WIDTH - df['y']\n",
        "        df['dist_from_sideline'] = np.minimum(df['dist_from_left'], df['dist_from_right'])\n",
        "        df['dist_from_endzone']  = np.minimum(df['x'], FIELD_LENGTH - df['x'])\n",
        "        return df, ['dist_from_sideline','dist_from_endzone']\n",
        "\n",
        "    def _create_role_specific_features(self, df):\n",
        "        new_cols = []\n",
        "        if {'is_receiver','velocity_alignment'}.issubset(df.columns):\n",
        "            df['receiver_optimality'] = df['is_receiver'] * df['velocity_alignment']\n",
        "            df['receiver_deviation']  = df['is_receiver'] * np.abs(df.get('velocity_perpendicular', 0.0))\n",
        "            new_cols.extend(['receiver_optimality','receiver_deviation'])\n",
        "        if {'is_coverage','closing_speed'}.issubset(df.columns):\n",
        "            df['defender_closing_speed'] = df['is_coverage'] * df['closing_speed']\n",
        "            new_cols.append('defender_closing_speed')\n",
        "        return df, new_cols\n",
        "\n",
        "    def _create_time_features(self, df):\n",
        "        df['frames_elapsed']  = df.groupby(self.gcols).cumcount()\n",
        "        df['normalized_time'] = df.groupby(self.gcols)['frames_elapsed'].transform(\n",
        "            lambda x: x / (x.max() + 1e-9)\n",
        "        )\n",
        "        df['time_sin'] = np.sin(2*np.pi*df['normalized_time'])\n",
        "        df['time_cos'] = np.cos(2*np.pi*df['normalized_time'])\n",
        "        return df, ['frames_elapsed','normalized_time','time_sin','time_cos']\n",
        "\n",
        "    def _create_jerk_features(self, df):\n",
        "        new_cols = []\n",
        "        if 'a' in df.columns:\n",
        "            df['jerk'] = df.groupby(self.gcols)['a'].diff().fillna(0.0) * FPS\n",
        "            new_cols.append('jerk')\n",
        "        if {'acceleration_x','acceleration_y'}.issubset(df.columns):\n",
        "            df['jerk_x'] = df.groupby(self.gcols)['acceleration_x'].diff().fillna(0.0) * FPS\n",
        "            df['jerk_y'] = df.groupby(self.gcols)['acceleration_y'].diff().fillna(0.0) * FPS\n",
        "            new_cols.extend(['jerk_x','jerk_y'])\n",
        "        return df, new_cols\n",
        "    def _create_curvature_land_features(self, df):\n",
        "        \"\"\"\n",
        "        -ËêΩÁÇπ‰æßÂêëÂÅèÂ∑ÆÔºàÁ¨¶Âè∑ÔºâÔºölanding_point Áõ∏ÂØπ‚ÄúÂΩìÂâçËøêÂä®ÊñπÂêë‚ÄùÁöÑÂ∑¶Âè≥ÂÅèÁ¶ª\n",
        "          lateral = cross(u_dir, vector_to_land)Ôºà>0 Ë°®Á§∫ËêΩÁÇπÂú®ËøêÂä®ÊñπÂêëÂ∑¶‰æßÔºâ\n",
        "        -bearing_to_land_signed: ËøêÂä®ÊñπÂêë vs ËêΩÁÇπÊñπ‰ΩçËßí\n",
        "        -ÈÄüÂ∫¶ÂΩí‰∏ÄÂåñÊõ≤ÁéáÔºö wrap(Œîdir)/ (s*Œît) ÔºåÁ™óÂè£Âåñ(3/5) ÁöÑÂùáÂÄº/ÁªùÂØπÂÄº\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "        # ‰æßÂêëÂÅèÂ∑Æ & bearing_to_land\n",
        "        if {'ball_land_x','ball_land_y'}.issubset(df.columns):\n",
        "            dx = df['ball_land_x'] - df['x']\n",
        "            dy = df['ball_land_y'] - df['y']\n",
        "            bearing = np.arctan2(dy, dx)\n",
        "            a_dir = np.deg2rad(df['dir'].fillna(0.0).values)\n",
        "            # ÊúâÁ¨¶Âè∑Êñπ‰ΩçÂ∑Æ\n",
        "            df['bearing_to_land_signed'] = np.rad2deg(np.arctan2(np.sin(bearing - a_dir), np.cos(bearing - a_dir)))\n",
        "            # ‰æßÂêëÂÅèÂ∑ÆÔºöd √ó u (2D cross, z ÂàÜÈáè)\n",
        "            ux, uy = np.cos(a_dir), np.sin(a_dir)\n",
        "            df['land_lateral_offset'] = dy*ux - dx*uy  # >0 ËêΩÁÇπÂú®Â∑¶‰æß\n",
        "\n",
        "        # Êõ≤ÁéáÔºàÊåâÂ∫èÂàóÔºâ\n",
        "        ddir = df.groupby(self.gcols)['dir'].diff().fillna(0.0)\n",
        "        ddir = ((ddir + 180.0) % 360.0) - 180.0\n",
        "        curvature = np.deg2rad(ddir).astype('float32') / (df['s'].replace(0, np.nan).astype('float32') * 0.1 + 1e-6)\n",
        "        df['curvature_signed'] = curvature.fillna(0.0)\n",
        "        df['curvature_abs'] = df['curvature_signed'].abs()\n",
        "\n",
        "        # Á™óÂè£ÂùáÂÄºÔºà3/5Ôºâ\n",
        "        for w in (3,5):\n",
        "            r = df.groupby(self.gcols)['curvature_signed'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "            df[f'curv_signed_roll{w}'] = r\n",
        "            r2 = df.groupby(self.gcols)['curvature_abs'].rolling(w, min_periods=1).mean().reset_index(level=[0,1,2], drop=True)\n",
        "            df[f'curv_abs_roll{w}'] = r2\n",
        "\n",
        "        new_cols = ['bearing_to_land_signed','land_lateral_offset',\n",
        "                    'curvature_signed','curvature_abs','curv_signed_roll3','curv_abs_roll3',\n",
        "                    'curv_signed_roll5','curv_abs_roll5']\n",
        "        return df, [c for c in new_cols if c in df.columns]\n",
        "\n",
        "    def _create_interaction_features(self, df, speed_eps=0.5):\n",
        "        \"\"\"\n",
        "        Lite Receiver‚ÄìDefender interaction features (conservative):\n",
        "          - Only K=1 (nearest opponent)\n",
        "          - Compute ONLY for target player (player_to_predict==True) if column exists\n",
        "          - Features:\n",
        "              opp_dmin        : distance to nearest opposite-side opponent (clipped [0,30])\n",
        "              opp_close_rate  : (v_opp - v_self) projected onto (opp->self) unit vector (clipped [-10,10])\n",
        "              opp_leverage    : sign of cross( self_vel , self->opp ) in 2D ( {-1,0,1} ), gated by speed\n",
        "        \"\"\"\n",
        "        import numpy as np\n",
        "\n",
        "        need = ['x','y','velocity_x','velocity_y','player_side','frame_id']\n",
        "        for c in need:\n",
        "            if c not in df.columns:\n",
        "                return df, []  # missing columns ‚Üí skip safely\n",
        "\n",
        "        out_cols = ['opp_dmin','opp_close_rate','opp_leverage']\n",
        "        for c in out_cols:\n",
        "            if c not in df.columns:\n",
        "                df[c] = np.nan\n",
        "\n",
        "        key = ['game_id','play_id','frame_id']\n",
        "        use_mask_global = ('player_to_predict' in df.columns)\n",
        "\n",
        "        for _, g in df.groupby(key, sort=False):\n",
        "            idx = g.index.values\n",
        "            if len(g) <= 1:\n",
        "                continue\n",
        "\n",
        "            pos = g[['x','y']].values.astype('float32')          # (N,2)\n",
        "            vel = g[['velocity_x','velocity_y']].values.astype('float32')\n",
        "            side_off = (g['player_side'].values == 'Offense')    # (N,)\n",
        "            side_def = ~side_off\n",
        "\n",
        "            # only compute for target players if available\n",
        "            if use_mask_global:\n",
        "                tgt_mask = g['player_to_predict'].astype(bool).values\n",
        "            else:\n",
        "                tgt_mask = np.ones(len(g), dtype=bool)\n",
        "\n",
        "            def _assign(A_mask, B_mask):\n",
        "                A_mask = A_mask & tgt_mask\n",
        "                A_idx = np.where(A_mask)[0]\n",
        "                B_idx = np.where(B_mask)[0]\n",
        "                if len(A_idx)==0 or len(B_idx)==0:\n",
        "                    return\n",
        "\n",
        "                Apos, Bpos = pos[A_idx], pos[B_idx]\n",
        "                Avel, Bvel = vel[A_idx], vel[B_idx]\n",
        "\n",
        "                # pairwise distances (K=1)\n",
        "                dx = Apos[:,None,0] - Bpos[None,:,0]\n",
        "                dy = Apos[:,None,1] - Bpos[None,:,1]\n",
        "                D  = np.sqrt(dx*dx + dy*dy) + 1e-6                 # (Na,Nb)\n",
        "                j  = np.argmin(D, axis=1)                          # nearest opponent\n",
        "\n",
        "                dmin = D[np.arange(len(A_idx)), j]\n",
        "                dmin = np.clip(dmin, 0.0, 30.0)                    # robust clip\n",
        "\n",
        "                # closing rate\n",
        "                r   = Apos - Bpos[j]                                # opp->self\n",
        "                u   = r / (np.linalg.norm(r, axis=1, keepdims=True) + 1e-6)\n",
        "                v_rel = Bvel[j] - Avel\n",
        "                close = np.einsum('ij,ij->i', v_rel, u)\n",
        "                close = np.clip(close, -10.0, 10.0)\n",
        "\n",
        "                # leverage sign, gated by own speed\n",
        "                speed   = np.linalg.norm(Avel, axis=1)\n",
        "                to_opp  = Bpos[j] - Apos                            # self->opp\n",
        "                cross_z = to_opp[:,0]*Avel[:,1] - to_opp[:,1]*Avel[:,0]\n",
        "                lever   = np.where(speed > speed_eps, np.sign(cross_z), 0).astype('int8')\n",
        "\n",
        "                rows = idx[A_idx]\n",
        "                df.loc[rows, 'opp_dmin']       = dmin\n",
        "                df.loc[rows, 'opp_close_rate'] = close\n",
        "                df.loc[rows, 'opp_leverage']   = lever\n",
        "\n",
        "            # Offense w.r.t. Defense\n",
        "            _assign(side_off, side_def)\n",
        "            # Defense w.r.t. Offense\n",
        "            _assign(side_def, side_off)\n",
        "\n",
        "        return df, out_cols\n",
        "\n",
        "    def _create_interaction_features_mid(self, df, speed_eps=0.5, k=2, radius=10.0):\n",
        "        \"\"\"\n",
        "        Medium-aggressive Receiver‚ÄìDefender interaction:\n",
        "          - K=2 nearest opposite-side opponents (still per-frame)\n",
        "          - Keep target-only computation if 'player_to_predict' exists\n",
        "          - New features (small set):\n",
        "              opp_d2                 : 2nd nearest distance (clipped [0,30])\n",
        "              opp_dmean_k2           : mean distance of top-2\n",
        "              opp_close_rate_min_k2  : min closing rate among top-2 (more threatening)\n",
        "              opp_pursuit_error_min_k2: min |angle( v_opp , dir to self )| in degrees\n",
        "              opp_density_r10        : #opponents within radius (10 yards)\n",
        "          - Light temporal smoothing on previous base features:\n",
        "              opp_dmin_roll3, opp_close_rate_roll3\n",
        "        \"\"\"\n",
        "\n",
        "        need = ['x','y','velocity_x','velocity_y','player_side','frame_id']\n",
        "        for c in need:\n",
        "            if c not in df.columns:\n",
        "                return df, []  # safe skip\n",
        "\n",
        "        # ‰øùËØÅ‰øùÂÆàÁâàÁöÑ‰∏âÂàóÂ≠òÂú®Ôºà‰ª•‰æø rollingÔºâ\n",
        "        base_cols = ['opp_dmin','opp_close_rate','opp_leverage']\n",
        "        for c in base_cols:\n",
        "            if c not in df.columns:\n",
        "                df[c] = np.nan\n",
        "\n",
        "        # Êñ∞Â¢ûÂàóÔºàÂàùÂßãÂåñÔºâ\n",
        "        new_cols = ['opp_d2','opp_dmean_k2','opp_close_rate_min_k2',\n",
        "                    'opp_pursuit_error_min_k2','opp_density_r10','ally_density_r10']\n",
        "        for c in new_cols:\n",
        "            if c not in df.columns:\n",
        "                df[c] = np.nan\n",
        "\n",
        "        key = ['game_id','play_id','frame_id']\n",
        "        use_mask_global = ('player_to_predict' in df.columns)\n",
        "\n",
        "        def angle_between(v1, v2, eps=1e-6):\n",
        "            # ËøîÂõûÂºßÂ∫¶ÁöÑÂ§πËßí (0..pi)\n",
        "            dot = np.einsum('ij,ij->i', v1, v2)\n",
        "            n1  = np.linalg.norm(v1, axis=1) + eps\n",
        "            n2  = np.linalg.norm(v2, axis=1) + eps\n",
        "            cos = np.clip(dot / (n1*n2), -1.0, 1.0)\n",
        "            return np.arccos(cos)\n",
        "\n",
        "        for _, g in df.groupby(key, sort=False):\n",
        "            idx = g.index.values\n",
        "            if len(g) <= 1:\n",
        "                continue\n",
        "\n",
        "            pos = g[['x','y']].values.astype('float32')\n",
        "            vel = g[['velocity_x','velocity_y']].values.astype('float32')\n",
        "            side_off = (g['player_side'].values == 'Offense')\n",
        "            side_def = ~side_off\n",
        "\n",
        "            # target-only\n",
        "            if use_mask_global:\n",
        "                tgt_mask = g['player_to_predict'].astype(bool).values\n",
        "            else:\n",
        "                tgt_mask = np.ones(len(g), dtype=bool)\n",
        "\n",
        "            def _assign(A_mask, B_mask):\n",
        "                A_mask = A_mask & tgt_mask\n",
        "                A_idx = np.where(A_mask)[0]\n",
        "                B_idx = np.where(B_mask)[0]\n",
        "                if len(A_idx)==0 or len(B_idx)==0:\n",
        "                    return\n",
        "\n",
        "                Apos, Bpos = pos[A_idx], pos[B_idx]\n",
        "                Avel, Bvel = vel[A_idx], vel[B_idx]\n",
        "\n",
        "                # pairwise distances\n",
        "                dx = Apos[:,None,0] - Bpos[None,:,0]\n",
        "                dy = Apos[:,None,1] - Bpos[None,:,1]\n",
        "                D  = np.sqrt(dx*dx + dy*dy) + 1e-6          # (Na,Nb)\n",
        "                # Top-2 indices along last axis\n",
        "                k_use = min(k, D.shape[1])\n",
        "                part = np.argpartition(D, kth=range(k_use), axis=1)[:, :k_use]  # (Na, k_use)\n",
        "                # Gather top-2 distances and opponent indices\n",
        "                rows = np.arange(len(A_idx))[:,None]\n",
        "                d_top = D[rows, part]                 # (Na, k_use)\n",
        "                jidx  = part                          # opponent indices for each A\n",
        "\n",
        "                # Âü∫‰∫é K=2 Êé®ÂØºÁöÑÊ¥æÁîüÈáè\n",
        "                d_sorted = np.sort(d_top, axis=1)     # ÂçáÂ∫èÔºöd1, d2\n",
        "                d1 = d_sorted[:,0]\n",
        "                d2 = d_sorted[:,1] if k_use >= 2 else d_sorted[:,0]\n",
        "\n",
        "                # ÂÖ≥Èó≠ÁéáÔºöÂØπÊØè‰∏™ÊâÄÈÄâÂØπÊâãËÆ°ÁÆóÔºåÂÜçÂèñ min\n",
        "                # opp->self Âçï‰ΩçÂêëÈáè\n",
        "                # ÂÖàÂèñÊúÄËøëÂØπÊâãÁî®‰∫é opp_dmin/close_rateÔºà‰∏é‰øùÂÆàÁâà‰øùÊåÅ‰∏ÄËá¥Âê´‰πâÔºâ\n",
        "                j1 = jidx[np.arange(len(A_idx)), np.argmin(d_top, axis=1)]\n",
        "                r1 = Apos - Bpos[j1]                         # opp->self\n",
        "                u1 = r1 / (np.linalg.norm(r1, axis=1, keepdims=True) + 1e-6)\n",
        "                vrel1 = Bvel[j1] - Avel\n",
        "                close1 = np.einsum('ij,ij->i', vrel1, u1)\n",
        "                close1 = np.clip(close1, -10.0, 10.0)\n",
        "\n",
        "                #Á¨¨‰∫åËøëÂØπÊâãÁöÑÂÖ≥Èó≠Áéá\n",
        "                if k_use >= 2:\n",
        "                    # ÂèñÁ¨¨‰∫åËøëÔºà‰∏çÊòØÊúÄÂ∞èÂÄº‰ΩçÁΩÆÔºâ\n",
        "                    order = np.argsort(d_top, axis=1)\n",
        "                    j2 = jidx[rows[:,0], order[:,1]]\n",
        "                    r2 = Apos - Bpos[j2]\n",
        "                    u2 = r2 / (np.linalg.norm(r2, axis=1, keepdims=True) + 1e-6)\n",
        "                    vrel2 = Bvel[j2] - Avel\n",
        "                    close2 = np.einsum('ij,ij->i', vrel2, u2)\n",
        "                    close2 = np.clip(close2, -10.0, 10.0)\n",
        "                    close_min_k2 = np.minimum(close1, close2)\n",
        "                else:\n",
        "                    close2 = close1\n",
        "                    close_min_k2 = close1\n",
        "\n",
        "                #pursuit errorÔºöÂØπÊâãÈÄüÂ∫¶ vs ÊåáÂêëÊàëÊñπËøûÁ∫øÔºàÂèñÊúÄÂ∞èÁªùÂØπÂÄºÔºåÂçï‰ΩçÂ∫¶Ôºâ\n",
        "                # ËßíÂ∫¶=atan2(|cross|, dot)‚â°arccos(dot/(||v||¬∑||u||))\n",
        "                ang1 = angle_between(Bvel[j1], u1) * 180.0/np.pi\n",
        "                if k_use >= 2:\n",
        "                    ang2 = angle_between(Bvel[j2], u2) * 180.0/np.pi\n",
        "                    perr_min_k2 = np.minimum(np.abs(ang1), np.abs(ang2))\n",
        "                else:\n",
        "                    perr_min_k2 = np.abs(ang1)\n",
        "\n",
        "                #Â±ÄÈÉ®ÂØÜÂ∫¶ÔºàÂçäÂæÑÂÜÖÁöÑÂØπÊâãÊï∞Ôºâ\n",
        "                density = (D <= radius).sum(axis=1).astype('float32')\n",
        "                #Âêå‰æßÂØÜÂ∫¶ÔºàAÊñπÂÜÖÁöÑ self-self Ë∑ùÁ¶ªÔºâÔºõÁÆÄÂçïËµ∑ËßÅÔºöÁî® Apos ‰∏§‰∏§Ë∑ùÁ¶ª\n",
        "                rows_idx = idx[A_idx]\n",
        "\n",
        "                # Âêå‰æßÂØÜÂ∫¶\n",
        "                try:\n",
        "                    from scipy.spatial.distance import cdist\n",
        "                    A2A = cdist(Apos, Apos) + 1e-6\n",
        "                except Exception:\n",
        "                    # Êó† scipy ÁöÑ fallback\n",
        "                    diff = Apos[:,None,:] - Apos[None,:,:]\n",
        "                    A2A = np.sqrt((diff * diff).sum(-1)) + 1e-6\n",
        "                same_density = (A2A <= radius).sum(axis=1) - 1  # Ëá™Ë∫´-1\n",
        "\n",
        "                # ÂÜôÂõû\n",
        "                df.loc[rows_idx, 'ally_density_r10'] = same_density.astype('float32')\n",
        "                df.loc[rows_idx, 'opp_dmin']       = np.clip(d1, 0.0, 30.0)\n",
        "                df.loc[rows_idx, 'opp_close_rate'] = close1\n",
        "                df.loc[rows_idx, 'opp_d2']                 = np.clip(d2, 0.0, 30.0)\n",
        "                df.loc[rows_idx, 'opp_dmean_k2']           = (np.clip(d1,0,30)+np.clip(d2,0,30))/2.0\n",
        "                df.loc[rows_idx, 'opp_close_rate_min_k2']  = close_min_k2\n",
        "                df.loc[rows_idx, 'opp_pursuit_error_min_k2']= perr_min_k2\n",
        "                df.loc[rows_idx, 'opp_density_r10']        = density\n",
        "\n",
        "            # Offense vs Defense\n",
        "            _assign(side_off, side_def)\n",
        "            # Defense vs Offense\n",
        "            _assign(side_def, side_off)\n",
        "\n",
        "        # ËΩªÈáèÊó∂Â∫èÂπ≥ÊªëÔºà3 Â∏ß rollingÔºâ\n",
        "        # Ê≥®ÊÑèÔºö‰ª•‰∏™‰ΩìÁª¥Â∫¶ÂàÜÁªÑÔºå‰∏çË∑®‰∫∫\n",
        "        for col in ['opp_dmin','opp_close_rate']:\n",
        "            if col in df.columns:\n",
        "                r = (\n",
        "                    df.groupby(self.gcols)[col]\n",
        "                      .rolling(3, min_periods=1).mean()\n",
        "                      .reset_index(level=list(range(len(self.gcols))), drop=True)\n",
        "                )\n",
        "                df[f'{col}_roll3'] = r\n",
        "\n",
        "        out_cols = base_cols + new_cols + ['opp_dmin_roll3','opp_close_rate_roll3']\n",
        "        # Âè™ËøîÂõûÊú¨ÂáΩÊï∞‚ÄúÊñ∞Â¢û/Êõ¥Êñ∞‚ÄùÁöÑÂàóÔºàÂ¶ÇÊûúÊúâÁöÑÊù•Ëá™‰øùÂÆàÁâàÂ∞±‰∏çÈáçÂ§çËÆ°Êï∞‰πüÊ≤°ÂÖ≥Á≥ªÔºâ\n",
        "        out_cols = [c for c in out_cols if c in df.columns]\n",
        "        return df, out_cols\n",
        "\n",
        "    def _create_qb_relative_features(self, df):\n",
        "        \"\"\"\n",
        "        QB-relative geometry (per frame):\n",
        "          - qb_distance\n",
        "          - vel_to_qb_alignment, vel_to_qb_perp\n",
        "          - bearing_to_qb_signed (player facing vs vector to QB)\n",
        "        ‰ªÖ‰æùËµñ: x,y,velocity_x,velocity_y,dir,player_role,frame_id\n",
        "        \"\"\"\n",
        "        need = ['x','y','velocity_x','velocity_y','dir','player_role','frame_id']\n",
        "        for c in need:\n",
        "            if c not in df.columns:\n",
        "                return df, []  # Áº∫ÂàóÂàôÂÆâÂÖ®Ë∑≥Ëøá\n",
        "\n",
        "        out_cols = ['qb_distance','vel_to_qb_alignment','vel_to_qb_perp','bearing_to_qb_signed','bearing_to_qb_sin','bearing_to_qb_cos']\n",
        "        for c in out_cols:\n",
        "            if c not in df.columns:\n",
        "                df[c] = np.nan\n",
        "\n",
        "        key = ['game_id','play_id','frame_id']\n",
        "        for _, g in df.groupby(key, sort=False):\n",
        "            idx = g.index.values\n",
        "\n",
        "            # ÊâæÊú¨Â∏ß QBÔºàÈÄöÂ∏∏ÂîØ‰∏ÄÔºõËã•Â§ö‰∫é1ÂèñÁ¨¨‰∏Ä‰∏™ÔºõÊâæ‰∏çÂà∞ÂàôË∑≥ËøáÔºâ\n",
        "            qb_rows = g[g['player_role'] == 'Passer']\n",
        "            if qb_rows.empty:\n",
        "                continue\n",
        "            qb_x = float(qb_rows.iloc[0]['x'])\n",
        "            qb_y = float(qb_rows.iloc[0]['y'])\n",
        "\n",
        "            dx = g['x'].values.astype('float32') - qb_x\n",
        "            dy = g['y'].values.astype('float32') - qb_y\n",
        "            dist = np.sqrt(dx*dx + dy*dy) + 1e-6\n",
        "            ux, uy = dx/dist, dy/dist  # QB->player Âçï‰ΩçÂêëÈáè\n",
        "\n",
        "            vx = g['velocity_x'].values.astype('float32')\n",
        "            vy = g['velocity_y'].values.astype('float32')\n",
        "\n",
        "            align = vx*ux + vy*uy\n",
        "            perp  = vx*(-uy) + vy*ux\n",
        "\n",
        "            # bearing Â∑ÆÔºöÁé©ÂÆ∂ÊúùÂêë vs ÊåáÂêë QB ÁöÑÊñπÂêëÔºàÊúâÁ¨¶Âè∑Ôºå(-180,180]Ôºâ\n",
        "            dir_rad = np.deg2rad(g['dir'].fillna(0.0).astype('float32').values)\n",
        "            to_qb_angle = np.arctan2(-dy, -dx)  # player->QB\n",
        "            bearing = np.rad2deg(np.arctan2(np.sin(to_qb_angle - dir_rad),\n",
        "                                            np.cos(to_qb_angle - dir_rad)))\n",
        "\n",
        "            df.loc[idx, 'qb_distance'] = dist\n",
        "            df.loc[idx, 'vel_to_qb_alignment'] = align\n",
        "            df.loc[idx, 'vel_to_qb_perp'] = perp\n",
        "            df.loc[idx, 'bearing_to_qb_signed'] = bearing\n",
        "            df.loc[idx, 'bearing_to_qb_sin'] = np.sin(np.deg2rad(bearing))\n",
        "            df.loc[idx, 'bearing_to_qb_cos'] = np.cos(np.deg2rad(bearing))\n",
        "\n",
        "        return df, out_cols\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def transform(self, df):\n",
        "        df = df.copy().sort_values(['game_id','play_id','nfl_id','frame_id'])\n",
        "        df = self._create_basic_features(df)\n",
        "\n",
        "        print(\"\\nStep 2/3: Adding selected advanced features...\")\n",
        "        for group_name in self.active_groups:\n",
        "            if group_name in self.feature_creators:\n",
        "                creator = self.feature_creators[group_name]\n",
        "                df, new_cols = creator(df)\n",
        "                self.created_feature_cols.extend(new_cols)\n",
        "                print(f\"  [+] Added '{group_name}' ({len(new_cols)} cols)\")\n",
        "            else:\n",
        "                print(f\"  [!] Unknown feature group: {group_name}\")\n",
        "\n",
        "        final_cols = sorted(set(self.created_feature_cols))\n",
        "        print(f\"\\nTotal features created: {len(final_cols)}\")\n",
        "        return df, final_cols\n",
        "\n",
        "# -------------------------------\n",
        "# Sequence builder (unified frame + safe targets)\n",
        "# -------------------------------\n",
        "def build_play_direction_map(df_in: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Return a Series indexed by (game_id, play_id) with values 'left'/'right'.\n",
        "    This keeps a clean MultiIndex that works for both pandas and cuDF pandas-API.\n",
        "    \"\"\"\n",
        "    s = (\n",
        "        df_in[['game_id','play_id','play_direction']]\n",
        "        .drop_duplicates()\n",
        "        .set_index(['game_id','play_id'])['play_direction']\n",
        "    )\n",
        "    return s  # MultiIndex Series\n",
        "\n",
        "\n",
        "def apply_direction_to_df(df: pd.DataFrame, dir_map: pd.Series) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Attach play_direction (if missing) and then unify to 'left'.\n",
        "    dir_map must be the MultiIndex Series produced by build_play_direction_map.\n",
        "    \"\"\"\n",
        "    if 'play_direction' not in df.columns:\n",
        "        dir_df = dir_map.reset_index()  # -> columns: game_id, play_id, play_direction\n",
        "        df = df.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n",
        "    return unify_left_direction(df)\n",
        "\n",
        "\n",
        "#[A] Áªü‰∏ÄÈîÆÁ±ªÂûãÁöÑËæÖÂä©ÂáΩÊï∞\n",
        "def _canonicalize_key_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    for c in ('game_id','play_id','nfl_id'):\n",
        "        if c in df.columns:\n",
        "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    # ‰∏¢ÊéâÁº∫Â§±ÈîÆ\n",
        "    df = df.dropna(subset=['game_id','play_id','nfl_id'])\n",
        "    # Áªü‰∏Ä‰∏∫ int64\n",
        "    df['game_id'] = df['game_id'].astype('int64')\n",
        "    df['play_id'] = df['play_id'].astype('int64')\n",
        "    df['nfl_id']  = df['nfl_id'].astype('int64')\n",
        "    return df\n",
        "\n",
        "def prepare_sequences_with_advanced_features(\n",
        "        input_df, output_df=None, test_template=None,\n",
        "        is_training=True, window_size=10, feature_groups=None):\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PREPARING SEQUENCES WITH ADVANCED FEATURES (UNIFIED FRAME)\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Window size: {window_size}\")\n",
        "\n",
        "    # --- [B] ÂÖàÁªü‰∏ÄÈîÆÁ±ªÂûãÔºàËæìÂÖ•/ËæìÂá∫/ÊµãËØïÊ®°ÊùøÈÉΩÂ§ÑÁêÜÔºâ ---\n",
        "    input_df  = _canonicalize_key_dtypes(input_df)\n",
        "    if is_training:\n",
        "        assert output_df is not None\n",
        "        output_df = _canonicalize_key_dtypes(output_df)\n",
        "    else:\n",
        "        assert test_template is not None\n",
        "        test_template = _canonicalize_key_dtypes(test_template)\n",
        "\n",
        "    if feature_groups is None:\n",
        "        feature_groups = [\n",
        "            'distance_rate','target_alignment','multi_window_rolling','extended_lags',\n",
        "            'velocity_changes','field_position','role_specific','time_features',\n",
        "            'jerk_features','interaction_features_mid'\n",
        "        ]\n",
        "\n",
        "    # --- Direction map & unify ---\n",
        "    dir_map   = build_play_direction_map(input_df)\n",
        "    input_df_u= unify_left_direction(input_df)\n",
        "\n",
        "    if is_training:\n",
        "        out_u = apply_direction_to_df(output_df, dir_map)\n",
        "        target_rows   = out_u\n",
        "        target_groups = out_u[['game_id','play_id','nfl_id']].drop_duplicates()\n",
        "    else:\n",
        "        if 'play_direction' not in test_template.columns:\n",
        "            dir_df = dir_map.reset_index()\n",
        "            test_template = test_template.merge(dir_df, on=['game_id','play_id'], how='left', validate='many_to_one')\n",
        "        target_rows   = test_template\n",
        "        target_groups = test_template[['game_id','play_id','nfl_id','play_direction']].drop_duplicates()\n",
        "\n",
        "    assert target_rows[['game_id','play_id','play_direction']].isna().sum().sum() == 0, \\\n",
        "        \"play_direction merge failed; check (game_id, play_id) coverage\"\n",
        "    print(\"play_direction merge OK:\", target_rows['play_direction'].value_counts(dropna=False).to_dict())\n",
        "\n",
        "    # --- FE ---\n",
        "    fe = FeatureEngineer(feature_groups)\n",
        "    processed_df, feature_cols = fe.transform(input_df_u)\n",
        "\n",
        "    # --- Build sequences ---\n",
        "    print(\"\\nStep 3/3: Creating sequences...\")\n",
        "    processed_df = processed_df.set_index(['game_id','play_id','nfl_id']).sort_index()\n",
        "    grouped = processed_df.groupby(level=['game_id','play_id','nfl_id'])\n",
        "\n",
        "    # [C] ÂèØÈÄâÔºöÊâìÂç∞ÈîÆË¶ÜÁõñÁéáÔºåÂø´ÈÄüÂÆö‰Ωç miss ÁöÑÁúüÊ≠£ÂéüÂõ†\n",
        "    avail_keys = (\n",
        "        processed_df.reset_index()[['game_id','play_id','nfl_id']]\n",
        "        .drop_duplicates()\n",
        "    )\n",
        "    inter = target_groups[['game_id','play_id','nfl_id']].merge(\n",
        "        avail_keys, on=['game_id','play_id','nfl_id'], how='inner'\n",
        "    )\n",
        "    print(f\"[COVERAGE] target_keys={len(target_groups)} | \"\n",
        "          f\"input_keys={len(avail_keys)} | \"\n",
        "          f\"matched={len(inter)}\")\n",
        "\n",
        "    # helpful indices\n",
        "    idx_x = feature_cols.index('x')\n",
        "    idx_y = feature_cols.index('y')\n",
        "\n",
        "    sequences, targets_dx, targets_dy, targets_fids, seq_meta = [], [], [], [], []\n",
        "\n",
        "    it = target_groups.itertuples(index=False)\n",
        "    it = tqdm(list(it), total=len(target_groups), desc=\"Creating sequences\")\n",
        "\n",
        "    for row in it:\n",
        "        gid = row[0]; pid = row[1]; nid = row[2]\n",
        "        play_dir = row[3] if (not is_training and len(row) >= 4) else None\n",
        "        key = (gid, pid, nid)\n",
        "\n",
        "        try:\n",
        "            group_df = grouped.get_group(key)\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        input_window = group_df.tail(window_size)\n",
        "\n",
        "        # --- [D] ËÆ≠ÁªÉÁ´Ø‰πüÂÖÅËÆ∏Â∑¶‰æßÂ°´ÂÖÖÔºà‰∏éÊµãËØï‰∏ÄËá¥ÔºâÔºåÈÅøÂÖçÂÖ®Ë¢´ <window_size ‰∏¢ÂºÉ ---\n",
        "        if len(input_window) < window_size:\n",
        "            pad_len = window_size - len(input_window)\n",
        "            pad_df = pd.DataFrame(np.nan, index=range(pad_len), columns=input_window.columns)\n",
        "            input_window = pd.concat([pad_df, input_window], ignore_index=True)\n",
        "\n",
        "        # input_window = input_window.fillna(group_df.mean(numeric_only=True))\n",
        "        input_window = input_window.fillna(input_window.mean(numeric_only=True))\n",
        "        seq = input_window[feature_cols].values\n",
        "\n",
        "        if np.isnan(seq).any():\n",
        "            seq = np.nan_to_num(seq, nan=0.0)\n",
        "\n",
        "        sequences.append(seq)\n",
        "\n",
        "        if is_training:\n",
        "            out_grp = target_rows[\n",
        "                (target_rows['game_id']==gid) &\n",
        "                (target_rows['play_id']==pid) &\n",
        "                (target_rows['nfl_id']==nid)\n",
        "            ].sort_values('frame_id')\n",
        "            if len(out_grp)==0:\n",
        "                sequences.pop()  # ÂõûÊªö\n",
        "                continue\n",
        "\n",
        "            last_x = seq[-1, idx_x]\n",
        "            last_y = seq[-1, idx_y]\n",
        "            dx = out_grp['x'].values - last_x\n",
        "            dy = out_grp['y'].values - last_y\n",
        "\n",
        "            targets_dx.append(dx.astype(np.float32))\n",
        "            targets_dy.append(dy.astype(np.float32))\n",
        "            targets_fids.append(out_grp['frame_id'].values.astype(np.int32))\n",
        "\n",
        "        seq_meta.append({\n",
        "            'game_id': gid,\n",
        "            'play_id': pid,\n",
        "            'nfl_id': nid,\n",
        "            'frame_id': int(input_window.iloc[-1]['frame_id']) if len(input_window) else -1,\n",
        "            'play_direction': (None if is_training else play_dir),\n",
        "        })\n",
        "\n",
        "    print(f\"Created {len(sequences)} sequences with {len(feature_cols)} features each\")\n",
        "\n",
        "    if is_training:\n",
        "        return sequences, targets_dx, targets_dy, targets_fids, seq_meta, feature_cols, dir_map\n",
        "    return sequences, seq_meta, feature_cols, dir_map\n",
        "\n",
        "# -------------------------------\n",
        "# Tools for model saving & loading\n",
        "# -------------------------------\n",
        "def _seed_dir(base_dir: Path, seed: int) -> Path:\n",
        "    d = base_dir / f\"seed_{seed}\"\n",
        "    d.mkdir(parents=True, exist_ok=True)\n",
        "    return d\n",
        "\n",
        "def save_fold_artifacts(seed:int, fold:int, scaler, mx:nn.Module, my:nn.Module, base_dir:Path):\n",
        "    sdir = _seed_dir(base_dir, seed)\n",
        "    joblib.dump(scaler, sdir / f\"scaler_fold{fold}.pkl\")\n",
        "    torch.save(mx.state_dict(), sdir / f\"model_dx_fold{fold}.pt\")\n",
        "    torch.save(my.state_dict(), sdir / f\"model_dy_fold{fold}.pt\")\n",
        "\n",
        "def write_meta(feature_cols:list, cfg:Config, base_dir:Path):\n",
        "    meta = {\n",
        "        \"seeds\": cfg.SEEDS,\n",
        "        \"n_folds\": cfg.N_FOLDS,\n",
        "        \"feature_cols\": feature_cols,\n",
        "        \"window_size\": cfg.WINDOW_SIZE,\n",
        "        \"max_future_horizon\": cfg.MAX_FUTURE_HORIZON,\n",
        "        \"feature_groups\": cfg.FEATURE_GROUPS,\n",
        "        \"version\": 1,\n",
        "        \"hidden_dim\": cfg.HIDDEN_DIM,\n",
        "        \"bidirectional\": getattr(cfg, \"BIDIRECTIONAL\", False),\n",
        "    }\n",
        "    with open(base_dir / \"meta.json\", \"w\") as f:\n",
        "        json.dump(meta, f)\n",
        "    print(f\"[META] wrote meta.json to {base_dir}\")\n",
        "\n",
        "def load_saved_ensemble(cfg:Config, base_dir:Path):\n",
        "    meta_path = base_dir / \"meta.json\"\n",
        "    assert meta_path.exists(), f\"meta.json not found: {meta_path}\"\n",
        "    with open(meta_path, \"r\") as f:\n",
        "        meta = json.load(f)\n",
        "\n",
        "    feature_cols = meta[\"feature_cols\"]\n",
        "    horizon = int(meta[\"max_future_horizon\"])\n",
        "    seeds   = meta[\"seeds\"]\n",
        "    n_folds = int(meta[\"n_folds\"])\n",
        "    hidden_dim   = int(meta.get(\"hidden_dim\", 128))\n",
        "    bidirectional= bool(meta.get(\"bidirectional\", False))\n",
        "\n",
        "    models_x, models_y, scalers = [], [], []\n",
        "    for seed in seeds:\n",
        "        sdir = base_dir / f\"seed_{seed}\"\n",
        "        for fold in range(1, n_folds + 1):\n",
        "            sc_path = sdir / f\"scaler_fold{fold}.pkl\"\n",
        "            dx_path = sdir / f\"model_dx_fold{fold}.pt\"\n",
        "            dy_path = sdir / f\"model_dy_fold{fold}.pt\"\n",
        "            if not (sc_path.exists() and dx_path.exists() and dy_path.exists()):\n",
        "                print(f\"[WARN] missing seed={seed} fold={fold}, skip\")\n",
        "                continue\n",
        "            scaler = joblib.load(sc_path)\n",
        "            mx = SeqModel(len(feature_cols), horizon,\n",
        "                          hidden_dim=hidden_dim, bidirectional=bidirectional\n",
        "                          ).to(cfg.DEVICE)\n",
        "            mx.load_state_dict(torch.load(dx_path, map_location=cfg.DEVICE)); mx.eval()\n",
        "            my = SeqModel(len(feature_cols), horizon,\n",
        "                          hidden_dim=hidden_dim, bidirectional=bidirectional\n",
        "                          ).to(cfg.DEVICE)\n",
        "            my.load_state_dict(torch.load(dy_path, map_location=cfg.DEVICE)); my.eval()\n",
        "            scalers.append(scaler); models_x.append(mx); models_y.append(my)\n",
        "\n",
        "    assert len(models_x) > 0, f\"No models loaded from {base_dir}\"\n",
        "    print(f\"[LOAD] loaded {len(models_x)} ŒîX & {len(models_y)} ŒîY models from {base_dir}\")\n",
        "    return models_x, models_y, scalers, meta\n",
        "\n",
        "# -------------------------------\n",
        "# Loss (Huber + time decay + 2nd-order velocity smooth)\n",
        "# -------------------------------\n",
        "class TemporalHuber(nn.Module):\n",
        "    def __init__(self, delta=0.5, time_decay=0.02, lam_smooth=0.01):\n",
        "        super().__init__()\n",
        "        self.delta = delta\n",
        "        self.time_decay = time_decay\n",
        "        self.lam_smooth = lam_smooth\n",
        "\n",
        "    def forward(self, pred, target, mask):\n",
        "        # base huber\n",
        "        err = pred - target\n",
        "        abs_err = torch.abs(err)\n",
        "        huber = torch.where(\n",
        "            abs_err <= self.delta,\n",
        "            0.5 * err * err,\n",
        "            self.delta * (abs_err - 0.5 * self.delta)\n",
        "        )\n",
        "\n",
        "        # time decay (keep your logic)\n",
        "        if self.time_decay and self.time_decay > 0:\n",
        "            L = pred.size(1)\n",
        "            t = torch.arange(L, device=pred.device, dtype=pred.dtype)\n",
        "            w = torch.exp(-self.time_decay * t).view(1, L)\n",
        "            huber = huber * w\n",
        "            mask  = mask  * w\n",
        "\n",
        "        main_loss = (huber * mask).sum() / (mask.sum() + 1e-8)\n",
        "\n",
        "        # velocity smooth (2nd difference ‚âà jerk), conservative maskÂØπÈΩê\n",
        "        if self.lam_smooth and pred.size(1) > 2:\n",
        "            d1 = pred[:, 1:] - pred[:, :-1]          # [B, T-1]\n",
        "            d2 = d1[:, 1:] - d1[:, :-1]              # [B, T-2]\n",
        "            m2 = mask[:, 2:]                         # ÂØπÈΩêÈïøÂ∫¶\n",
        "            smooth = (d2 * d2) * m2\n",
        "            smooth_loss = smooth.sum() / (m2.sum() + 1e-8)\n",
        "        else:\n",
        "            smooth_loss = pred.new_tensor(0.0)\n",
        "\n",
        "        return main_loss + self.lam_smooth * smooth_loss\n",
        "\n",
        "\n",
        "# class SeqModel(nn.Module):\n",
        "#     def __init__(self, input_dim, horizon):\n",
        "#         super().__init__()\n",
        "#         self.gru = nn.GRU(input_dim, 128, num_layers=2, batch_first=True, dropout=0.1)\n",
        "#         self.pool_ln = nn.LayerNorm(128)\n",
        "#         self.pool_attn = nn.MultiheadAttention(128, num_heads=4, batch_first=True)\n",
        "#         self.pool_query = nn.Parameter(torch.randn(1, 1, 128))\n",
        "#         self.head = nn.Sequential(\n",
        "#             nn.Linear(128, 128), nn.GELU(), nn.Dropout(0.2), nn.Linear(128, horizon)\n",
        "#         )\n",
        "#     def forward(self, x):\n",
        "#         h, _ = self.gru(x)\n",
        "#         B = h.size(0)\n",
        "#         q = self.pool_query.expand(B, -1, -1)\n",
        "#         ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))\n",
        "#         out = self.head(ctx.squeeze(1))\n",
        "#         return torch.cumsum(out, dim=1)\n",
        "\n",
        "class ResidualMLP(nn.Module):\n",
        "    def __init__(self, d_in, d_hidden, horizon, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_in, d_hidden)\n",
        "        self.fc2 = nn.Linear(d_hidden, d_hidden)\n",
        "        self.proj = nn.Linear(d_in, d_hidden)  # skip\n",
        "        self.out = nn.Linear(d_hidden, horizon)\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.act = nn.GELU()\n",
        "    def forward(self, x):\n",
        "        y = self.drop(self.act(self.fc1(x)))\n",
        "        y = self.drop(self.act(self.fc2(y)) + self.proj(x))  # ÊÆãÂ∑Æ\n",
        "        return self.out(y)\n",
        "\n",
        "class SeqModel(nn.Module):\n",
        "    def __init__(self, input_dim, horizon, hidden_dim=128, num_layers=2, bidirectional=False,\n",
        "                 use_residual=True, n_queries=2):\n",
        "        super().__init__()\n",
        "        self.bidirectional = bidirectional\n",
        "        self.use_residual = use_residual\n",
        "        self.gru = nn.GRU(\n",
        "            input_dim, hidden_dim, num_layers=num_layers, batch_first=True,\n",
        "            dropout=0.1, bidirectional=bidirectional\n",
        "        )\n",
        "        h_out = hidden_dim * (2 if bidirectional else 1)\n",
        "        self.in_proj = nn.Linear(input_dim, h_out) if use_residual else None\n",
        "\n",
        "        self.pool_ln   = nn.LayerNorm(h_out)\n",
        "        self.pool_attn = nn.MultiheadAttention(h_out, num_heads=4, batch_first=True)\n",
        "        self.pool_query= nn.Parameter(torch.randn(1, n_queries, h_out))  # Â§öqueryÊ±áËÅöÊõ¥Â§ö‰∏ä‰∏ãÊñá\n",
        "        self.head      = ResidualMLP(h_out*n_queries, hidden_dim, horizon)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, _ = self.gru(x)                        # [B,T,h_out]\n",
        "        if self.use_residual:\n",
        "            h = h + self.in_proj(x)               # Êó∂Èó¥Áª¥ÊÆãÂ∑Æ\n",
        "\n",
        "        B = h.size(0)\n",
        "        q = self.pool_query.expand(B, -1, -1)     # [B,Q,h_out]\n",
        "        ctx, _ = self.pool_attn(q, self.pool_ln(h), self.pool_ln(h))  # [B,Q,h_out]\n",
        "        ctx = ctx.reshape(B, -1)                  # ÊãºÊàê [B, Q*h_out]\n",
        "        out = self.head(ctx)                      # [B, H]\n",
        "        return torch.cumsum(out, dim=1)\n",
        "\n",
        "\n",
        "def prepare_targets(batch_axis, max_h):\n",
        "    tensors, masks = [], []\n",
        "    for arr in batch_axis:\n",
        "        L = len(arr)\n",
        "        padded = np.pad(arr, (0, max_h - L), constant_values=0).astype(np.float32)\n",
        "        mask = np.zeros(max_h, dtype=np.float32)\n",
        "        mask[:L] = 1.0\n",
        "        tensors.append(torch.tensor(padded))\n",
        "        masks.append(torch.tensor(mask))\n",
        "    return torch.stack(tensors), torch.stack(masks)\n",
        "\n",
        "def train_model(X_train, y_train, X_val, y_val, input_dim, horizon, config, noise_std=0.01, model_kwargs=None):\n",
        "    device = config.DEVICE\n",
        "    # model = SeqModel(input_dim, horizon).to(device)\n",
        "    model = SeqModel(input_dim, horizon, **(model_kwargs or {})).to(device)\n",
        "    criterion = TemporalHuber(delta=0.5, time_decay=0.03)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.LEARNING_RATE, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=5, factor=0.5, verbose=False)\n",
        "\n",
        "    # build batches (keep numpy ‚Üí torch)\n",
        "    def build_batches(X, Y):\n",
        "        batches = []\n",
        "        B = config.BATCH_SIZE\n",
        "        for i in range(0, len(X), B):\n",
        "            end = min(i + B, len(X))\n",
        "            xs = torch.tensor(np.stack(X[i:end]).astype(np.float32))\n",
        "            ys, ms = prepare_targets([Y[j] for j in range(i, end)], horizon)\n",
        "            batches.append((xs, ys, ms))\n",
        "        return batches\n",
        "\n",
        "    tr_batches = build_batches(X_train, y_train)\n",
        "    va_batches = build_batches(X_val,   y_val)\n",
        "\n",
        "    best_loss, best_state, bad = float('inf'), None, 0\n",
        "    for epoch in range(1, config.EPOCHS + 1):\n",
        "        model.train()\n",
        "        train_losses = []\n",
        "        for bx, by, bm in tr_batches:\n",
        "            bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
        "            # ËÆ≠ÁªÉÊúüÂ¢ûÂº∫Ôºà‰∏é TTA ÂØπÈΩêÔºâ\n",
        "            bx = add_random_gaussian(bx, sigma_max=noise_std)  # ÈöèÊú∫Âº∫Â∫¶Âô™Â£∞\n",
        "            bx = random_time_mask(bx, p=0.10, max_width=3)    # Êó∂Èó¥mask\n",
        "            bx = flip_context_keep_last(bx, p=0.10)           # ÂèçËΩ¨ÂâçT-1ÔºåÊú´Â∏ß‰∏çÂä®\n",
        "            pred = model(bx)\n",
        "\n",
        "            loss = criterion(pred, by, bm)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            train_losses.append(loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        with torch.no_grad():\n",
        "            for bx, by, bm in va_batches:\n",
        "                bx, by, bm = bx.to(device), by.to(device), bm.to(device)\n",
        "                pred = model(bx)\n",
        "                val_losses.append(criterion(pred, by, bm).item())\n",
        "\n",
        "        trl, val = float(np.mean(train_losses)), float(np.mean(val_losses))\n",
        "        scheduler.step(val)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"  Epoch {epoch}: train={trl:.4f}, val={val:.4f}\")\n",
        "\n",
        "        if val < best_loss:\n",
        "            best_loss, bad = val, 0\n",
        "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
        "        else:\n",
        "            bad += 1\n",
        "            if bad >= config.PATIENCE:\n",
        "                print(f\"  Early stop at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    if best_state:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, best_loss\n",
        "\n",
        "def predict_with_tta_per_model(mx, my, scaler, X_test_raw, device, tta=6, noise_std=0.01, use_flip=True):\n",
        "    \"\"\"\n",
        "    ÂØπÂçï‰∏™ (mx,my,scaler) ÂÅö TTAÔºåËøîÂõû [N,H] ÁöÑ dx,dy È¢ÑÊµã„ÄÇ\n",
        "    - Âú®‚ÄúÊ†áÂáÜÂåñÂêéÁöÑÁ©∫Èó¥‚ÄùÂä†Âô™Â£∞Ôºà‰∏éËÆ≠ÁªÉ‰∏ÄËá¥Ôºâ\n",
        "    - ÂèØÈÄâÔºöÂèçËΩ¨Ââç T-1 Â∏ßÔºàÊú´Â∏ß‰∏çÂä®ÔºâÔºå‰∏éÊú™ÂèçËΩ¨ÁªìÊûúÂπ≥Âùá\n",
        "    - ÈáçÂ§ç tta Ê¨°ÂèñÂùáÂÄº\n",
        "    \"\"\"\n",
        "    mx.eval(); my.eval()\n",
        "    outs_dx, outs_dy = [], []\n",
        "    base = np.stack([scaler.transform(s) for s in X_test_raw]).astype(np.float32)\n",
        "    xt = torch.tensor(base, device=device)\n",
        "\n",
        "    for _ in range(max(1, tta)):\n",
        "        xt_aug = xt\n",
        "        if noise_std and noise_std > 0:\n",
        "            xt_aug = xt_aug + torch.randn_like(xt_aug) * noise_std\n",
        "\n",
        "        with torch.no_grad():\n",
        "            dx = mx(xt_aug)\n",
        "            dy = my(xt_aug)\n",
        "            if use_flip and xt_aug.size(1) > 1:\n",
        "                ctx = xt_aug[:, :-1].flip(1)\n",
        "                xt_flip = torch.cat([ctx, xt_aug[:, -1:].clone()], dim=1)\n",
        "                dx = 0.5 * (dx + mx(xt_flip))\n",
        "                dy = 0.5 * (dy + my(xt_flip))\n",
        "\n",
        "        outs_dx.append(dx.detach().cpu().numpy())\n",
        "        outs_dy.append(dy.detach().cpu().numpy())\n",
        "\n",
        "    return np.mean(outs_dx, axis=0), np.mean(outs_dy, axis=0)\n",
        "\n",
        "import random as _py_random\n",
        "import torch\n",
        "\n",
        "def random_time_mask(bx, p=0.1, max_width=3):\n",
        "    \"\"\"\n",
        "    Âú®Êó∂Èó¥Áª¥ÂÅöÁü≠ÊÆµÂ§çÂà∂ÈÅÆÊå°ÔºöÈöèÊú∫Êåë‰∏ÄÊÆµ [s, s+w) Áî®Ââç/Âêé‰∏ÄÂ∏ßÂÄºÊõøÊç¢„ÄÇ\n",
        "    - bx: [B, T, D] (torch, ÊîØÊåÅÂú® GPU ‰∏äÂéüÂú∞Êîπ)\n",
        "    \"\"\"\n",
        "    if p <= 0 or max_width <= 0:\n",
        "        return bx\n",
        "    B, T, D = bx.shape\n",
        "    if T <= 1:\n",
        "        return bx\n",
        "    for i in range(B):\n",
        "        if _py_random.random() < p:\n",
        "            w = _py_random.randint(1, max_width)\n",
        "            s = _py_random.randint(0, max(0, T - 1 - w))\n",
        "            if s > 0:\n",
        "                bx[i, s:s+w] = bx[i, s-1].unsqueeze(0)\n",
        "            else:\n",
        "                bx[i, s:s+w] = bx[i, s+w].unsqueeze(0)\n",
        "    return bx\n",
        "\n",
        "def flip_context_keep_last(bx, p=0.1):\n",
        "    \"\"\"\n",
        "    ‰ªÖÂèçËΩ¨Ââç T-1 Â∏ßÔºà‰øùÊåÅÊúÄÂêé‰∏ÄÂ∏ß‰∏çÂä®ÔºâÔºåÂà∂ÈÄ†‚Äú‰∏ä‰∏ãÊñáÂèçËΩ¨‚Äù„ÄÇ\n",
        "    \"\"\"\n",
        "    if p <= 0:\n",
        "        return bx\n",
        "    B, T, D = bx.shape\n",
        "    if T <= 1:\n",
        "        return bx\n",
        "    mask = torch.rand(B, device=bx.device) < p\n",
        "    if mask.any():\n",
        "        ctx = bx[mask, :-1].flip(1)\n",
        "        bx[mask] = torch.cat([ctx, bx[mask, -1:].clone()], dim=1)\n",
        "    return bx\n",
        "\n",
        "def add_random_gaussian(bx, sigma_max=0.02):\n",
        "    \"\"\"\n",
        "    ÁªôÊï¥ÊÆµÂ∫èÂàóÂä†‰∏ÄÊ¨°È´òÊñØÂô™Â£∞ÔºàÂº∫Â∫¶Âú® [0, sigma_max] ÂÜÖÈöèÊú∫Ôºâ„ÄÇ\n",
        "    \"\"\"\n",
        "    if sigma_max <= 0:\n",
        "        return bx\n",
        "    sigma = sigma_max * torch.rand(1, device=bx.device)\n",
        "    return bx + torch.randn_like(bx) * sigma\n",
        "\n",
        "# ------------------------------_\n",
        "# Main pipeline (MODIFICADO PARA ENSEMBLE DE SEMILLAS)\n",
        "# ------------------------------_\n",
        "class CFG(Config):\n",
        "    # A√±adimos la lista de semillas para el ensemble\n",
        "    SEEDS = [42, 19, 89,64,33] # ¬°Puedes cambiar o a√±adir m√°s semillas aqu√≠!\n",
        "\n",
        "def main():\n",
        "    cfg = CFG()\n",
        "    print(\"=\"*80)\n",
        "    print(f\"RUN MODE: TRAIN={getattr(cfg, 'TRAIN', False)} | SUB={getattr(cfg, 'SUB', False)}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"cuDF backend active? {USE_CUDF}\")\n",
        "\n",
        "    # --- sanity checks ---\n",
        "    if not cfg.TRAIN and not cfg.SUB:\n",
        "        raise ValueError(\"Please set a run mode: TRAIN=True/SUB=False or TRAIN=False/SUB=True\")\n",
        "    if cfg.TRAIN and cfg.SUB:\n",
        "        raise ValueError(\"TRAIN and SUB cannot both be True\")\n",
        "\n",
        "    # ---------------------------\n",
        "    # TRAIN: train & save to a writable dir\n",
        "    # ---------------------------\n",
        "    if cfg.TRAIN:\n",
        "        # redirect saving path to a writable place\n",
        "        save_dir = Path(\"./saved_models\")\n",
        "        save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        cfg.MODELS_DIR = save_dir   # <<< key line: write into /kaggle/working\n",
        "\n",
        "        # 1) load training data\n",
        "        print(\"\\n[1/4] Âä†ËΩΩËÆ≠ÁªÉÊï∞ÊçÆ‚Ä¶\")\n",
        "        train_input_files  = [cfg.DATA_DIR / f\"train/input_2023_w{w:02d}.csv\"  for w in range(1, 19)]\n",
        "        train_output_files = [cfg.DATA_DIR / f\"train/output_2023_w{w:02d}.csv\" for w in range(1, 19)]\n",
        "        train_input  = pd.concat([pd.read_csv(f) for f in train_input_files  if f.exists()], ignore_index=True)\n",
        "        train_output = pd.concat([pd.read_csv(f) for f in train_output_files if f.exists()], ignore_index=True)\n",
        "\n",
        "        # 2) features + sequences (unified direction)\n",
        "        print(\"\\n[2/4] ÁâπÂæÅ‰∏éÂ∫èÂàóÔºàÁªü‰∏ÄÊñπÂêëÔºâ‚Ä¶\")\n",
        "        feature_groups = getattr(cfg, \"FEATURE_GROUPS\", None)\n",
        "        seqs, tdx, tdy, tfids, seq_meta, feat_cols, dir_map = prepare_sequences_with_advanced_features(\n",
        "            train_input, output_df=train_output, is_training=True,\n",
        "            window_size=cfg.WINDOW_SIZE, feature_groups=feature_groups\n",
        "        )\n",
        "        sequences  = list(seqs)\n",
        "        targets_dx = list(tdx)\n",
        "        targets_dy = list(tdy)\n",
        "\n",
        "        # 2.5) write meta to the same (writable) dir\n",
        "        write_meta(feat_cols, cfg, base_dir=cfg.MODELS_DIR)\n",
        "\n",
        "\n",
        "        # 3) multi-seed √ó KFold, save per-fold artifacts\n",
        "        print(\"\\n[3/4] Â§öÁßçÂ≠ê √ó K ÊäòËÆ≠ÁªÉÂπ∂‰øùÂ≠òÊ®°Âûã‚Ä¶\")\n",
        "        # groups = np.array([d['game_id'] for d in seq_meta])\n",
        "        groups = np.array([f\"{d['game_id']}_{d['play_id']}\" for d in seq_meta])\n",
        "\n",
        "        seeds = getattr(cfg, \"SEEDS\", [cfg.SEED])\n",
        "        all_rmse = []        # ÊâÄÊúâ seed√ófold ÁöÑ per-dim RMSE\n",
        "        cv_log = []          # ‰πüÊääÊØèÊäòÊåáÊ†áÊîæËøõÂàóË°®ÔºåÊúÄÂêéÂÜô json\n",
        "\n",
        "        for seed in seeds:\n",
        "            print(f\"\\n{'='*70}\\n   Seed {seed}\\n{'='*70}\")\n",
        "            set_seed(seed)\n",
        "            gkf = GroupKFold(n_splits=cfg.N_FOLDS)\n",
        "\n",
        "            fold_rmses = []  # ÂΩìÂâç seed ÁöÑÊØèÊäò RMSEÔºàper-dimÔºâ\n",
        "\n",
        "            for fold, (tr, va) in enumerate(gkf.split(sequences, groups=groups), 1):\n",
        "                print(f\"\\n{'-'*60}\\nFold {fold}/{cfg.N_FOLDS} (seed {seed})\\n{'-'*60}\")\n",
        "\n",
        "                X_tr = [sequences[i] for i in tr]\n",
        "                X_va = [sequences[i] for i in va]\n",
        "\n",
        "                scaler = StandardScaler()\n",
        "                scaler.fit(np.vstack([s for s in X_tr]))\n",
        "\n",
        "                X_tr_sc = np.stack([scaler.transform(s) for s in X_tr]).astype(np.float32)\n",
        "                X_va_sc = np.stack([scaler.transform(s) for s in X_va]).astype(np.float32)\n",
        "\n",
        "\n",
        "                # model_kwargs = dict(hidden_dim=cfg.HIDDEN_DIM, bidirectional=getattr(cfg, \"BIDIRECTIONAL\", False))\n",
        "                model_kwargs = dict(\n",
        "                    hidden_dim=cfg.HIDDEN_DIM,\n",
        "                    bidirectional=getattr(cfg, \"BIDIRECTIONAL\", False),\n",
        "                    use_residual=True,\n",
        "                    n_queries=2\n",
        "                )\n",
        "\n",
        "                print(\"ËÆ≠ÁªÉ ŒîX ‚Ä¶\")\n",
        "                mx, loss_x = train_model(\n",
        "                    X_tr_sc, [targets_dx[i] for i in tr],\n",
        "                    X_va_sc, [targets_dx[i] for i in va],\n",
        "                    X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg,\n",
        "                    model_kwargs=model_kwargs\n",
        "                )\n",
        "\n",
        "                print(\"ËÆ≠ÁªÉ ŒîY ‚Ä¶\")\n",
        "                my, loss_y = train_model(\n",
        "                    X_tr_sc, [targets_dy[i] for i in tr],\n",
        "                    X_va_sc, [targets_dy[i] for i in va],\n",
        "                    X_tr_sc.shape[-1], cfg.MAX_FUTURE_HORIZON, cfg,\n",
        "                    model_kwargs=model_kwargs\n",
        "                )\n",
        "\n",
        "\n",
        "                # --- NEW: ËÆ°ÁÆó‰∏âÂè£ÂæÑÔºåÈªòËÆ§Áî® per-dim Ëøõ CV Ê±áÊÄª ---\n",
        "                rmse_perdim = compute_val_rmse(\n",
        "                    mx, my, X_va_sc,\n",
        "                    [targets_dx[i] for i in va],\n",
        "                    [targets_dy[i] for i in va],\n",
        "                    cfg.MAX_FUTURE_HORIZON, cfg.DEVICE, mode=\"per-dim\"\n",
        "                )\n",
        "                rmse_2d = compute_val_rmse(\n",
        "                    mx, my, X_va_sc,\n",
        "                    [targets_dx[i] for i in va],\n",
        "                    [targets_dy[i] for i in va],\n",
        "                    cfg.MAX_FUTURE_HORIZON, cfg.DEVICE, mode=\"2d\"\n",
        "                )\n",
        "                mean_dist = compute_val_rmse(\n",
        "                    mx, my, X_va_sc,\n",
        "                    [targets_dx[i] for i in va],\n",
        "                    [targets_dy[i] for i in va],\n",
        "                    cfg.MAX_FUTURE_HORIZON, cfg.DEVICE, mode=\"mean-dist\"\n",
        "                )\n",
        "\n",
        "                print(f\"[VAL] seed {seed} fold {fold} ‚Üí \"\n",
        "                      f\"Huber dx={loss_x:.5f}, dy={loss_y:.5f} | \"\n",
        "                      f\"per-dim RMSE={rmse_perdim:.4f} | 2D RMSE={rmse_2d:.4f} | meanDist={mean_dist:.4f} yards\")\n",
        "\n",
        "                fold_rmses.append(rmse_perdim)\n",
        "                all_rmse.append(rmse_perdim)\n",
        "                cv_log.append({\n",
        "                    \"seed\": seed, \"fold\": fold,\n",
        "                    \"rmse_perdim\": rmse_perdim,\n",
        "                    \"rmse_2d\": rmse_2d,\n",
        "                    \"mean_dist\": mean_dist,\n",
        "                    \"loss_dx\": float(loss_x),\n",
        "                    \"loss_dy\": float(loss_y),\n",
        "                })\n",
        "\n",
        "                # ‰øùÂ≠òÊ®°Âûã\n",
        "                save_fold_artifacts(seed=seed, fold=fold, scaler=scaler, mx=mx, my=my, base_dir=cfg.MODELS_DIR)\n",
        "\n",
        "            # --- NEW: ÂΩìÂâç seed Ê±áÊÄª ---\n",
        "            print(f\"[SEED SUMMARY] seed {seed} RMSEs: {[f'{r:.4f}' for r in fold_rmses]} | \"\n",
        "                  f\"mean={float(np.mean(fold_rmses)):.4f} yards\")\n",
        "\n",
        "        # --- NEW: ÊâÄÊúâ seeds√ófolds ÁöÑÊúÄÁªàÊ±áÊÄª & ËêΩÁõò ---\n",
        "        print(f\"[CV SUMMARY] all folds RMSEs: {[f'{r:.4f}' for r in all_rmse]}\")\n",
        "        print(f\"[CV SUMMARY] overall mean RMSE = {float(np.mean(all_rmse)):.4f} yards\")\n",
        "\n",
        "        # ÂÜôÂà∞Á£ÅÁõòÔºàÊñπ‰æøÂõûÁúãÔºâ\n",
        "        try:\n",
        "            with open(cfg.MODELS_DIR / \"cv_metrics.json\", \"w\") as f:\n",
        "                json.dump({\"per_fold\": cv_log, \"overall_mean_perdim\": float(np.mean(all_rmse))}, f, indent=2)\n",
        "            print(f\"‚úì CV metrics written to {cfg.MODELS_DIR / 'cv_metrics.json'}\")\n",
        "        except Exception as e:\n",
        "            print(f\"[WARN] writing cv_metrics.json failed: {e}\")\n",
        "\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPLETE (TRAIN)!\")\n",
        "        print(\"=\"*80)\n",
        "        print(f\"‚úì Models saved to: {cfg.MODELS_DIR}\")\n",
        "        print(f\"Seeds: {cfg.SEEDS} | Folds: {cfg.N_FOLDS} ‚Üí checkpoints per axis: {len(cfg.SEEDS)*cfg.N_FOLDS}\")\n",
        "        print(f\"Features used: {len(feat_cols)}  (cuDF active: {USE_CUDF})\")\n",
        "        return\n",
        "\n",
        "    # ---------------------------\n",
        "    # SUB: Evaluation API mode (new format)\n",
        "    # ---------------------------\n",
        "    if cfg.SUB:\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"EVALUATION API MODE\")\n",
        "        print(\"=\"*80)\n",
        "        print(\"Models will be loaded on first predict() call\")\n",
        "        print(\"The inference server is set up in Cell 9\")\n",
        "        print(\"In competition rerun, the server will handle requests automatically\")\n",
        "        print(\"=\"*80)\n",
        "        return\n",
        "\n",
        "    # Â¶ÇÊûú‰∏§‰∏™ flag ÈÉΩÊ≤°ÂºÄÔºåÁªôÂá∫ÊèêÈÜí\n",
        "    raise ValueError(\"ËØ∑Âú® Config ‰∏≠ËÆæÁΩÆËøêË°åÊ®°ÂºèÔºöTRAIN=True/SUB=False Êàñ TRAIN=False/SUB=True\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n",
        "# =============================================================================\n",
        "# Evaluation API Server Setup\n",
        "# =============================================================================\n",
        "\n",
        "# Global variables to store models (loaded once on first predict call)\n",
        "_models_loaded = False\n",
        "_models_x = None\n",
        "_models_y = None\n",
        "_scalers = None\n",
        "_meta = None\n",
        "_feature_cols = None\n",
        "\n",
        "def load_models_once():\n",
        "    \"\"\"Load models on first predict call (no 5-minute time limit)\"\"\"\n",
        "    global _models_loaded, _models_x, _models_y, _scalers, _meta, _feature_cols\n",
        "\n",
        "    if _models_loaded:\n",
        "        return\n",
        "\n",
        "    print(\"[SERVER] Loading models for first time...\")\n",
        "    cfg = Config()\n",
        "    cfg.MODELS_DIR = Path(\"/kaggle/input/hsiaosuan-sttn/saved_models\")\n",
        "\n",
        "    _models_x, _models_y, _scalers, _meta = load_saved_ensemble(cfg, base_dir=cfg.MODELS_DIR)\n",
        "    _feature_cols = _meta[\"feature_cols\"]\n",
        "\n",
        "    _models_loaded = True\n",
        "    print(f\"[SERVER] Loaded {len(_models_x)} models successfully\")\n",
        "\n",
        "\n",
        "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inference function: process each batch of data\n",
        "\n",
        "    Args:\n",
        "        test: Frames to predict (contains game_id, play_id, nfl_id, frame_id, etc.)\n",
        "        test_input: Available input data (historical frames)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with x, y coordinates\n",
        "    \"\"\"\n",
        "    global _models_x, _models_y, _scalers, _meta, _feature_cols\n",
        "\n",
        "    # First call: load models (no time limit)\n",
        "    if not _models_loaded:\n",
        "        load_models_once()\n",
        "\n",
        "    # Convert to pandas (our code is pandas-based)\n",
        "    test_pd = test.to_pandas()\n",
        "    test_input_pd = test_input.to_pandas()\n",
        "\n",
        "    cfg = Config()\n",
        "    saved_window = int(_meta.get(\"window_size\", cfg.WINDOW_SIZE))\n",
        "    saved_groups = _meta.get(\"feature_groups\", cfg.FEATURE_GROUPS)\n",
        "\n",
        "    # Build sequences\n",
        "    test_seqs, test_meta, feat_cols_t, _ = prepare_sequences_with_advanced_features(\n",
        "        test_input_pd, test_template=test_pd, is_training=False,\n",
        "        window_size=saved_window, feature_groups=saved_groups\n",
        "    )\n",
        "\n",
        "    idx_x = feat_cols_t.index('x')\n",
        "    idx_y = feat_cols_t.index('y')\n",
        "\n",
        "    X_test_raw = list(test_seqs)\n",
        "    x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n",
        "    y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n",
        "\n",
        "    # TTA inference\n",
        "    tta_times = 6\n",
        "    tta_noise = 0.01\n",
        "    use_flip_ta = True\n",
        "\n",
        "    all_preds_dx, all_preds_dy = [], []\n",
        "    for mx, my, sc in zip(_models_x, _models_y, _scalers):\n",
        "        dx_tta, dy_tta = predict_with_tta_per_model(\n",
        "            mx, my, sc, X_test_raw, cfg.DEVICE,\n",
        "            tta=tta_times, noise_std=tta_noise, use_flip=use_flip_ta\n",
        "        )\n",
        "        all_preds_dx.append(dx_tta)\n",
        "        all_preds_dy.append(dy_tta)\n",
        "\n",
        "    ens_dx = np.mean(all_preds_dx, axis=0)\n",
        "    ens_dy = np.mean(all_preds_dy, axis=0)\n",
        "\n",
        "    H = ens_dx.shape[1]\n",
        "\n",
        "    # Build predictions\n",
        "    rows = []\n",
        "    tt_idx = test_pd.set_index(['game_id','play_id','nfl_id']).sort_index()\n",
        "\n",
        "    for i, meta_row in enumerate(test_meta):\n",
        "        gid = meta_row['game_id']\n",
        "        pid = meta_row['play_id']\n",
        "        nid = meta_row['nfl_id']\n",
        "        play_is_right = (meta_row['play_direction'] == 'right')\n",
        "\n",
        "        try:\n",
        "            fids = tt_idx.loc[(gid, pid, nid), 'frame_id']\n",
        "            if isinstance(fids, pd.Series):\n",
        "                fids = fids.sort_values().tolist()\n",
        "            else:\n",
        "                fids = [int(fids)]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for t, fid in enumerate(fids):\n",
        "            tt = min(t, H - 1)\n",
        "            x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, FIELD_LENGTH)\n",
        "            y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, FIELD_WIDTH)\n",
        "            x_out, y_out = invert_to_original_direction(x_uni, y_uni, play_is_right)\n",
        "            rows.append({'x': x_out, 'y': y_out})\n",
        "\n",
        "    # Return Polars DataFrame (recommended)\n",
        "    predictions = pl.DataFrame(rows)\n",
        "\n",
        "    assert len(predictions) == len(test)\n",
        "    return predictions"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T14:30:16.073798Z",
          "iopub.execute_input": "2025-12-02T14:30:16.074235Z",
          "iopub.status.idle": "2025-12-02T14:30:16.116753Z",
          "shell.execute_reply.started": "2025-12-02T14:30:16.074214Z",
          "shell.execute_reply": "2025-12-02T14:30:16.115965Z"
        },
        "id": "1SSbdpL7ed9U"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nfl_gru.py\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "TIMETAG = \"20251108_071543\"\n",
        "dest_dir = \"./src\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "source_dir = f\"/kaggle/input/1114kaiyuan-0562/output/20251108_071543/src\"\n",
        "shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)\n",
        "\n",
        "USE_CUDF = False\n",
        "try:\n",
        "    # zero/low-code GPU acceleration for DataFrame ops\n",
        "    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    USE_CUDF = True\n",
        "    print(\"using cuda_backend pandas for faster parallel data processing\")\n",
        "except Exception:\n",
        "    print(\"cuda df not used\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from src.config import Config\n",
        "\n",
        "# =============================================================================\n",
        "# Evaluation API Server Setup\n",
        "# =============================================================================\n",
        "# New imports for evaluation API\n",
        "import polars as pl\n",
        "from src.utils import load_saved_ensemble_stt, invert_to_original_direction\n",
        "from src.preprocess import prepare_sequences_with_advanced_features\n",
        "from src.model import STTransformer\n",
        "from src.predict import predict_sst\n",
        "\n",
        "# Global variables to store models (loaded once on first predict call)\n",
        "_models_loaded = False\n",
        "_models = None\n",
        "_scalers = None\n",
        "_meta = None\n",
        "_feature_cols = None\n",
        "\n",
        "\n",
        "def load_models_once():\n",
        "    \"\"\"Load models on first predict call (no 5-minute time limit)\"\"\"\n",
        "    global _models_loaded, _models, _scalers, _meta, _feature_cols\n",
        "\n",
        "    if _models_loaded:\n",
        "        return\n",
        "\n",
        "    print(\"[SERVER] Loading models for first time...\")\n",
        "    cfg = Config()\n",
        "    cfg.MODELS_DIR = Path(f\"/kaggle/input/1114kaiyuan-0562/output/20251108_071543\")\n",
        "\n",
        "    _models, _scalers, _meta = load_saved_ensemble_stt(cfg.MODELS_DIR, STTransformer)\n",
        "    _feature_cols = _meta[\"feature_cols\"]\n",
        "\n",
        "    _models_loaded = True\n",
        "    print(f\"[SERVER] Loaded {len(_models)} models successfully\")\n",
        "\n",
        "\n",
        "def predict(\n",
        "    test: pl.DataFrame, test_input: pl.DataFrame\n",
        ") -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inference function: process each batch of data\n",
        "\n",
        "    Args:\n",
        "        test: Frames to predict (contains game_id, play_id, nfl_id, frame_id, etc.)\n",
        "        test_input: Available input data (historical frames)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with x, y coordinates\n",
        "    \"\"\"\n",
        "    global _models, _scalers, _meta, _feature_cols\n",
        "\n",
        "    # First call: load models (no time limit)\n",
        "    if not _models_loaded:\n",
        "        load_models_once()\n",
        "\n",
        "    # Convert to pandas (our code is pandas-based)\n",
        "    test_pd = test.to_pandas()\n",
        "    test_input_pd = test_input.to_pandas()\n",
        "\n",
        "    cfg = Config()\n",
        "    saved_groups = _meta.get(\"feature_groups\", cfg.FEATURE_GROUPS)\n",
        "\n",
        "    # Build sequences\n",
        "    test_seqs, test_meta, feat_cols_t = prepare_sequences_with_advanced_features(\n",
        "        test_input_pd,\n",
        "        test_pd,\n",
        "        feature_groups=saved_groups,\n",
        "    )\n",
        "\n",
        "    idx_x = feat_cols_t.index(\"x\")\n",
        "    idx_y = feat_cols_t.index(\"y\")\n",
        "\n",
        "    X_test_raw = list(test_seqs)\n",
        "    x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n",
        "    y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n",
        "\n",
        "    all_preds_dx, all_preds_dy = [], []\n",
        "    for m, sc in zip(_models, _scalers):\n",
        "        dx_tta, dy_tta = predict_sst(\n",
        "            m,\n",
        "            sc,\n",
        "            X_test_raw,\n",
        "            cfg.DEVICE,\n",
        "        )\n",
        "        all_preds_dx.append(dx_tta)\n",
        "        all_preds_dy.append(dy_tta)\n",
        "\n",
        "    ens_dx = np.mean(all_preds_dx, axis=0)\n",
        "    ens_dy = np.mean(all_preds_dy, axis=0)\n",
        "\n",
        "    H = ens_dx.shape[1]\n",
        "\n",
        "    # Build predictions\n",
        "    rows = []\n",
        "    tt_idx = test_pd.set_index([\"game_id\", \"play_id\", \"nfl_id\"]).sort_index()\n",
        "\n",
        "    for i, meta_row in enumerate(test_meta):\n",
        "        gid = meta_row[\"game_id\"]\n",
        "        pid = meta_row[\"play_id\"]\n",
        "        nid = meta_row[\"nfl_id\"]\n",
        "        play_dir = meta_row[\"play_direction\"]\n",
        "\n",
        "        try:\n",
        "            fids = tt_idx.loc[(gid, pid, nid), \"frame_id\"]\n",
        "            if isinstance(fids, pd.Series):\n",
        "                fids = fids.sort_values().tolist()\n",
        "            else:\n",
        "                fids = [int(fids)]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for t, fid in enumerate(fids):\n",
        "            tt = min(t, H - 1)\n",
        "            x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, Config.FIELD_X_MAX)\n",
        "            y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, Config.FIELD_Y_MAX)\n",
        "            x_uni, y_uni = invert_to_original_direction(\n",
        "                x_uni, y_uni, play_dir == \"right\"\n",
        "            )\n",
        "            rows.append({\"x\": x_uni, \"y\": y_uni})\n",
        "\n",
        "    predictions = pl.DataFrame(rows)\n",
        "\n",
        "    assert len(predictions) == len(test)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T14:30:16.118194Z",
          "iopub.execute_input": "2025-12-02T14:30:16.118525Z",
          "iopub.status.idle": "2025-12-02T14:30:16.134272Z",
          "shell.execute_reply.started": "2025-12-02T14:30:16.118506Z",
          "shell.execute_reply": "2025-12-02T14:30:16.133614Z"
        },
        "id": "erL-4J2Ved9V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nfl_gru2.py\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "TIMETAG = \"20251108_071543\"\n",
        "dest_dir = \"./src\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "source_dir = f\"/kaggle/input/1113gru-0576/output/20251108_071543/src\"\n",
        "shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)\n",
        "\n",
        "USE_CUDF = False\n",
        "try:\n",
        "    # zero/low-code GPU acceleration for DataFrame ops\n",
        "    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    USE_CUDF = True\n",
        "    print(\"using cuda_backend pandas for faster parallel data processing\")\n",
        "except Exception:\n",
        "    print(\"cuda df not used\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from src.config import Config\n",
        "\n",
        "# =============================================================================\n",
        "# Evaluation API Server Setup\n",
        "# =============================================================================\n",
        "# New imports for evaluation API\n",
        "import polars as pl\n",
        "from src.utils import load_saved_ensemble_stt, invert_to_original_direction\n",
        "from src.preprocess import prepare_sequences_with_advanced_features\n",
        "from src.model import STTransformer\n",
        "from src.predict import predict_sst\n",
        "\n",
        "# Global variables to store models (loaded once on first predict call)\n",
        "_models_loaded = False\n",
        "_models = None\n",
        "_scalers = None\n",
        "_meta = None\n",
        "_feature_cols = None\n",
        "\n",
        "\n",
        "def load_models_once():\n",
        "    \"\"\"Load models on first predict call (no 5-minute time limit)\"\"\"\n",
        "    global _models_loaded, _models, _scalers, _meta, _feature_cols\n",
        "\n",
        "    if _models_loaded:\n",
        "        return\n",
        "\n",
        "    print(\"[SERVER] Loading models for first time...\")\n",
        "    cfg = Config()\n",
        "    cfg.MODELS_DIR = Path(f\"/kaggle/input/1113gru-0576/output/20251108_071543\")\n",
        "\n",
        "    _models, _scalers, _meta = load_saved_ensemble_stt(cfg.MODELS_DIR, STTransformer)\n",
        "    _feature_cols = _meta[\"feature_cols\"]\n",
        "\n",
        "    _models_loaded = True\n",
        "    print(f\"[SERVER] Loaded {len(_models)} models successfully\")\n",
        "\n",
        "\n",
        "def predict(\n",
        "    test: pl.DataFrame, test_input: pl.DataFrame\n",
        ") -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inference function: process each batch of data\n",
        "\n",
        "    Args:\n",
        "        test: Frames to predict (contains game_id, play_id, nfl_id, frame_id, etc.)\n",
        "        test_input: Available input data (historical frames)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with x, y coordinates\n",
        "    \"\"\"\n",
        "    global _models, _scalers, _meta, _feature_cols\n",
        "\n",
        "    # First call: load models (no time limit)\n",
        "    if not _models_loaded:\n",
        "        load_models_once()\n",
        "\n",
        "    # Convert to pandas (our code is pandas-based)\n",
        "    test_pd = test.to_pandas()\n",
        "    test_input_pd = test_input.to_pandas()\n",
        "\n",
        "    cfg = Config()\n",
        "    saved_groups = _meta.get(\"feature_groups\", cfg.FEATURE_GROUPS)\n",
        "\n",
        "    # Build sequences\n",
        "    test_seqs, test_meta, feat_cols_t = prepare_sequences_with_advanced_features(\n",
        "        test_input_pd,\n",
        "        test_pd,\n",
        "        feature_groups=saved_groups,\n",
        "    )\n",
        "\n",
        "    idx_x = feat_cols_t.index(\"x\")\n",
        "    idx_y = feat_cols_t.index(\"y\")\n",
        "\n",
        "    X_test_raw = list(test_seqs)\n",
        "    x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n",
        "    y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n",
        "\n",
        "    all_preds_dx, all_preds_dy = [], []\n",
        "    for m, sc in zip(_models, _scalers):\n",
        "        dx_tta, dy_tta = predict_sst(\n",
        "            m,\n",
        "            sc,\n",
        "            X_test_raw,\n",
        "            cfg.DEVICE,\n",
        "        )\n",
        "        all_preds_dx.append(dx_tta)\n",
        "        all_preds_dy.append(dy_tta)\n",
        "\n",
        "    ens_dx = np.mean(all_preds_dx, axis=0)\n",
        "    ens_dy = np.mean(all_preds_dy, axis=0)\n",
        "\n",
        "    H = ens_dx.shape[1]\n",
        "\n",
        "    # Build predictions\n",
        "    rows = []\n",
        "    tt_idx = test_pd.set_index([\"game_id\", \"play_id\", \"nfl_id\"]).sort_index()\n",
        "\n",
        "    for i, meta_row in enumerate(test_meta):\n",
        "        gid = meta_row[\"game_id\"]\n",
        "        pid = meta_row[\"play_id\"]\n",
        "        nid = meta_row[\"nfl_id\"]\n",
        "        play_dir = meta_row[\"play_direction\"]\n",
        "\n",
        "        try:\n",
        "            fids = tt_idx.loc[(gid, pid, nid), \"frame_id\"]\n",
        "            if isinstance(fids, pd.Series):\n",
        "                fids = fids.sort_values().tolist()\n",
        "            else:\n",
        "                fids = [int(fids)]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for t, fid in enumerate(fids):\n",
        "            tt = min(t, H - 1)\n",
        "            x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, Config.FIELD_X_MAX)\n",
        "            y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, Config.FIELD_Y_MAX)\n",
        "            x_uni, y_uni = invert_to_original_direction(\n",
        "                x_uni, y_uni, play_dir == \"right\"\n",
        "            )\n",
        "            rows.append({\"x\": x_uni, \"y\": y_uni})\n",
        "\n",
        "    predictions = pl.DataFrame(rows)\n",
        "\n",
        "    assert len(predictions) == len(test)\n",
        "    return predictions\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T14:30:16.135077Z",
          "iopub.execute_input": "2025-12-02T14:30:16.135322Z",
          "iopub.status.idle": "2025-12-02T14:30:16.158667Z",
          "shell.execute_reply.started": "2025-12-02T14:30:16.1353Z",
          "shell.execute_reply": "2025-12-02T14:30:16.15785Z"
        },
        "id": "WsRWTRdKed9V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile nfl_2026.py\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "\n",
        "TIMETAG = \"20251108_071543\"\n",
        "dest_dir = \"./src\"\n",
        "os.makedirs(dest_dir, exist_ok=True)\n",
        "\n",
        "source_dir = f\"/kaggle/input/nfl-bdb-2026/NFL2026_111325/NFL2026_111325/src/\"\n",
        "shutil.copytree(source_dir, dest_dir, dirs_exist_ok=True)\n",
        "\n",
        "USE_CUDF = False\n",
        "try:\n",
        "    # zero/low-code GPU acceleration for DataFrame ops\n",
        "    os.environ[\"CUDF_PANDAS_BACKEND\"] = \"cudf\"\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    USE_CUDF = True\n",
        "    print(\"using cuda_backend pandas for faster parallel data processing\")\n",
        "except Exception:\n",
        "    print(\"cuda df not used\")\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "import json\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from src.config import Config\n",
        "\n",
        "# =============================================================================\n",
        "# Evaluation API Server Setup\n",
        "# =============================================================================\n",
        "# New imports for evaluation API\n",
        "import polars as pl\n",
        "from src.utils import load_saved_ensemble_stt, invert_to_original_direction\n",
        "from src.preprocess import prepare_sequences_with_advanced_features\n",
        "from src.model import STTransformer\n",
        "from src.predict import predict_sst\n",
        "\n",
        "# Global variables to store models (loaded once on first predict call)\n",
        "_models_loaded = False\n",
        "_models = None\n",
        "_scalers = None\n",
        "_meta = None\n",
        "_feature_cols = None\n",
        "\n",
        "\n",
        "def load_models_once():\n",
        "    \"\"\"Load models on first predict call (no 5-minute time limit)\"\"\"\n",
        "    global _models_loaded, _models, _scalers, _meta, _feature_cols\n",
        "\n",
        "    if _models_loaded:\n",
        "        return\n",
        "\n",
        "    print(\"[SERVER] Loading models for first time...\")\n",
        "    cfg = Config()\n",
        "    cfg.MODELS_DIR = Path(f\"/kaggle/input/nfl-bdb-2026/NFL2026_111325/NFL2026_111325\")\n",
        "\n",
        "    _models, _scalers, _meta = load_saved_ensemble_stt(cfg.MODELS_DIR, STTransformer)\n",
        "    _feature_cols = _meta[\"feature_cols\"]\n",
        "\n",
        "    _models_loaded = True\n",
        "    print(f\"[SERVER] Loaded {len(_models)} models successfully\")\n",
        "\n",
        "\n",
        "def predict(\n",
        "    test: pl.DataFrame, test_input: pl.DataFrame\n",
        ") -> pl.DataFrame | pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Inference function: process each batch of data\n",
        "\n",
        "    Args:\n",
        "        test: Frames to predict (contains game_id, play_id, nfl_id, frame_id, etc.)\n",
        "        test_input: Available input data (historical frames)\n",
        "\n",
        "    Returns:\n",
        "        DataFrame with x, y coordinates\n",
        "    \"\"\"\n",
        "    global _models, _scalers, _meta, _feature_cols\n",
        "\n",
        "    # First call: load models (no time limit)\n",
        "    if not _models_loaded:\n",
        "        load_models_once()\n",
        "\n",
        "    # Convert to pandas (our code is pandas-based)\n",
        "    test_pd = test.to_pandas()\n",
        "    test_input_pd = test_input.to_pandas()\n",
        "\n",
        "    cfg = Config()\n",
        "    saved_groups = _meta.get(\"feature_groups\", cfg.FEATURE_GROUPS)\n",
        "\n",
        "    # Build sequences\n",
        "    test_seqs, test_meta, feat_cols_t = prepare_sequences_with_advanced_features(\n",
        "        test_input_pd,\n",
        "        test_pd,\n",
        "        feature_groups=saved_groups,\n",
        "    )\n",
        "\n",
        "    idx_x = feat_cols_t.index(\"x\")\n",
        "    idx_y = feat_cols_t.index(\"y\")\n",
        "\n",
        "    X_test_raw = list(test_seqs)\n",
        "    x_last_uni = np.array([s[-1, idx_x] for s in X_test_raw], dtype=np.float32)\n",
        "    y_last_uni = np.array([s[-1, idx_y] for s in X_test_raw], dtype=np.float32)\n",
        "\n",
        "    all_preds_dx, all_preds_dy = [], []\n",
        "    for m, sc in zip(_models, _scalers):\n",
        "        dx_tta, dy_tta = predict_sst(\n",
        "            m,\n",
        "            sc,\n",
        "            X_test_raw,\n",
        "            cfg.DEVICE,\n",
        "        )\n",
        "        all_preds_dx.append(dx_tta)\n",
        "        all_preds_dy.append(dy_tta)\n",
        "\n",
        "    ens_dx = np.mean(all_preds_dx, axis=0)\n",
        "    ens_dy = np.mean(all_preds_dy, axis=0)\n",
        "\n",
        "    H = ens_dx.shape[1]\n",
        "\n",
        "    # Build predictions\n",
        "    rows = []\n",
        "    tt_idx = test_pd.set_index([\"game_id\", \"play_id\", \"nfl_id\"]).sort_index()\n",
        "\n",
        "    for i, meta_row in enumerate(test_meta):\n",
        "        gid = meta_row[\"game_id\"]\n",
        "        pid = meta_row[\"play_id\"]\n",
        "        nid = meta_row[\"nfl_id\"]\n",
        "        play_dir = meta_row[\"play_direction\"]\n",
        "\n",
        "        try:\n",
        "            fids = tt_idx.loc[(gid, pid, nid), \"frame_id\"]\n",
        "            if isinstance(fids, pd.Series):\n",
        "                fids = fids.sort_values().tolist()\n",
        "            else:\n",
        "                fids = [int(fids)]\n",
        "        except KeyError:\n",
        "            continue\n",
        "\n",
        "        for t, fid in enumerate(fids):\n",
        "            tt = min(t, H - 1)\n",
        "            x_uni = np.clip(x_last_uni[i] + ens_dx[i, tt], 0, Config.FIELD_X_MAX)\n",
        "            y_uni = np.clip(y_last_uni[i] + ens_dy[i, tt], 0, Config.FIELD_Y_MAX)\n",
        "            x_uni, y_uni = invert_to_original_direction(\n",
        "                x_uni, y_uni, play_dir == \"right\"\n",
        "            )\n",
        "            rows.append({\"x\": x_uni, \"y\": y_uni})\n",
        "\n",
        "    predictions = pl.DataFrame(rows)\n",
        "\n",
        "    assert len(predictions) == len(test)\n",
        "    return predictions"
      ],
      "metadata": {
        "trusted": true,
        "id": "HlTY0rO8ed9V"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# --- ENSEMBLE: 5 open source models with equal weights ---\n",
        "import importlib\n",
        "import polars as pl\n",
        "import pandas as pd\n",
        "import kaggle_evaluation.nfl_inference_server\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONSTANTS\n",
        "# ============================================================================\n",
        "FIELD_X_MAX = 120.0\n",
        "FIELD_Y_MAX = 53.3\n",
        "\n",
        "def invert_to_original_direction(x_u, y_u, play_dir_right: bool):\n",
        "    if not play_dir_right:\n",
        "        return float(x_u), float(y_u)\n",
        "    return float(Config.FIELD_X_MAX - x_u), float(Config.FIELD_Y_MAX - y_u)\n",
        "\n",
        "# Import the 5 models\n",
        "nfl_gru = importlib.import_module('nfl_gru')         # 0.562\n",
        "nfl_gru3 = importlib.import_module('nfl_gru3')       # 0.584\n",
        "nfl_gru4 = importlib.import_module('nfl_2026')       # 0.565\n",
        "nfl_gnn = importlib.import_module('nfl_gnn')         # 0.580\n",
        "nfl_mymodel = importlib.import_module('nfl_mymodel') # 0.577\n",
        "\n",
        "\n",
        "def get_open_predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pd.DataFrame:\n",
        "    # Get predictions from all 5 models\n",
        "    pred_gru = nfl_gru.predict(test, test_input)\n",
        "    pred_gru3 = nfl_gru3.predict(test, test_input)\n",
        "    pred_gru4 = nfl_gru4.predict(test, test_input)\n",
        "    pred_gnn = nfl_gnn.predict(test, test_input)\n",
        "    pred_mymodel = nfl_mymodel.predict(test, test_input)\n",
        "\n",
        "    # Ensure all are Pandas DataFrames\n",
        "    if isinstance(pred_gru, pl.DataFrame):\n",
        "        pred_gru = pred_gru.to_pandas()\n",
        "    if isinstance(pred_gru3, pl.DataFrame):\n",
        "        pred_gru3 = pred_gru3.to_pandas()\n",
        "    if isinstance(pred_gru4, pl.DataFrame):\n",
        "        pred_gru4 = pred_gru4.to_pandas()\n",
        "    if isinstance(pred_gnn, pl.DataFrame):\n",
        "        pred_gnn = pred_gnn.to_pandas()\n",
        "    if isinstance(pred_mymodel, pl.DataFrame):\n",
        "        pred_mymodel = pred_mymodel.to_pandas()\n",
        "\n",
        "    # Equal weights: 5 models √ó 20% each\n",
        "    pred_ensemble = (\n",
        "        0.20 * pred_gru[['x', 'y']].values +\n",
        "        0.20 * pred_gru3[['x', 'y']].values +\n",
        "        0.20 * pred_gru4[['x', 'y']].values +\n",
        "        0.20 * pred_gnn[['x', 'y']].values +\n",
        "        0.20 * pred_mymodel[['x', 'y']].values\n",
        "    )\n",
        "\n",
        "    return pl.DataFrame({'x': pred_ensemble[:, 0], 'y': pred_ensemble[:, 1]})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-02T14:32:08.694797Z",
          "iopub.execute_input": "2025-12-02T14:32:08.695073Z",
          "iopub.status.idle": "2025-12-02T14:32:48.172781Z",
          "shell.execute_reply.started": "2025-12-02T14:32:08.69505Z",
          "shell.execute_reply": "2025-12-02T14:32:48.166742Z"
        },
        "id": "7cYl36pied9W"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "SEED = 2025\n",
        "WINDOW_SIZE = 20\n",
        "PREDICT_SIZE = 94\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "def seed_everything(seed):\n",
        "    import random\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(seed=SEED)\n",
        "\n",
        "# ============================================================================\n",
        "# DATA LOADING & FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "def load_data(mode='train', inputs=None, outputs=None):\n",
        "    inputs['frame_id_max'] = inputs.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].transform('max')\n",
        "    inputs['frame_id'] = inputs['frame_id'] - inputs['frame_id_max']\n",
        "    inputs.drop(['frame_id_max'], axis=1, inplace=True)\n",
        "\n",
        "    df = pd.concat((inputs, outputs)).sort_values(['game_id', 'play_id', 'nfl_id', 'frame_id'])\n",
        "    df['game_id_play_id_nfl_id'] = df['game_id'].astype(str) + df['play_id'].astype(str) + df['nfl_id'].astype(str)\n",
        "\n",
        "    return df\n",
        "\n",
        "def unify_play_direction(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    XMAX, YMAX = 120, 53.3\n",
        "    dir_col = 'play_direction'\n",
        "\n",
        "    play_left = (df.groupby(['game_id', 'play_id'])[dir_col]\n",
        "                   .transform('first')\n",
        "                   .astype(str).str.lower().str.startswith('l'))\n",
        "    flip = play_left.fillna(False).values\n",
        "\n",
        "    x_cols = ['x', 'ball_land_x']\n",
        "    y_cols = ['y', 'ball_land_y']\n",
        "\n",
        "    for c in x_cols:\n",
        "        if c in df.columns:\n",
        "            df.loc[flip, c] = XMAX - df.loc[flip, c]\n",
        "    for c in y_cols:\n",
        "        if c in df.columns:\n",
        "            df.loc[flip, c] = YMAX - df.loc[flip, c]\n",
        "\n",
        "    for ang_col in ['o', 'dir']:\n",
        "        if ang_col in df.columns:\n",
        "            df.loc[flip, ang_col] = (df.loc[flip, ang_col] + 180.0) % 360.0\n",
        "\n",
        "    return df\n",
        "\n",
        "def add_target_role_specific_features(df):\n",
        "    receiver_mask = (df['player_role'] == 'Targeted Receiver').astype(float)\n",
        "    defense_mask = (df['player_role'] == 'Defensive Coverage').astype(float)\n",
        "\n",
        "    for c in ['dis_to_ball', 'dis_x_to_ball', 'dis_y_to_ball',\n",
        "              'dis_expected_to_ball', 'dis_expected_x_to_ball', 'dis_expected_y_to_ball',\n",
        "              'expected_v_x', 'expected_v_y', 'expected_v_x_diff', 'expected_v_y_diff',\n",
        "              'v_x', 'v_y', 'a_x', 'a_y', 's', 'a', 'expected_x', 'expected_y']:\n",
        "        df[f'receiver_{c}'] = receiver_mask * df[c]\n",
        "        df[f'defense_{c}'] = defense_mask * df[c]\n",
        "    return df\n",
        "\n",
        "def FE(df):\n",
        "    index = df.index\n",
        "    df = unify_play_direction(df)\n",
        "\n",
        "    df['age'] = df['game_id'] // (10**6) - (df['player_birth_date'].fillna('1993-01-22')).apply(lambda x: int(x[:4]))\n",
        "\n",
        "    df['sin_dir'] = np.sin(np.radians(df['dir']))\n",
        "    df['cos_dir'] = np.cos(np.radians(df['dir']))\n",
        "    df['sin_o'] = np.sin(np.radians(df['o']))\n",
        "    df['cos_o'] = np.cos(np.radians(df['o']))\n",
        "\n",
        "    df['v_x'] = df['s'] * df['sin_dir']\n",
        "    df['v_y'] = df['s'] * df['cos_dir']\n",
        "    df['a_x'] = df['a'] * df['sin_dir']\n",
        "    df['a_y'] = df['a'] * df['cos_dir']\n",
        "\n",
        "    df['time_remain'] = (df.groupby(['game_id', 'play_id', 'nfl_id'])['frame_id'].transform('max') - df['frame_id']) / 10\n",
        "\n",
        "    df['expected_x'] = np.clip(df['x'] + df['time_remain'] * df['v_x'], 0, 120)\n",
        "    df['expected_y'] = np.clip(df['y'] + df['time_remain'] * df['v_y'], 0, 53.3)\n",
        "\n",
        "    df['dis_x_to_ball'] = df['x'] - df['ball_land_x']\n",
        "    df['dis_y_to_ball'] = df['y'] - df['ball_land_y']\n",
        "    df['dis_to_ball'] = np.sqrt(df['dis_x_to_ball']**2 + df['dis_y_to_ball']**2)\n",
        "\n",
        "    df['expected_v_x'] = df['dis_x_to_ball'] / (df['time_remain'] + 0.1)\n",
        "    df['expected_v_y'] = df['dis_y_to_ball'] / (df['time_remain'] + 0.1)\n",
        "\n",
        "    df['expected_v_x_diff'] = df['expected_v_x'] - df['v_x']\n",
        "    df['expected_v_y_diff'] = df['expected_v_y'] - df['v_y']\n",
        "\n",
        "    df['dis_expected_x_to_ball'] = df['expected_x'] - df['ball_land_x']\n",
        "    df['dis_expected_y_to_ball'] = df['expected_y'] - df['ball_land_y']\n",
        "    df['dis_expected_to_ball'] = np.sqrt(df['dis_expected_x_to_ball']**2 + df['dis_expected_y_to_ball']**2)\n",
        "\n",
        "    df['initial_x'] = df.groupby(['game_id', 'play_id', 'nfl_id'])['x'].transform('first')\n",
        "    df['initial_y'] = df.groupby(['game_id', 'play_id', 'nfl_id'])['y'].transform('first')\n",
        "    df['deviation_from_initial'] = np.sqrt((df['x'] - df['initial_x'])**2 + (df['y'] - df['initial_y'])**2)\n",
        "\n",
        "    df = add_target_role_specific_features(df)\n",
        "\n",
        "    mirror_df = df[df['player_role'] == 'Targeted Receiver'][['game_id', 'play_id', 'frame_id', 'x', 'y']]\\\n",
        "                   .rename(columns={'x': 'mirror_offset_x', 'y': 'mirror_offset_y'})\n",
        "    df = df.merge(mirror_df, on=['game_id', 'play_id', 'frame_id'], how='left')\n",
        "    df['mirror_offset_x'] = df['x'] - df['mirror_offset_x']\n",
        "    df['mirror_offset_y'] = df['y'] - df['mirror_offset_y']\n",
        "\n",
        "    for (c1, c2) in [('ball_land_x', 'ball_land_y')]:\n",
        "        df[f'out_length_{c1}'] = np.maximum(df[c1] - 120, 0) + np.maximum(-df[c1], 0)\n",
        "        df[f'out_length_{c2}'] = np.maximum(df[c2] - 53.3, 0) + np.maximum(-df[c2], 0)\n",
        "        df[c1] = df[c1].clip(0, 120)\n",
        "        df[c2] = df[c2].clip(0, 53.3)\n",
        "\n",
        "    for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']:\n",
        "        df[f'{c}_overall_mean'] = df.groupby(['game_id', 'play_id', 'frame_id'])[c].transform('mean')\n",
        "        df[f'{c}-{c}_overall_mean'] = df[c] - df[f'{c}_overall_mean']\n",
        "\n",
        "    for role in ['Targeted Receiver', 'Defensive Coverage']:\n",
        "        role_mask = df['player_role'] == role\n",
        "        for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']:\n",
        "            df.loc[role_mask, f'{c}_friend_mean'] = df[role_mask].groupby(['game_id', 'play_id', 'frame_id'])[c].transform('mean')\n",
        "\n",
        "    for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']:\n",
        "        tmp1 = df[df['player_role'] == 'Targeted Receiver'].groupby(['game_id', 'play_id', 'frame_id'])[c].mean().reset_index()\n",
        "        tmp1['player_role'] = 'Defensive Coverage'\n",
        "        tmp2 = df[df['player_role'] == 'Defensive Coverage'].groupby(['game_id', 'play_id', 'frame_id'])[c].mean().reset_index()\n",
        "        tmp2['player_role'] = 'Targeted Receiver'\n",
        "        tmp = pd.concat((tmp1, tmp2))\n",
        "        tmp.columns = ['game_id', 'play_id', 'frame_id', f'{c}_enemy_mean', 'player_role']\n",
        "        df = df.merge(tmp, on=['game_id', 'play_id', 'frame_id', 'player_role'], how='left')\n",
        "\n",
        "    for c in ['x', 'y', 's', 'a', 'v_x', 'v_y', 'a_x', 'a_y', 'dis_x_to_ball', 'dis_y_to_ball'] + \\\n",
        "            [f'{c}_overall_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] + \\\n",
        "            [f'{c}_friend_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] + \\\n",
        "            [f'{c}_enemy_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] + \\\n",
        "            [f'{c}-{c}_overall_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']]:\n",
        "        for gap in [1, 3, 5]:\n",
        "            df[c + f\"_diff{gap}\"] = df.groupby(['game_id', 'play_id', 'nfl_id'])[c].diff(gap)\n",
        "\n",
        "    df['play_direction'] = (df['play_direction'] == 'right').astype(int)\n",
        "    df['player_side'] = (df['player_side'] == 'Defense').astype(int)\n",
        "\n",
        "    for c in ['player_role']:\n",
        "        for v in ['Targeted Receiver', 'Defensive Coverage']:\n",
        "            df[f'{c}_{v}'] = (df[c] == v).astype(np.int8)\n",
        "\n",
        "    df.index = index\n",
        "    return df\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE COLUMNS (198 features)\n",
        "# ============================================================================\n",
        "BASE = [\n",
        "    'x', 'y', 'sin_dir', 'cos_dir', 'sin_o', 'cos_o', 's', 'a', 'v_x', 'v_y', 'a_x', 'a_y',\n",
        "    'ball_land_x', 'ball_land_y', 'time_remain', 'expected_x', 'expected_y',\n",
        "    'dis_x_to_ball', 'dis_y_to_ball', 'dis_to_ball', 'deviation_from_initial',\n",
        "    'expected_v_x', 'expected_v_y', 'expected_v_x_diff', 'expected_v_y_diff',\n",
        "    'dis_expected_x_to_ball', 'dis_expected_y_to_ball', 'dis_expected_to_ball',\n",
        "    'mirror_offset_x', 'mirror_offset_y', 'age',\n",
        "    'out_length_ball_land_x', 'out_length_ball_land_y'\n",
        "]\n",
        "\n",
        "FEATURES = (\n",
        "    BASE + ['player_side'] +\n",
        "    [f'{c}_{t}' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y'] for t in ['friend_mean', 'enemy_mean']] +\n",
        "    [f'{c}_overall_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] +\n",
        "    [f'{c}-{c}_overall_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] +\n",
        "    [c + f\"_diff{gap}\" for c in ['x', 'y', 's', 'a', 'v_x', 'v_y', 'a_x', 'a_y', 'dis_x_to_ball', 'dis_y_to_ball'] +\n",
        "                                [f'{c}_overall_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] +\n",
        "                                [f'{c}_friend_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] +\n",
        "                                [f'{c}_enemy_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']] +\n",
        "                                [f'{c}-{c}_overall_mean' for c in ['x', 'y', 'v_x', 'v_y', 'a_x', 'a_y']]\n",
        "     for gap in [1, 3, 5]] +\n",
        "    [f'{c}_{v}' for c in ['player_role'] for v in ['Targeted Receiver', 'Defensive Coverage']] +\n",
        "    [f'receiver_{feat}' for feat in ['dis_to_ball', 'dis_x_to_ball', 'dis_y_to_ball',\n",
        "                                     'dis_expected_to_ball', 'dis_expected_x_to_ball', 'dis_expected_y_to_ball',\n",
        "                                     'expected_v_x', 'expected_v_y', 'expected_v_x_diff', 'expected_v_y_diff',\n",
        "                                     'v_x', 'v_y', 'a_x', 'a_y', 's', 'a', 'expected_x', 'expected_y']] +\n",
        "    [f'defense_{feat}' for feat in ['dis_to_ball', 'dis_x_to_ball', 'dis_y_to_ball',\n",
        "                                    'dis_expected_to_ball', 'dis_expected_x_to_ball', 'dis_expected_y_to_ball',\n",
        "                                    'expected_v_x', 'expected_v_y', 'expected_v_x_diff', 'expected_v_y_diff',\n",
        "                                    'v_x', 'v_y', 'a_x', 'a_y', 's', 'a', 'expected_x', 'expected_y']]\n",
        ")\n",
        "\n",
        "print(f\"‚úì Using {len(FEATURES)} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# MODEL DEFINITION\n",
        "# ============================================================================\n",
        "class CumsumModel(nn.Module):\n",
        "    def __init__(self, WINDOW_SIZE=8, N_FEATURES=6, PREDICT_SIZE=94,\n",
        "                 n_head=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.T, self.F, self.P = WINDOW_SIZE, N_FEATURES, PREDICT_SIZE\n",
        "\n",
        "        self.reduce = nn.Linear(self.F, 64)\n",
        "        self.causal = nn.Sequential(\n",
        "            nn.Conv1d(64, 128, kernel_size=3, padding=2, dilation=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv1d(128, 256, kernel_size=3, padding=2, dilation=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(256)\n",
        "        self.mha = nn.MultiheadAttention(embed_dim=256, num_heads=n_head,\n",
        "                                         dropout=dropout, batch_first=True)\n",
        "        self.gru = nn.GRU(256, 256, batch_first=True, num_layers=2, dropout=dropout)\n",
        "        self.decoder = nn.Linear(256, self.P * 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        z = self.reduce(x)\n",
        "        z = self.causal(z.transpose(1, 2)).transpose(1, 2)\n",
        "        z = self.norm1(z)\n",
        "        attn_out, _ = self.mha(z, z, z)\n",
        "        z = z + attn_out\n",
        "        out, _ = self.gru(z)\n",
        "        hid = out[:, -1, :]\n",
        "        pred = self.decoder(hid)\n",
        "        return torch.cumsum(pred.view(B, self.P, 2), dim=1)\n",
        "\n",
        "# ============================================================================\n",
        "# LOAD MODELS (TOP 3 FOLDS ONLY)\n",
        "# ============================================================================\n",
        "print(\"Loading top 3 CumsumModel folds...\")\n",
        "models = []\n",
        "top_folds = [4, 6, 9]  # base_model4.pt, base_model6.pt, base_model9.pt\n",
        "\n",
        "for fold in top_folds:\n",
        "    model = torch.load(f'/kaggle/input/cumsum-model-dataset/outputs/base_model{fold}.pt',\n",
        "                       map_location='cpu', weights_only=False)\n",
        "    model.eval()\n",
        "    models.append(model)\n",
        "    print(f\"  ‚úì Loaded base_model{fold}.pt\")\n",
        "\n",
        "print(f\"‚úì Loaded {len(models)} models\\n\")\n",
        "\n",
        "# ============================================================================\n",
        "# DATASET CREATION\n",
        "# ============================================================================\n",
        "def create_dataset(df, norm_value, index=0):\n",
        "    X_tensor, y_tensor, mask_tensor = [], [], []\n",
        "    groups = df.groupby('game_id_play_id_nfl_id')\n",
        "\n",
        "    for uid, tmp in tqdm(groups, desc=\"Creating dataset\", leave=False):\n",
        "        X_array = np.zeros((WINDOW_SIZE, len(FEATURES)))\n",
        "        y_array = np.zeros((PREDICT_SIZE, 2))\n",
        "        mask_array = np.zeros((PREDICT_SIZE))\n",
        "\n",
        "        X = tmp[(tmp['frame_id'] <= index) & (tmp['frame_id'] > -WINDOW_SIZE + index)][FEATURES].fillna(0).values\n",
        "        y = tmp[(tmp['frame_id'] > index) & (tmp['frame_id'] <= PREDICT_SIZE + index)][['x', 'y']].values\n",
        "\n",
        "        if len(y) > 0 and len(X) > 0:\n",
        "            X_array[-len(X):] = X / norm_value\n",
        "            y_array[:len(y)] = y / np.array([120, 53])\n",
        "            mask_array[:len(y)] = 1\n",
        "\n",
        "            X_tensor.append(torch.Tensor(X_array))\n",
        "            y_tensor.append(torch.Tensor(y_array - X_array[-1][:2]))\n",
        "            mask_tensor.append(torch.Tensor(mask_array))\n",
        "\n",
        "    return torch.stack(X_tensor), torch.stack(y_tensor), torch.stack(mask_tensor)\n",
        "\n",
        "# ============================================================================\n",
        "# MAIN PREDICTION FUNCTION\n",
        "# ============================================================================\n",
        "def predict(test: pl.DataFrame, test_input: pl.DataFrame) -> pl.DataFrame | pd.DataFrame:\n",
        "    try:\n",
        "        # Step 1: Get open source ensemble\n",
        "        open_ensemble = get_open_predict(test, test_input)\n",
        "\n",
        "        # Step 2: Process data\n",
        "        test_df = load_data('test', test_input.to_pandas(), test.to_pandas())\n",
        "\n",
        "        # Store original play direction BEFORE FE transforms it\n",
        "        play_dir_map = (test_df.groupby(['game_id', 'play_id'])['play_direction']\n",
        "                         .first()\n",
        "                         .reset_index())\n",
        "\n",
        "        test_df = FE(test_df)\n",
        "\n",
        "        # Step 3: Create dataset (remove tqdm for cleaner output)\n",
        "        norm_value = test_df[FEATURES].max().values + 0.01\n",
        "        testX_tensor, _, testmask_tensor = create_dataset(test_df, norm_value, index=0)\n",
        "\n",
        "        # Step 4: Predict with top 3 folds\n",
        "        weights = [0.50, 0.30, 0.20]\n",
        "        all_preds = []\n",
        "\n",
        "        for model in models:\n",
        "            fold_pred = []\n",
        "            with torch.no_grad():\n",
        "                for i in range(0, len(testX_tensor), BATCH_SIZE):\n",
        "                    x_batch = testX_tensor[i:i+BATCH_SIZE]\n",
        "                    # Optional TTA: x_batch += torch.randn_like(x_batch) * 0.005\n",
        "                    y_pred = (model(x_batch) + testX_tensor[i:i+BATCH_SIZE, -1:, :2]) * torch.tensor([120, 53])\n",
        "\n",
        "                    m_batch = testmask_tensor[i:i+BATCH_SIZE].bool()\n",
        "                    fold_pred.append(y_pred[m_batch].cpu().numpy())\n",
        "\n",
        "            all_preds.append(np.concatenate(fold_pred))\n",
        "\n",
        "        # Weighted ensemble\n",
        "        ensemble_pred = sum(p * w for p, w in zip(all_preds, weights))\n",
        "\n",
        "        # Step 5: Post-process\n",
        "        future_mask = test_df['frame_id'] > 0\n",
        "        assert ensemble_pred.shape[0] == future_mask.sum(), \"Shape mismatch!\"\n",
        "\n",
        "        test_df.loc[future_mask, ['x', 'y']] = ensemble_pred\n",
        "        test_df[['x', 'y']] = test_df.groupby(['game_id', 'play_id', 'nfl_id'])[['x', 'y']].ffill().bfill()\n",
        "\n",
        "        # Reverse play direction using original values\n",
        "        test_df = test_df.merge(play_dir_map, on=['game_id', 'play_id'],\n",
        "                                 how='left', suffixes=('', '_orig'))\n",
        "        flip = test_df['play_direction_orig'].str.lower().str.startswith('l').fillna(False)\n",
        "\n",
        "        test_df.loc[flip, 'x'] = 120 - test_df.loc[flip, 'x']\n",
        "        test_df.loc[flip, 'y'] = 53.3 - test_df.loc[flip, 'y']\n",
        "        test_df[['x', 'y']] = test_df[['x', 'y']].clip(lower=[0, 0], upper=[120, 53.3])\n",
        "\n",
        "        # Step 6: Final ensemble\n",
        "        own_x = test_df.loc[future_mask, 'x'].values\n",
        "        own_y = test_df.loc[future_mask, 'y'].values\n",
        "\n",
        "        # To this (since open_ensemble is now a DataFrame):\n",
        "        return pl.DataFrame({\n",
        "            'x': 0.20 * own_x + 0.80 * open_ensemble['x'].to_numpy(),\n",
        "            'y': 0.20 * own_y + 0.80 * open_ensemble['y'].to_numpy()\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"ERROR: {e}\")\n",
        "        return get_open_predict(test, test_input)\n",
        "\n",
        "# ============================================================================\n",
        "# START SERVER\n",
        "# ============================================================================\n",
        "inference_server = kaggle_evaluation.nfl_inference_server.NFLInferenceServer(predict)\n",
        "\n",
        "if os.getenv('KAGGLE_IS_COMPETITION_RERUN'):\n",
        "    inference_server.serve()\n",
        "else:\n",
        "    inference_server.run_local_gateway(('/kaggle/input/nfl-big-data-bowl-2026-prediction/',))\n",
        "\n",
        "print(\"üöÄ Server ready!\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "4NddF4dged9W"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}